[
  {
    "id": 1,
    "question": "Which scalability strategy involves adding more power (CPU, RAM) to an existing single server?",
    "options": [
      "Horizontal scaling",
      "Vertical scaling",
      "Elastic scaling",
      "Database sharding"
    ],
    "answer": "Vertical scaling",
    "explanation": "Vertical scaling (scale-up) increases the capacity of a single machine. Horizontal scaling (scale-out) increases capacity by adding more machines to the pool.",
    "difficulty": "Beginner"
  },
  {
    "id": 2,
    "question": "What is the primary function of a Load Balancer in a distributed system?",
    "options": [
      "To encrypt data packets between the client and server",
      "To distribute incoming network traffic across multiple servers",
      "To persist user session data in a central database",
      "To monitor the CPU usage of individual application threads"
    ],
    "answer": "To distribute incoming network traffic across multiple servers",
    "explanation": "Load balancers act as the traffic cop, routing client requests across all servers to ensure no single server is overloaded. This increases reliability and availability.",
    "difficulty": "Beginner"
  },
  {
    "id": 3,
    "question": "In the context of databases, what does 'ACID' stand for?",
    "options": [
      "Atomicity, Consistency, Isolation, Durability",
      "Availability, Concurrency, Integrity, Distribution",
      "Authentication, Compression, Indexing, Duplication",
      "Asynchronous, Centralized, Integrated, Dynamic"
    ],
    "answer": "Atomicity, Consistency, Isolation, Durability",
    "explanation": "ACID is a standard set of properties that guarantees that database transactions are processed reliably. It ensures data integrity despite errors or power failures.",
    "difficulty": "Beginner"
  },
  {
    "id": 4,
    "question": "Which load balancing algorithm assigns requests to servers in a sequential, rotating order?",
    "options": [
      "Least Connections",
      "IP Hash",
      "Round Robin",
      "Weighted Response Time"
    ],
    "answer": "Round Robin",
    "explanation": "Round Robin iterates through a list of servers in order, distributing requests one by one. Least Connections routes to the server with the fewest active requests.",
    "difficulty": "Beginner"
  },
  {
    "id": 5,
    "question": "What is the primary trade-off when choosing a SQL (Relational) database over a NoSQL database?",
    "options": [
      "SQL databases do not support indexing",
      "SQL databases generally offer less rigid structure and schema flexibility",
      "SQL databases generally provide less strict consistency guarantees than NoSQL",
      "SQL databases generally scale horizontally more easily than NoSQL"
    ],
    "answer": "SQL databases generally offer less rigid structure and schema flexibility",
    "explanation": "NoSQL databases are designed to be schema-less or have flexible schemas, allowing for rapid iteration. SQL databases require predefined schemas with fixed data types.",
    "difficulty": "Beginner"
  },
  {
    "id": 6,
    "question": "What is the main purpose of using a Content Delivery Network (CDN)?",
    "options": [
      "To secure the database using encryption protocols",
      "To reduce latency by delivering content from geographically closer servers",
      "To balance the load between the application server and the database",
      "To automate the deployment of microservices"
    ],
    "answer": "To reduce latency by delivering content from geographically closer servers",
    "explanation": "CDNs cache static assets (images, CSS, JS) on edge servers located near the user. This minimizes the physical distance data must travel, reducing latency.",
    "difficulty": "Beginner"
  },
  {
    "id": 7,
    "question": "Which caching strategy involves checking the cache first; on a miss, retrieving data from the database and updating the cache?",
    "options": [
      "Write-through cache",
      "Write-back cache",
      "Cache-aside (Lazy loading)",
      "Write-around cache"
    ],
    "answer": "Cache-aside (Lazy loading)",
    "explanation": "In Cache-aside, the application manages the cache lookup and database retrieval. The cache only updates when there is a miss, minimizing unnecessary writes to the cache.",
    "difficulty": "Beginner"
  },
  {
    "id": 8,
    "question": "What is 'Sharding' in the context of database design?",
    "options": [
      "Creating a read-replica of the main database",
      "Splitting a large database into smaller, faster, more easily managed parts called data shards",
      "Compressing database files to save storage space",
      "Converting a SQL schema to a NoSQL schema"
    ],
    "answer": "Splitting a large database into smaller, faster, more easily managed parts called data shards",
    "explanation": "Sharding is a method of horizontal scaling where data is partitioned across multiple distinct database instances (shards) based on a shard key.",
    "difficulty": "Beginner"
  },
  {
    "id": 9,
    "question": "According to the CAP Theorem, a distributed system can only simultaneously guarantee how many of the following properties: Consistency, Availability, Partition Tolerance?",
    "options": [
      "All three",
      "Two out of three",
      "Only one",
      "It depends on the network latency"
    ],
    "answer": "Two out of three",
    "explanation": "The CAP theorem states that in the event of a network partition (P), a distributed system must trade off Consistency (C) and Availability (A). It cannot guarantee all three simultaneously.",
    "difficulty": "Beginner"
  },
  {
    "id": 10,
    "question": "Which architectural style structures an application as a collection of loosely coupled, independently deployable services?",
    "options": [
      "Monolithic architecture",
      "Microservices architecture",
      "Serverless architecture",
      "Mainframe architecture"
    ],
    "answer": "Microservices architecture",
    "explanation": "Microservices break the app into small services running unique processes. A Monolithic architecture combines all functionality into a single deployable unit.",
    "difficulty": "Beginner"
  },
  {
    "id": 11,
    "question": "What is the primary role of a Message Queue in system architecture?",
    "options": [
      "To synchronize two databases in real-time",
      "To enable asynchronous communication between services, allowing them to process tasks independently",
      "To store user passwords securely for authentication",
      "To route HTTP requests to the correct API endpoint"
    ],
    "answer": "To enable asynchronous communication between services, allowing them to process tasks independently",
    "explanation": "Message queues allow services to communicate without being directly connected (decoupling). The producer sends a message to the queue, and the consumer processes it later.",
    "difficulty": "Beginner"
  },
  {
    "id": 12,
    "question": "What is 'Latency' in the context of system performance?",
    "options": [
      "The total number of requests processed per second",
      "The time delay between a request and the response",
      "The percentage of time a system is operational",
      "The total amount of data transferred over the network"
    ],
    "answer": "The time delay between a request and the response",
    "explanation": "Latency measures the time it takes for a data packet to travel from source to destination. Throughput measures the volume of data processed in a given time.",
    "difficulty": "Beginner"
  },
  {
    "id": 13,
    "question": "Which database replication strategy involves sending all data modification commands to a single primary node?",
    "options": [
      "Multi-master replication",
      "Leaderless replication",
      "Master-Slave (Primary-Replica) replication",
      "Sharding"
    ],
    "answer": "Master-Slave (Primary-Replica) replication",
    "explanation": "In Master-Slave replication, only the Primary (Master) handles writes. The data is then replicated asynchronously or synchronously to the Read Replicas (Slaves).",
    "difficulty": "Beginner"
  },
  {
    "id": 14,
    "question": "What is the primary benefit of using an API Gateway in a microservices architecture?",
    "options": [
      "It acts as a reverse proxy to route requests, handle SSL termination, and enforce rate limiting",
      "It stores the database schema for all microservices",
      "It compiles the source code of microservices into machine code",
      "It replaces the need for a load balancer"
    ],
    "answer": "It acts as a reverse proxy to route requests, handle SSL termination, and enforce rate limiting",
    "explanation": "An API Gateway provides a single entry point for all clients. It handles cross-cutting concerns like authentication, logging, and rate limiting before routing to services.",
    "difficulty": "Beginner"
  },
  {
    "id": 15,
    "question": "What does 'Stateless' mean regarding a server or application?",
    "options": [
      "The server does not store any client session context between requests",
      "The server is powered down and not consuming energy",
      "The server stores every previous request ever made by the user",
      "The server cannot be scaled horizontally"
    ],
    "answer": "The server does not store any client session context between requests",
    "explanation": "In a stateless system, every request from a client contains all the information needed to process it. This simplifies horizontal scaling because any server can handle any request.",
    "difficulty": "Beginner"
  },
  {
    "id": 16,
    "question": "Which mechanism is used to prevent a specific API endpoint from being overwhelmed by too many user requests?",
    "options": [
      "Data sharding",
      "Rate limiting",
      "Database indexing",
      "Circuit breaking"
    ],
    "answer": "Rate limiting",
    "explanation": "Rate limiting restricts the number of requests a user or client can make within a specific time window. This protects the service from denial-of-service attacks or sudden spikes.",
    "difficulty": "Beginner"
  },
  {
    "id": 17,
    "question": "In database indexing, what is the main disadvantage of creating too many indexes on a table?",
    "options": [
      "Read performance degrades significantly",
      "Write performance degrades because indexes must be updated on every insert, update, or delete",
      "The database size decreases",
      "The data becomes inconsistent"
    ],
    "answer": "Write performance degrades because indexes must be updated on every insert, update, or delete",
    "explanation": "Indexes speed up reads but slow down writes. The database must update the index structures every time the data is modified, adding overhead.",
    "difficulty": "Beginner"
  },
  {
    "id": 18,
    "question": "What is the primary purpose of 'Consistent Hashing' in distributed systems?",
    "options": [
      "To ensure that all nodes have the exact same data",
      "To minimize the number of keys that need to be moved when nodes are added or removed",
      "To compress the data before storing it on disk",
      "To hash user passwords for authentication"
    ],
    "answer": "To minimize the number of keys that need to be moved when nodes are added or removed",
    "explanation": "Unlike standard hashing, consistent hashing ensures that only a small fraction of keys are remapped when the hash table size changes (e.g., a server fails or is added).",
    "difficulty": "Beginner"
  },
  {
    "id": 19,
    "question": "Which deployment strategy involves creating two identical production environments (Blue and Green) and switching traffic between them?",
    "options": [
      "Canary Deployment",
      "Rolling Deployment",
      "Blue-Green Deployment",
      "Big Bang Deployment"
    ],
    "answer": "Blue-Green Deployment",
    "explanation": "Blue-Green deployment reduces downtime by keeping an old version (Blue) running while the new version (Green) is deployed and tested, then instantly switching traffic.",
    "difficulty": "Beginner"
  },
  {
    "id": 20,
    "question": "What is 'Throughput'?",
    "options": [
      "The time taken to process a single request",
      "The number of processing units available in the CPU",
      "The amount of data or requests processed successfully within a specific time frame",
      "The delay introduced by network propagation"
    ],
    "answer": "The amount of data or requests processed successfully within a specific time frame",
    "explanation": "Throughput measures the rate of processing, usually requests per second (RPS) or transactions per second (TPS). High throughput indicates a high-capacity system.",
    "difficulty": "Beginner"
  },
  {
    "id": 21,
    "question": "Which pattern helps prevent a cascading failure in a distributed system by stopping requests to a failing service?",
    "options": [
      "The Circuit Breaker pattern",
      "The Repository pattern",
      "The Singleton pattern",
      "The Observer pattern"
    ],
    "answer": "The Circuit Breaker pattern",
    "explanation": "A Circuit Breaker detects failures and prevents the application from trying to perform an operation that is likely to fail, allowing the system to recover without overwhelming the failing service.",
    "difficulty": "Beginner"
  },
  {
    "id": 22,
    "question": "What is the primary function of the Domain Name System (DNS)?",
    "options": [
      "To assign IP addresses to devices on a local network",
      "To translate human-readable domain names (like google.com) into machine-readable IP addresses",
      "To encrypt web traffic using SSL/TLS",
      "To route emails between different mail servers"
    ],
    "answer": "To translate human-readable domain names (like google.com) into machine-readable IP addresses",
    "explanation": "DNS acts as the internet's phonebook. It maps domain names to IP addresses so browsers can load internet resources.",
    "difficulty": "Beginner"
  },
  {
    "id": 23,
    "question": "Which term describes a system's ability to remain operational even when some of its components fail?",
    "options": [
      "Scalability",
      "Fault Tolerance",
      "Efficiency",
      "Portability"
    ],
    "answer": "Fault Tolerance",
    "explanation": "Fault tolerance is the property that enables a system to continue operating properly in the event of the failure of some of its components, often using redundancy.",
    "difficulty": "Beginner"
  },
  {
    "id": 24,
    "question": "What is a 'Read Replica' in a database architecture?",
    "options": [
      "A backup copy of the database stored on tape",
      "A live copy of the primary database that handles only read operations",
      "The main database node that handles both read and write operations",
      "A cached version of the database schema"
    ],
    "answer": "A live copy of the primary database that handles only read operations",
    "explanation": "Read replicas offload read traffic from the primary database. They are updated asynchronously via replication from the primary instance.",
    "difficulty": "Beginner"
  },
  {
    "id": 25,
    "question": "In a NoSQL 'Key-Value' store, how is data retrieved?",
    "options": [
      "By querying using SQL-like syntax",
      "By using a unique Key associated with the Value",
      "By traversing relationships between nodes",
      "By indexing every column in the table"
    ],
    "answer": "By using a unique Key associated with the Value",
    "explanation": "Key-Value stores are schema-less data stores optimized for retrieving a single value (the payload) when provided with a specific unique key.",
    "difficulty": "Beginner"
  },
  {
    "id": 26,
    "question": "What is the primary advantage of a 'NoSQL' document database over a relational database?",
    "options": [
      "Strict ACID compliance for financial transactions",
      "Support for complex joins across multiple tables",
      "Flexible schema design to handle unstructured or semi-structured data",
      "Guaranteed data consistency across all nodes immediately"
    ],
    "answer": "Flexible schema design to handle unstructured or semi-structured data",
    "explanation": "Document databases (like MongoDB) store data in JSON-like documents, allowing developers to evolve the schema without requiring complex migrations.",
    "difficulty": "Beginner"
  },
  {
    "id": 27,
    "question": "What does 'High Availability' (HA) generally refer to?",
    "options": [
      "A system that runs on very expensive hardware",
      "A system that is accessible and operational with minimal downtime, usually through redundancy",
      "A system that processes data with extremely high throughput",
      "A system that has a very high security rating"
    ],
    "answer": "A system that is accessible and operational with minimal downtime, usually through redundancy",
    "explanation": "High availability design ensures that a system agrees to an operational level (uptime) of nearly 100%, usually achieved by eliminating single points of failure.",
    "difficulty": "Beginner"
  },
  {
    "id": 28,
    "question": "Which caching policy removes the least recently used item when the cache is full?",
    "options": [
      "LFU (Least Frequently Used)",
      "LRU (Least Recently Used)",
      "FIFO (First In, First Out)",
      "LIFO (Last In, First Out)"
    ],
    "answer": "LRU (Least Recently Used)",
    "explanation": "LRU evicts the item that has not been used for the longest time, based on the assumption that recently used data is more likely to be requested again.",
    "difficulty": "Beginner"
  },
  {
    "id": 29,
    "question": "What is the main purpose of 'Data Denormalization'?",
    "options": [
      "To reduce data redundancy and improve data integrity",
      "To add redundant data to a database to improve read performance",
      "To encrypt sensitive data in the database",
      "To migrate data from SQL to NoSQL"
    ],
    "answer": "To add redundant data to a database to improve read performance",
    "explanation": "Denormalization is the process of combining tables to optimize read performance at the cost of write performance and increased data redundancy.",
    "difficulty": "Beginner"
  },
  {
    "id": 30,
    "question": "Which technique allows a system to handle traffic spikes by automatically adding or removing computing resources?",
    "options": [
      "Elasticity (Auto Scaling)",
      "Clustering",
      "Virtualization",
      "Containerization"
    ],
    "answer": "Elasticity (Auto Scaling)",
    "explanation": "Elasticity defines the ability of a system to automatically expand or shrink resources based on the current workload demand, ensuring cost-efficiency and performance.",
    "difficulty": "Beginner"
  },
  {
    "id": 31,
    "question": "What is 'Sticky Sessions' (Session Affinity) in the context of load balancing?",
    "options": [
      "Distributing requests evenly regardless of the user",
      "Routing a specific user's requests to the same server for the duration of their session",
      "Keeping a session open indefinitely until the user logs out",
      "A security mechanism to prevent session hijacking"
    ],
    "answer": "Routing a specific user's requests to the same server for the duration of their session",
    "explanation": "Sticky sessions ensure that a client always connects to the same server, which is necessary if the application stores session state locally on the server.",
    "difficulty": "Beginner"
  },
  {
    "id": 32,
    "question": "What is the primary difference between 'Push' and 'Pull' based communication models?",
    "options": [
      "Push sends data to the consumer; Pull requires the consumer to request data",
      "Push is synchronous; Pull is asynchronous",
      "Push uses HTTP; Pull uses TCP",
      "Push is for databases; Pull is for caches"
    ],
    "answer": "Push sends data to the consumer; Pull requires the consumer to request data",
    "explanation": "In a Push model (like Pub/Sub), the producer initiates the transfer. In a Pull model (like polling), the consumer checks for updates or requests data.",
    "difficulty": "Beginner"
  },
  {
    "id": 33,
    "question": "What is a 'Single Point of Failure' (SPOF)?",
    "options": [
      "A component in the system that, if it fails, causes the entire system to stop working",
      "The primary server in a master-slave replication setup",
      "A software bug that causes a system crash",
      "The location where the code is deployed"
    ],
    "answer": "A component in the system that, if it fails, causes the entire system to stop working",
    "explanation": "A SPOF is a critical part of a system that has no redundancy. System design aims to eliminate SPOFs to improve reliability.",
    "difficulty": "Beginner"
  },
  {
    "id": 34,
    "question": "In the context of system design, what is 'Latency' usually measured in?",
    "options": [
      "Megabytes (MB)",
      "Requests per second (RPS)",
      "Milliseconds (ms) or seconds",
      "Hertz (Hz)"
    ],
    "answer": "Milliseconds (ms) or seconds",
    "explanation": "Latency is a time-based metric representing the delay before a transfer of data begins following an instruction for its transfer.",
    "difficulty": "Beginner"
  },
  {
    "id": 35,
    "question": "Which of the following best describes 'Eventual Consistency'?",
    "options": [
      "All nodes see the same data at the exact same time",
      "The system guarantees that, if no new updates are made, eventually all accesses will return the last updated value",
      "Data is written to the database immediately without checking validity",
      "The system does not guarantee that a read will return the most recent write"
    ],
    "answer": "The system guarantees that, if no new updates are made, eventually all accesses will return the last updated value",
    "explanation": "Eventual consistency is a consistency model used in distributed systems to achieve high availability, where the system guarantees that data will propagate to all nodes eventually, though not instantly.",
    "difficulty": "Beginner"
  },
  {
    "id": 36,
    "question": "In the context of the CAP theorem, what characteristic is sacrificed when a distributed system prioritizes Availability and Partition Tolerance (AP) during a network partition?",
    "options": [
      "Latency",
      "Consistency",
      "Scalability",
      "Durability"
    ],
    "answer": "Consistency",
    "explanation": "An AP system continues to accept requests despite network partitions, meaning different nodes may return stale or conflicting data, sacrificing strong consistency for availability.",
    "difficulty": "Intermediate"
  },
  {
    "id": 37,
    "question": "What is the primary advantage of using a consistent hashing algorithm for distributing requests in a distributed cache cluster?",
    "options": [
      "It ensures perfect uniform distribution of keys regardless of node count",
      "It minimizes the number of keys that need to be remapped when nodes are added or removed",
      "It guarantees strong consistency between the cache and the database",
      "It eliminates the need for a load balancer"
    ],
    "answer": "It minimizes the number of keys that need to be remapped when nodes are added or removed",
    "explanation": "Consistent hashing maps both data and nodes to a ring, ensuring that adding or removing a node only affects the keys immediately adjacent to it, rather than causing a massive remapping of the entire dataset.",
    "difficulty": "Intermediate"
  },
  {
    "id": 38,
    "question": "Why is the 'Leaky Bucket' algorithm often preferred over the 'Token Bucket' algorithm for traffic shaping in network interfaces?",
    "options": [
      "It allows for immediate bursts of traffic at maximum line rate",
      "It smooths out traffic bursts to a steady rate, preventing network congestion",
      "It requires significantly less memory to implement",
      "It guarantees zero packet loss"
    ],
    "answer": "It smooths out traffic bursts to a steady rate, preventing network congestion",
    "explanation": "The Leaky Bucket algorithm releases packets at a constant rate, effectively burst-smoothing traffic. In contrast, Token Bucket allows bursts up to the bucket's capacity, which can overwhelm a receiver.",
    "difficulty": "Intermediate"
  },
  {
    "id": 39,
    "question": "In a write-heavy database workload, how does the 'Write-Back' (or Write-Behind) caching strategy improve performance compared to 'Write-Through'?",
    "options": [
      "Data is written to the database synchronously, ensuring immediate consistency",
      "The application reads data directly from the cache without checking the database",
      "Data is written to the cache first and the database later asynchronously, reducing write latency",
      "It eliminates the need for a cache invalidation strategy"
    ],
    "answer": "Data is written to the cache first and the database later asynchronously, reducing write latency",
    "explanation": "Write-Back acknowledges the write as soon as the cache is updated and syncs to the database later. This reduces I/O wait time for the writer but introduces a risk of data loss if the cache fails before syncing.",
    "difficulty": "Intermediate"
  },
  {
    "id": 40,
    "question": "What is the main purpose of implementing a 'Read Replica' in a database architecture?",
    "options": [
      "To provide stronger data consistency for write operations",
      "To distribute write operations across multiple nodes",
      "To offload read traffic from the primary database instance",
      "To automatically shard the database horizontally"
    ],
    "answer": "To offload read traffic from the primary database instance",
    "explanation": "Read replicas are asynchronous copies of the primary database that handle read-only queries. This reduces the load on the primary node, allowing it to dedicate resources to handling write transactions.",
    "difficulty": "Intermediate"
  },
  {
    "id": 41,
    "question": "In the context of microservices, what is the primary benefit of using an 'Event-Driven Architecture' via a message broker?",
    "options": [
      "It ensures tight coupling between services for data consistency",
      "It allows services to scale independently based on load",
      "It guarantees that all messages are processed in strict chronological order",
      "It removes the need for service discovery"
    ],
    "answer": "It allows services to scale independently based on load",
    "explanation": "By decoupling services via asynchronous events, the message broker acts as a buffer. Producers can send messages regardless of consumer speed, and consumers can scale up/down independently to handle the backlog.",
    "difficulty": "Intermediate"
  },
  {
    "id": 42,
    "question": "Which load balancing algorithm is most suitable for a system where backend servers have significantly different hardware capabilities?",
    "options": [
      "Round Robin",
      "Least Connections",
      "Source IP Hash",
      "Random"
    ],
    "answer": "Least Connections",
    "explanation": "Least Connections directs traffic to the server with the fewest active connections. Unlike Round Robin (which assumes equal capacity), this prevents a weaker server from becoming overloaded by the same number of requests as a stronger server.",
    "difficulty": "Intermediate"
  },
  {
    "id": 43,
    "question": "What is 'Sharding' in the context of database scaling, and how does it differ from 'Replication'?",
    "options": [
      "Sharding copies data to multiple nodes for availability; replication splits data across nodes",
      "Sharding splits data horizontally across multiple nodes; replication copies the same data to multiple nodes",
      "Sharding is used for read-heavy workloads; replication is used for write-heavy workloads",
      "Sharding is only possible with NoSQL databases"
    ],
    "answer": "Sharding splits data horizontally across multiple nodes; replication copies the same data to multiple nodes",
    "explanation": "Sharding partitions a large dataset into smaller chunks (shards) spread across servers to scale write capacity. Replication copies the same dataset to multiple servers to improve read throughput and redundancy.",
    "difficulty": "Intermediate"
  },
  {
    "id": 44,
    "question": "Why is 'Exponential Backoff' commonly utilized in retry mechanisms for distributed systems?",
    "options": [
      "To ensure the request is processed within a fixed timeout",
      "To increase the priority of the retry with every attempt",
      "To reduce congestion and give the service time to recover from the failure",
      "To guarantee that the request will eventually succeed"
    ],
    "answer": "To reduce congestion and give the service time to recover from the failure",
    "explanation": "Exponential backoff increases the wait time between retries exponentially (e.g., 1s, 2s, 4s). This prevents the client from hammering a struggling service, which can worsen the outage or trigger rate-limiters.",
    "difficulty": "Intermediate"
  },
  {
    "id": 45,
    "question": "What is the 'Thundering Herd' problem in the context of caching, and how is it typically mitigated?",
    "options": [
      "Too many cache misses causing high database load; mitigated by cache warming",
      "A network storm causing packet loss; mitigated by retries",
      "Servers crashing due to memory leaks; mitigated by garbage collection",
      "Data inconsistency between shards; mitigated by two-phase commit"
    ],
    "answer": "Too many cache misses causing high database load; mitigated by cache warming",
    "explanation": "The Thundering Herd occurs when a cached object expires and numerous concurrent requests try to fetch it from the database simultaneously. 'Cache warming' (preloading) or 'lock buffering' prevents this stampede.",
    "difficulty": "Intermediate"
  },
  {
    "id": 46,
    "question": "What is the role of a 'Sidecar' pattern in a containerized microservices architecture?",
    "options": [
      "To act as a primary load balancer for the service",
      "To deploy a separate utility process alongside the service to handle cross-cutting concerns (e.g., logging, monitoring)",
      "To store the database credentials securely",
      "To manage the database migrations"
    ],
    "answer": "To deploy a separate utility process alongside the service to handle cross-cutting concerns (e.g., logging, monitoring)",
    "explanation": "The Sidecar pattern deploys helper components alongside the main service in the same container/pod. This abstracts infrastructure logic (like networking or observability) away from the application code.",
    "difficulty": "Intermediate"
  },
  {
    "id": 47,
    "question": "What trade-off is made when using an 'Eventual Consistency' model in a distributed database?",
    "options": [
      "High availability is sacrificed for immediate consistency",
      "Strong consistency is sacrificed to improve latency and availability",
      "Partition tolerance is sacrificed to reduce network overhead",
      "Durability is sacrificed to improve write performance"
    ],
    "answer": "Strong consistency is sacrificed to improve latency and availability",
    "explanation": "Eventual consistency allows the system to remain highly available and low-latency by accepting updates immediately, but there is a time window where replicas may hold stale data before converging.",
    "difficulty": "Intermediate"
  },
  {
    "id": 48,
    "question": "How does a 'Chord' DHT (Distributed Hash Table) primarily locate data in a peer-to-peer network?",
    "options": [
      "By broadcasting a query to all nodes in the network",
      "By using a centralized directory server",
      "By routing requests through a ring topology using finger tables",
      "By performing a SQL join operation on every node"
    ],
    "answer": "By routing requests through a ring topology using finger tables",
    "explanation": "Chord organizes nodes in a ring and assigns them IDs. It uses 'finger tables' (routing tables) to allow nodes to locate keys in O(log N) hops by forwarding the request to the node closest to the key.",
    "difficulty": "Intermediate"
  },
  {
    "id": 49,
    "question": "What is the primary technical challenge introduced by 'Distributed Transactions' (such as 2-Phase Commit) compared to local transactions?",
    "options": [
      "They are faster than local transactions",
      "They require all participating nodes to lock resources, potentially reducing availability and performance",
      "They do not support ACID properties",
      "They are simpler to debug and monitor"
    ],
    "answer": "They require all participating nodes to lock resources, potentially reducing availability and performance",
    "explanation": "Distributed transactions require coordination (locking and commitment) across multiple network nodes. This drastically increases latency and creates a single point of failureâ€”if the coordinator fails, resources remain locked.",
    "difficulty": "Intermediate"
  },
  {
    "id": 50,
    "question": "Why might an architect choose 'Range Partitioning' over 'Hash Partitioning' for a database table?",
    "options": [
      "To ensure perfectly even data distribution across all shards",
      "To support efficient range queries (e.g., 'select all users from date X to Y')",
      "To eliminate the need for a separate lookup service",
      "To allow for easier addition of new nodes"
    ],
    "answer": "To support efficient range queries (e.g., 'select all users from date X to Y')",
    "explanation": "Range partitioning keeps data with similar values together on the same shard. This allows for efficient retrieval of sequential data, whereas Hash Partitioning scatters sequential data randomly, making range scans expensive.",
    "difficulty": "Intermediate"
  },
  {
    "id": 51,
    "question": "What distinguishes a 'Reverse Proxy' from a standard 'Load Balancer'?",
    "options": [
      "A reverse proxy balances traffic at Layer 2; a load balancer works at Layer 7",
      "A reverse proxy terminates client connections and acts on behalf of the server; a load balancer simply forwards traffic",
      "A reverse proxy is only used for caching, while a load balancer is for scaling",
      "There is no technical difference"
    ],
    "answer": "A reverse proxy terminates client connections and acts on behalf of the server; a load balancer simply forwards traffic",
    "explanation": "While often overlapping in function, a Reverse Proxy specifically sits in front of a server and handles the server side of the connection (SSL termination, caching), whereas a Load Balancer focuses on distributing traffic across healthy targets.",
    "difficulty": "Intermediate"
  },
  {
    "id": 52,
    "question": "In a Publish/Subscribe (Pub/Sub) messaging system, what is the primary mechanism for ensuring fault tolerance if a message is not successfully processed by a subscriber?",
    "options": [
      "The publisher resends the message immediately",
      "Dead Letter Queues (DLQ)",
      "The subscriber crashes intentionally",
      "The message is dropped silently"
    ],
    "answer": "Dead Letter Queues (DLQ)",
    "explanation": "When a subscriber fails to process a message after a defined number of retries (due to bugs or transient errors), the message is moved to a Dead Letter Queue for later inspection or reprocessing, preventing infinite loops.",
    "difficulty": "Intermediate"
  },
  {
    "id": 53,
    "question": "What is the primary benefit of 'Connection Pooling' in a high-concurrency web application?",
    "options": [
      "It encrypts the database traffic",
      "It reduces the overhead of establishing and tearing down TCP connections",
      "It automatically scales the database storage",
      "It creates a new thread for every user request"
    ],
    "answer": "It reduces the overhead of establishing and tearing down TCP connections",
    "explanation": "Establishing a database connection is expensive (handshake, auth). Pooling maintains a set of open connections that applications can reuse, significantly reducing latency and CPU overhead compared to opening a new connection per request.",
    "difficulty": "Intermediate"
  },
  {
    "id": 54,
    "question": "Which strategy is most effective for preventing 'Cache Stampede' (also known as Dogpile Effect) when a popular cache entry expires?",
    "options": [
      "Increasing the cache size indefinitely",
      "Implementing a probabilistic early expiration or mutex lock for regeneration",
      "Setting the Time-To-Live (TTL) to 0",
      "Using a stronger hash function"
    ],
    "answer": "Implementing a probabilistic early expiration or mutex lock for regeneration",
    "explanation": "To prevent multiple threads from simultaneously detecting a miss and hitting the database, systems use a 'lock' or randomized early expiration so only one thread fetches the data while others wait for the cached result.",
    "difficulty": "Intermediate"
  },
  {
    "id": 55,
    "question": "What is 'Idempotency' in the context of REST API design, and why is it crucial for distributed systems?",
    "options": [
      "The ability of an API to handle multiple requests with the same parameters without changing the state beyond the first request",
      "The requirement that every API request must be unique",
      "The ability of a system to handle infinite traffic",
      "The process of encrypting API keys"
    ],
    "answer": "The ability of an API to handle multiple requests with the same parameters without changing the state beyond the first request",
    "explanation": "In unreliable networks, clients may retry requests. Idempotent methods (like PUT or DELETE with specific keys) ensure that retrying a request does not create duplicate data or corrupt state (e.g., charging a credit card twice).",
    "difficulty": "Intermediate"
  },
  {
    "id": 56,
    "question": "How does a 'Bloom Filter' optimize database lookups in a key-value store?",
    "options": [
      "It compresses the data before storage",
      "It acts as a probabilistic data structure to quickly check if an element is definitely NOT in a set",
      "It indexes all columns for faster searching",
      "It encrypts the keys for security"
    ],
    "answer": "It acts as a probabilistic data structure to quickly check if an element is definitely NOT in a set",
    "explanation": "A Bloom Filter is a memory-efficient probabilistic structure. If it says an item is absent, it is definitely absent. If it says it is present, it *might* be present. This saves expensive disk I/O for non-existent keys.",
    "difficulty": "Intermediate"
  },
  {
    "id": 57,
    "question": "What is the 'Quorum' mechanism in distributed databases regarding replication (W + R > Replication Factor)?",
    "options": [
      "A method to elect a new leader node",
      "A strategy to ensure that reads see the latest write by overlapping write and acknowledgement sets",
      "A way to compress network packets",
      "A backup strategy for cold storage"
    ],
    "answer": "A strategy to ensure that reads see the latest write by overlapping write and acknowledgement sets",
    "explanation": "If a system requires W writes and R reads to acknowledge, ensuring W + R > N (total replicas) guarantees that the read set overlaps with the write set, ensuring the client retrieves the latest data.",
    "difficulty": "Intermediate"
  },
  {
    "id": 58,
    "question": "What specific problem does 'Vertical Scaling' (Scale Up) eventually face that 'Horizontal Scaling' (Scale Out) does not?",
    "options": [
      "Network latency",
      "Hardware limitations (CPU/Memory) of a single machine",
      "Complexity of data consistency",
      "License costs per core"
    ],
    "answer": "Hardware limitations (CPU/Memory) of a single machine",
    "explanation": "Vertical scaling is limited by the maximum capacity of a single physical server. Horizontal scaling can theoretically grow indefinitely by adding more commodity machines, bypassing single-machine hardware limits.",
    "difficulty": "Intermediate"
  },
  {
    "id": 59,
    "question": "In system design, what is the primary function of an 'API Gateway'?",
    "options": [
      "To store user passwords",
      "To act as a single entry point for handling cross-cutting concerns like rate limiting, auth, and routing",
      "To serve as the primary database",
      "To replace the need for a load balancer"
    ],
    "answer": "To act as a single entry point for handling cross-cutting concerns like rate limiting, auth, and routing",
    "explanation": "An API Gateway sits between clients and backend services. It centralizes infrastructure logic such as request routing, authentication, SSL termination, and rate limiting, keeping microservices lightweight.",
    "difficulty": "Intermediate"
  },
  {
    "id": 60,
    "question": "Why is a 'Long-Polling' mechanism generally more resource-efficient for the server than 'Short-Polling'?",
    "options": [
      "It closes the connection immediately after a request",
      "It keeps the connection open until data is available, reducing the frequency of empty requests",
      "It uses UDP instead of TCP",
      "It requires the client to initiate a new connection every second"
    ],
    "answer": "It keeps the connection open until data is available, reducing the frequency of empty requests",
    "explanation": "Short-polling involves the client repeatedly asking 'got data?', creating high request churn. Long-polling holds the connection until the server has a message or a timeout, significantly reducing the number of empty network round-trips.",
    "difficulty": "Intermediate"
  },
  {
    "id": 61,
    "question": "What is the main disadvantage of using a 'Master-Master' (Multi-Master) database replication configuration?",
    "options": [
      "Read performance is slower than Master-Slave",
      "Complexity in handling conflict resolution when two nodes update the same data concurrently",
      "It does not support automatic failover",
      "Data is not replicated synchronously"
    ],
    "answer": "Complexity in handling conflict resolution when two nodes update the same data concurrently",
    "explanation": "In Multi-Master setups, both nodes can accept writes. If the same ID is updated simultaneously on different nodes, the system must use complex conflict resolution (timestamps, vector clocks) to merge data, which adds significant application complexity.",
    "difficulty": "Intermediate"
  },
  {
    "id": 62,
    "question": "When comparing SQL and NoSQL databases, what characteristic of 'Wide-Column Stores' (like Cassandra) makes them suitable for high write throughput?",
    "options": [
      "They support complex ACID transactions",
      "They use a Log-Structured Merge (LSM) tree structure that optimizes sequential writes",
      "They store data in a normalized schema",
      "They rely entirely on in-memory storage"
    ],
    "answer": "They use a Log-Structured Merge (LSM) tree structure that optimizes sequential writes",
    "explanation": "LSM trees buffer writes in memory and periodically flush them as immutable sorted files (SSTables) to disk sequentially. This avoids random disk I/O, allowing for significantly higher write speeds compared to B-Trees.",
    "difficulty": "Intermediate"
  },
  {
    "id": 63,
    "question": "What is the primary function of a 'Heartbeat' signal in a distributed cluster?",
    "options": [
      "To synchronize data between nodes",
      "To broadcast user requests to all servers",
      "To detect node failures or liveness by periodic signaling",
      "To balance the CPU load"
    ],
    "answer": "To detect node failures or liveness by periodic signaling",
    "explanation": "Nodes send periodic 'heartbeat' messages to a monitor or peers. If the heartbeat stops, the system assumes the node has failed and triggers health checks or failover procedures to maintain availability.",
    "difficulty": "Intermediate"
  },
  {
    "id": 64,
    "question": "How does 'Content Delivery Network' (CDN) caching improve performance for static assets?",
    "options": [
      "By compressing the database files",
      "By serving content from edge servers geographically closer to the user",
      "By dynamically generating HTML on the fly",
      "By encrypting the connection using SSL/TLS"
    ],
    "answer": "By serving content from edge servers geographically closer to the user",
    "explanation": "CDNs cache static assets (images, CSS, JS) in Point of Presence (PoP) servers located globally. This reduces the Round Trip Time (RTT) and bandwidth costs by serving data from the nearest edge rather than the origin server.",
    "difficulty": "Intermediate"
  },
  {
    "id": 65,
    "question": "What is 'Throughput' in the context of system performance metrics?",
    "options": [
      "The time taken to process a single request",
      "The number of requests or units of work processed by a system per unit of time",
      "The total number of errors encountered",
      "The percentage of time the system is operational"
    ],
    "answer": "The number of requests or units of work processed by a system per unit of time",
    "explanation": "Throughput measures the rate of processing (e.g., Requests Per Second). High throughput indicates the system can handle a large volume of work, whereas low latency refers to the speed of a single unit of work.",
    "difficulty": "Intermediate"
  },
  {
    "id": 66,
    "question": "In the context of storage systems, what is the 'RAID 5' configuration primarily known for balancing?",
    "options": [
      "Performance and cost",
      "Data redundancy and storage efficiency via distributed parity",
      "Maximum read speed and write speed equally",
      "Full mirroring of data"
    ],
    "answer": "Data redundancy and storage efficiency via distributed parity",
    "explanation": "RAID 5 stripes data and parity blocks across all disks. It provides fault tolerance for a single drive failure while utilizing more usable storage capacity than RAID 1 (mirroring), offering a balance between safety and efficiency.",
    "difficulty": "Intermediate"
  },
  {
    "id": 67,
    "question": "Why is 'Stateless' architecture preferred for horizontal scaling of web servers?",
    "options": [
      "Stateless servers require more memory",
      "It allows any server to handle any request without relying on local session context",
      "Stateful servers cannot be connected to a load balancer",
      "It is easier to debug stateful systems"
    ],
    "answer": "It allows any server to handle any request without relying on local session context",
    "explanation": "If servers are stateless, incoming requests can be routed to any available instance. If servers maintain local state (sessions), a load balancer must implement 'sticky sessions', which complicates scaling and recovery.",
    "difficulty": "Intermediate"
  },
  {
    "id": 68,
    "question": "What is the trade-off when using 'Strong Consistency' in a globally distributed database?",
    "options": [
      "Higher availability and lower latency",
      "Increased latency and potential unavailability during partitions",
      "Lower storage costs",
      "Simpler application code"
    ],
    "answer": "Increased latency and potential unavailability during partitions",
    "explanation": "Strong consistency requires synchronous replication or locking across nodes before a write is acknowledged. This forces the system to wait for network round-trips between data centers, increasing latency and reducing availability per CAP theorem.",
    "difficulty": "Intermediate"
  },
  {
    "id": 69,
    "question": "What is the purpose of 'Decommissioning' a node in a distributed storage system that uses consistent hashing?",
    "options": [
      "To permanently delete all data in the system",
      "To redistribute the data assigned to that node to other nodes in the cluster",
      "To upgrade the CPU of the node",
      "To change the hashing algorithm"
    ],
    "answer": "To redistribute the data assigned to that node to other nodes in the cluster",
    "explanation": "When a node is removed, the consistent hashing ring shifts, and the data ranges previously owned by that node are reassigned to its adjacent neighbors. This ensures data is not lost and remains available.",
    "difficulty": "Intermediate"
  },
  {
    "id": 70,
    "question": "In an Event Sourcing architecture, how is the current state of an entity reconstructed?",
    "options": [
      "By querying a single 'Current State' table",
      "By replaying the sequence of historical events for that entity",
      "By reading from a backup replica",
      "By recalculating the state from the UI input"
    ],
    "answer": "By replaying the sequence of historical events for that entity",
    "explanation": "Event Sourcing stores state changes as a log of immutable events. To get the current state, the system reads all events related to the entity ID and replays them sequentially to compute the final state.",
    "difficulty": "Intermediate"
  },
  {
    "id": 71,
    "question": "In a system using the 'Two-Phase Commit' (2PC) protocol for distributed transactions, what is the primary risk if the coordinator node fails permanently during the 'Ready' phase?",
    "options": [
      "All participating transactions will be rolled back automatically",
      "Participants that voted 'Yes' will remain blocked, holding locks indefinitely until the coordinator recovers",
      "The system will immediately promote a new coordinator using the Raft consensus algorithm",
      "Participating nodes will proceed with the commit using a consensus of their local logs"
    ],
    "answer": "Participants that voted 'Yes' will remain blocked, holding locks indefinitely until the coordinator recovers",
    "explanation": "2PC is a blocking protocol. If the coordinator fails after sending prepare-votes, participants must wait for the final decision, causing resource locks and halting progress.",
    "difficulty": "Advanced"
  },
  {
    "id": 72,
    "question": "When comparing LSM Trees (Log-Structured Merge-trees) to B-Trees for write-heavy workloads, which characteristic is an inherent advantage of the LSM Tree architecture?",
    "options": [
      "Faster read performance due to lower read amplification",
      "Random writes are converted to sequential writes, significantly improving write throughput",
      "Point lookups do not require checking multiple data structures (memtable and SSTables)",
      "No background compaction processes are required, reducing CPU overhead"
    ],
    "answer": "Random writes are converted to sequential writes, significantly improving write throughput",
    "explanation": "LSM trees buffer writes in memory (MemTable) and flush them as immutable sorted files (SSTables), turning expensive random disk I/O into efficient sequential I/O.",
    "difficulty": "Advanced"
  },
  {
    "id": 73,
    "question": "What specific problem does the introduction of 'Virtual Nodes' (vnodes) solve in a Consistent Hashing ring?",
    "options": [
      "It reduces the network latency between the client and the server",
      "It ensures a more uniform distribution of data and minimizes data movement when nodes are added or removed",
      "It eliminates the need for a replication factor greater than one",
      "It allows the hash function to be reversible"
    ],
    "answer": "It ensures a more uniform distribution of data and minimizes data movement when nodes are added or removed",
    "explanation": "Without vnodes, a small number of physical nodes can result in imbalanced load ranges. Vnodes randomize the distribution, ensuring smooth rebalancing.",
    "difficulty": "Advanced"
  },
  {
    "id": 74,
    "question": "In the context of the CAP theorem, what does the PACELC theorem state regarding a system that sacrifices Consistency for Availability during a Partition (A)?",
    "options": [
      "It must sacrifice Latency for Consistency during normal operation (C)",
      "It must sacrifice Consistency for Latency during normal operation (L)",
      "It automatically becomes a CP system during failure",
      "It cannot achieve high availability without replicas"
    ],
    "answer": "It must sacrifice Consistency for Latency during normal operation (L)",
    "explanation": "PACELC extends CAP: if Partitioned, trade off Availability vs Consistency; Else (E), trade off Latency vs Consistency. An AP system (no partition) typically optimizes for latency over consistency.",
    "difficulty": "Advanced"
  },
  {
    "id": 75,
    "question": "What is the primary trade-off when using a 'Chatty' (fine-grained) interaction pattern between microservices compared to a 'Chunky' (coarse-grained) pattern?",
    "options": [
      "Increased network latency and potential reliability issues due to multiple round-trips",
      "Decreased encapsulation of business logic within service boundaries",
      "Inability to reuse services across different domains",
      "Stronger consistency guarantees between distributed databases"
    ],
    "answer": "Increased network latency and potential reliability issues due to multiple round-trips",
    "explanation": "Chatty interactions require multiple network calls to complete a single logical transaction, increasing latency and the surface area for network failures.",
    "difficulty": "Advanced"
  },
  {
    "id": 76,
    "question": "Which algorithm is most suitable for implementing a rate limiter that allows traffic bursts but strictly enforces a long-term average rate?",
    "options": [
      "Fixed Window Counter",
      "Leaky Bucket",
      "Token Bucket",
      "Sliding Window Log"
    ],
    "answer": "Token Bucket",
    "explanation": "The Token Bucket algorithm allows bursts up to the bucket capacity (max tokens) while replenishing tokens at a fixed rate, effectively handling both bursts and average rate limiting.",
    "difficulty": "Advanced"
  },
  {
    "id": 77,
    "question": "In a distributed database using Leader-based Replication (e.g., Raft), what is the 'Log Replication' factor required to guarantee that a committed entry is not lost during a failover?",
    "options": [
      "Written to the leader's disk only",
      "Written to a majority (N/2 + 1) of nodes",
      "Written to all follower nodes asynchronously",
      "Written to at least one follower synchronously"
    ],
    "answer": "Written to a majority (N/2 + 1) of nodes",
    "explanation": "Consensus protocols like Raft define a commit as being successfully replicated to a majority of the cluster. This ensures durability even if the leader fails.",
    "difficulty": "Advanced"
  },
  {
    "id": 78,
    "question": "What is the primary function of the 'Sidecar' pattern in a service mesh architecture?",
    "options": [
      "To act as a primary database replication agent for the service",
      "To abstract network communication features (retries, circuit breaking, telemetry) into a separate process alongside the service",
      "To serve static assets and front-end content for the microservice",
      "To compile and build the microservice code during deployment"
    ],
    "answer": "To abstract network communication features (retries, circuit breaking, telemetry) into a separate process alongside the service",
    "explanation": "The sidecar pattern deploys a helper utility (proxy) alongside the service to handle infrastructure concerns like traffic management, keeping the application logic clean.",
    "difficulty": "Advanced"
  },
  {
    "id": 79,
    "question": "Why is 'Write Amplification' a critical metric when selecting storage engines for SSDs (Solid State Drives)?",
    "options": [
      "High write amplification increases the read IOPS latency",
      "SSDs have a limited number of program/erase cycles; high write amplification wears out the drive faster",
      "Write amplification causes data corruption in distributed file systems",
      "It prevents the usage of WAL (Write-Ahead Logging) protocols"
    ],
    "answer": "SSDs have a limited number of program/erase cycles; high write amplification wears out the drive faster",
    "explanation": "Write amplification means writing more physical data than logical data requested. Since SSD blocks degrade with every write, high amplification significantly reduces hardware lifespan.",
    "difficulty": "Advanced"
  },
  {
    "id": 80,
    "question": "In a 'Pub-Sub' system using a 'Topic' exchange, how does the system generally ensure 'At-Least-Once' delivery guarantees?",
    "options": [
      "By ignoring consumer acknowledgments to maximize throughput",
      "By persisting messages and only deleting them after an acknowledgment from the consumer",
      "By using UDP protocol to broadcast messages instantly",
      "By requiring every consumer to process the message within 1 second"
    ],
    "answer": "By persisting messages and only deleting them after an acknowledgment from the consumer",
    "explanation": "To ensure delivery, brokers persist messages. If a consumer fails to acknowledge (crash/bug), the broker redelivers the message, ensuring it is processed at least once.",
    "difficulty": "Advanced"
  },
  {
    "id": 81,
    "question": "What is 'Head-of-Line Blocking' in the context of HTTP/1.1 versus HTTP/2?",
    "options": [
      "A DoS attack that targets the load balancer's queue",
      "A phenomenon in HTTP/1.1 where a slow request blocks subsequent requests on the same TCP connection",
      "A database lock contention issue where the first transaction locks the header row",
      "The delay caused by the TLS handshake establishing the connection"
    ],
    "answer": "A phenomenon in HTTP/1.1 where a slow request blocks subsequent requests on the same TCP connection",
    "explanation": "HTTP/1.1 pipelining allows requests to be sent in parallel, but responses must be returned in order. A slow first response blocks later ones, resolved in HTTP/2 via multiplexing.",
    "difficulty": "Advanced"
  },
  {
    "id": 82,
    "question": "What is the fundamental difference between 'Orchestration' and 'Choreography' in a microservices architecture?",
    "options": [
      "Orchestration uses a central controller to manage logic, while Choreography relies on autonomous services reacting to events",
      "Orchestration uses asynchronous messaging, while Choreography uses synchronous RPC calls",
      "Orchestration is only applicable to monolithic applications, while Choreography is for microservices",
      "Orchestration requires a shared database, while Choreography requires database-per-service"
    ],
    "answer": "Orchestration uses a central controller to manage logic, while Choreography relies on autonomous services reacting to events",
    "explanation": "Orchestration centralizes decision logic (conductor). Choreography decentralizes it, where services listen for events and decide independently what to do.",
    "difficulty": "Advanced"
  },
  {
    "id": 83,
    "question": "When designing a globally distributed system, why might you choose 'Multi-Leader' replication over 'Leader-Based' replication?",
    "options": [
      "To avoid complex conflict resolution and guarantee strong consistency",
      "To allow writes to the nearest data center, reducing write latency and improving availability",
      "To simplify backup and recovery processes",
      "To eliminate the need for load balancers"
    ],
    "answer": "To allow writes to the nearest data center, reducing write latency and improving availability",
    "explanation": "Multi-leader allows accepting writes in multiple regions locally. The trade-off is the significant complexity of handling write conflicts (data convergence) asynchronously.",
    "difficulty": "Advanced"
  },
  {
    "id": 84,
    "question": "How does a 'Bloom Filter' optimize the read performance of a key-value store like Cassandra or HBase?",
    "options": [
      "It compresses the data on disk to reduce I/O bandwidth",
      "It acts as a probabilistic set membership filter to avoid unnecessary disk reads for non-existent keys",
      "It indexes the data to allow range scans",
      "It encrypts the keys to secure the data at rest"
    ],
    "answer": "It acts as a probabilistic set membership filter to avoid unnecessary disk reads for non-existent keys",
    "explanation": "Before searching disk-heavy SSTables, the Bloom Filter quickly checks if a key *might* exist. If it says 'No', the key is definitely not there, saving expensive I/O.",
    "difficulty": "Advanced"
  },
  {
    "id": 85,
    "question": "What is the 'Split-Brain' problem in distributed systems, and how is it typically mitigated?",
    "options": [
      "It occurs when two nodes disagree on data; mitigated by using a strong hash algorithm",
      "It occurs when a network partition creates two independent leaders; mitigated by a quorum or fencing token",
      "It occurs when a client sends requests to both read and write replicas; mitigated by sticky sessions",
      "It occurs when the cache expires; mitigated by write-through caching"
    ],
    "answer": "It occurs when a network partition creates two independent leaders; mitigated by a quorum or fencing token",
    "explanation": "Split-brain happens when nodes lose contact and both assume leadership, causing data divergence. Quorum (majority vote) prevents both from having valid authority to commit writes.",
    "difficulty": "Advanced"
  },
  {
    "id": 86,
    "question": "In the context of API Gateway patterns, what is the purpose of 'Rate Limiting' at the gateway level versus the service level?",
    "options": [
      "Service level prevents DDoS; Gateway level prevents database overload",
      "Gateway level protects the entire infrastructure from overload and enforces global quotas, while service level protects specific resources",
      "Gateway level is used for authentication, while service level is used for authorization",
      "There is no difference; they perform the exact same function"
    ],
    "answer": "Gateway level protects the entire infrastructure from overload and enforces global quotas, while service level protects specific resources",
    "explanation": "Gateway-level limiting acts as a first line of defense for the system as a whole (e.g., API caps), whereas service-level limits manage specific resource consumption (e.g., DB connections).",
    "difficulty": "Advanced"
  },
  {
    "id": 87,
    "question": "What is the primary distinction between 'Sharding' and 'Horizontal Scaling'?",
    "options": [
      "Sharding refers to the distribution of data, while Horizontal Scaling refers to the addition of compute nodes",
      "Horizontal Scaling is only applicable to stateless services, while Sharding is for databases",
      "Sharding is a type of Vertical Scaling",
      "Horizontal Scaling increases single-node power; Sharding adds more nodes"
    ],
    "answer": "Sharding refers to the distribution of data, while Horizontal Scaling refers to the addition of compute nodes",
    "explanation": "While related, sharding is a specific technique to distribute data across nodes. Horizontal scaling is the general strategy of adding more nodes (compute or data) to increase capacity.",
    "difficulty": "Advanced"
  },
  {
    "id": 88,
    "question": "In 'Event Sourcing', what is the role of a 'Snapshot'?",
    "options": [
      "To back up the entire database to a cold storage solution",
      "To store the current state of an entity to avoid replaying the entire event stream for reading",
      "To encrypt sensitive events in the log",
      "To capture the user interface state at the time of the click"
    ],
    "answer": "To store the current state of an entity to avoid replaying the entire event stream for reading",
    "explanation": "Replaying thousands of events to build an aggregate state is slow. Snapshots persist the state at specific versions (e.g., every 100 events) to optimize read performance.",
    "difficulty": "Advanced"
  },
  {
    "id": 89,
    "question": "What is 'Write Skew' in the context of database isolation levels?",
    "options": [
      "A phenomenon where two concurrent transactions update the same row simultaneously",
      "A phenomenon where two transactions read overlapping data sets and concurrently make disjoint updates that violate a constraint when combined",
      "The delay between writing to the primary and replicating to the secondary",
      "A data corruption error caused by disk failure"
    ],
    "answer": "A phenomenon where two transactions read overlapping data sets and concurrently make disjoint updates that violate a constraint when combined",
    "explanation": "Write skew occurs in 'Repeatable Read' (or Snapshot Isolation) where the database checks for conflicts on the *same* row, but misses logic spanning multiple rows (e.g., shift scheduling rosters).",
    "difficulty": "Advanced"
  },
  {
    "id": 90,
    "question": "Which load balancing algorithm is most appropriate if you need to guarantee that a specific user always hits the same backend server, provided that server is healthy?",
    "options": [
      "Round Robin",
      "Least Connections",
      "IP Hash (or consistent hashing based on source IP)",
      "Random"
    ],
    "answer": "IP Hash (or consistent hashing based on source IP)",
    "explanation": "Hashing the source IP (or session ID) ensures that the request maps to the same server every time, which is required for sticky sessions unless session state is externalized.",
    "difficulty": "Advanced"
  },
  {
    "id": 91,
    "question": "Why are 'Covering Indexes' preferred in SQL databases for high-read scenarios?",
    "options": [
      "They lock the entire table to prevent concurrent updates",
      "They contain all columns required by the query, allowing the database to retrieve results from the index alone without looking up the table data",
      "They automatically compress the data to save space",
      "They allow the query to bypass the transaction log"
    ],
    "answer": "They contain all columns required by the query, allowing the database to retrieve results from the index alone without looking up the table data",
    "explanation": "If an index contains all queried columns (SELECT, JOIN, WHERE), the optimizer performs an 'Index Only Scan', skipping the expensive table lookup (heap access).",
    "difficulty": "Advanced"
  },
  {
    "id": 92,
    "question": "What is the primary benefit of 'Geographically Partitioned' sharding compared to 'Range-based' sharding?",
    "options": [
      "It allows queries to be served locally to a region, minimizing latency",
      "It guarantees an even distribution of data regardless of user geography",
      "It simplifies the process of re-balancing shards when nodes are added",
      "It eliminates the need for a global directory service"
    ],
    "answer": "It allows queries to be served locally to a region, minimizing latency",
    "explanation": "Geo-partitioning locates data physically closer to the users who own it. While distribution might be uneven, the latency reduction for geo-specific queries is the primary advantage.",
    "difficulty": "Advanced"
  },
  {
    "id": 93,
    "question": "In the 'Saga' pattern for distributed transactions, what is the role of a 'Compensating Transaction'?",
    "options": [
      "To retry the original transaction if it fails due to a transient error",
      "To undo the changes made by a preceding transaction in the saga to maintain data consistency across services",
      "To commit the transaction to the database after all participants agree",
      "To notify the user that the transaction has failed"
    ],
    "answer": "To undo the changes made by a preceding transaction in the saga to maintain data consistency across services",
    "explanation": "Sagas lack atomic locks. If a step fails, the system runs compensating transactions (logical rollbacks) for all completed steps to revert the state.",
    "difficulty": "Advanced"
  },
  {
    "id": 94,
    "question": "What is the 'Read Repair' mechanism in Dynamo-style databases (eventually consistent)?",
    "options": [
      "A process that reads from the disk and fixes bad sectors",
      "A background anti-entropy mechanism that updates stale replicas during a read operation",
      "A method to rewrite the SQL query to optimize execution time",
      "A synchronization tool used only during database backups"
    ],
    "answer": "A background anti-entropy mechanism that updates stale replicas during a read operation",
    "explanation": "When a client reads data, the coordinator contacts multiple replicas. If versions differ, it sends the latest version to the stale nodes to reconcile them.",
    "difficulty": "Advanced"
  },
  {
    "id": 95,
    "question": "What is the 'Phi Accrual Failure Detector' used in distributed systems like Akka or Cassandra?",
    "options": [
      "To detect SQL injection attacks",
      "To calculate a suspicion level (phi) based on network latency history rather than simple heartbeats",
      "To accrue financial transaction logs for auditing",
      "To detect memory leaks in the JVM"
    ],
    "answer": "To calculate a suspicion level (phi) based on network latency history rather than simple heartbeats",
    "explanation": "Unlike binary heartbeats, phi accrual interprets heartbeat intervals over time to generate a continuous suspicion score, making it resilient to temporary network glitches.",
    "difficulty": "Advanced"
  },
  {
    "id": 96,
    "question": "Why is UDP preferred over TCP for real-time streaming protocols like WebRTC or some live video streams?",
    "options": [
      "UDP guarantees packet delivery which is required for video",
      "UDP's lack of error correction and retransmission reduces latency and jitter",
      "UDP is inherently encrypted",
      "UDP supports larger packet sizes than TCP"
    ],
    "answer": "UDP's lack of error correction and retransmission reduces latency and jitter",
    "explanation": "In real-time streaming, late packets are useless. Retransmitting lost packets (TCP behavior) increases latency. UDP prioritizes speed and continuous flow over perfection.",
    "difficulty": "Advanced"
  },
  {
    "id": 97,
    "question": "What is 'Lease' based locking compared to a simple 'Lock' with TTL?",
    "options": [
      "Lease requires the holder to periodically renew the grant, preventing deadlock if the holder crashes",
      "Lock is persistent while Lease is transient",
      "Lease allows multiple writers concurrently",
      "Lock is stored in RAM while Lease is stored on disk"
    ],
    "answer": "Lease requires the holder to periodically renew the grant, preventing deadlock if the holder crashes",
    "explanation": "A simple TTL lock expires regardless of activity. A Lease must be actively kept alive (heartbeat); if the process crashes, it stops renewing and the lock is released immediately.",
    "difficulty": "Advanced"
  },
  {
    "id": 98,
    "question": "What is the 'HyperLogLog' (HLL) data structure used for in system design?",
    "options": [
      "To store graph relationships efficiently",
      "To estimate the cardinality of large sets (count distinct items) with minimal memory",
      "To compress text documents for full-text search",
      "To prioritize messages in a task queue"
    ],
    "answer": "To estimate the cardinality of large sets (count distinct items) with minimal memory",
    "explanation": "HLL uses hash observations and probabilistic math to count unique items (e.g., Daily Active Users) with roughly 1-2% error using only ~12kb of memory, regardless of set size.",
    "difficulty": "Advanced"
  },
  {
    "id": 99,
    "question": "How does 'QUIC' (HTTP/3) improve performance over TCP+TLS (HTTP/2) during connection establishment?",
    "options": [
      "It compresses the headers more efficiently",
      "It combines the cryptographic handshake with the transport handshake, reducing latency",
      "It uses multiple TCP connections to speed up data transfer",
      "It eliminates the need for UDP"
    ],
    "answer": "It combines the cryptographic handshake with the transport handshake, reducing latency",
    "explanation": "QUIC runs over UDP. It integrates the TLS handshake into its own setup packets, reducing connection setup rounds from 3 (TCP + TLS) to typically 1 or 2.",
    "difficulty": "Advanced"
  },
  {
    "id": 100,
    "question": "What is the 'Tail Latency' (or P99 latency) specifically measuring in a system?",
    "options": [
      "The average response time of the system",
      "The maximum response time observed over a long period",
      "The response time experienced by the slowest 1% of requests",
      "The time taken for the network packet to travel from the client to the server"
    ],
    "answer": "The response time experienced by the slowest 1% of requests",
    "explanation": "Tail latency (often P99 or P99.9) focuses on the outliers. Optimizing for the average (P50) ignores a significant portion of users who experience poor performance.",
    "difficulty": "Advanced"
  }
]