[
  {
    "id": 1,
    "question": "What is the worst-case time complexity of searching for an element in an unsorted array of size n?",
    "options": [
      "O(1)",
      "O(log n)",
      "O(n)",
      "O(n^2)"
    ],
    "answer": "O(n)",
    "explanation": "In an unsorted array, there is no correlation between element values and indices, requiring a linear scan in the worst case. O(log n) applies only to sorted structures using binary search.",
    "difficulty": "Beginner"
  },
  {
    "id": 2,
    "question": "Which data structure operates on the Last-In, First-Out (LIFO) principle?",
    "options": [
      "Queue",
      "Stack",
      "Linked List",
      "Priority Queue"
    ],
    "answer": "Stack",
    "explanation": "A Stack allows insertion and removal only from the top, meaning the last element added is the first one removed. Queues use FIFO, while Linked Lists and Priority Queues allow more flexible access.",
    "difficulty": "Beginner"
  },
  {
    "id": 3,
    "question": "What is the primary prerequisite for applying the standard Binary Search algorithm to a data structure?",
    "options": [
      "The data structure must be a linked list",
      "The data structure must be sorted",
      "The data structure must have a fixed size",
      "The data structure must contain unique elements"
    ],
    "answer": "The data structure must be sorted",
    "explanation": "Binary search relies on eliminating half of the remaining search space based on the sorted order of elements. It works with duplicates; unique elements are not required.",
    "difficulty": "Beginner"
  },
  {
    "id": 4,
    "question": "What is the time complexity of accessing an element at a specific index in a standard array?",
    "options": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n log n)"
    ],
    "answer": "O(1)",
    "explanation": "Arrays provide random access to elements via direct index calculation using pointer arithmetic. Linked lists require O(n) traversal to reach a specific index.",
    "difficulty": "Beginner"
  },
  {
    "id": 5,
    "question": "In Big-O notation, which complexity represents an algorithm that divides the problem size by 2 in every step?",
    "options": [
      "O(n)",
      "O(log n)",
      "O(n^2)",
      "O(2^n)"
    ],
    "answer": "O(log n)",
    "explanation": "Dividing the problem size by a constant factor (e.g., 2) repeatedly results in logarithmic growth. O(n) implies linear reduction, while O(2^n) implies exponential growth.",
    "difficulty": "Beginner"
  },
  {
    "id": 6,
    "question": "Which sorting algorithm has a worst-case time complexity of O(n^2) but is often used for small datasets due to low overhead?",
    "options": [
      "Merge Sort",
      "Quick Sort",
      "Bubble Sort",
      "Heap Sort"
    ],
    "answer": "Bubble Sort",
    "explanation": "Bubble Sort repeatedly swaps adjacent elements, resulting in O(n^2) comparisons. Merge Sort and Heap Sort are O(n log n), and while Quick Sort is O(n^2) worst case, its average case is O(n log n).",
    "difficulty": "Beginner"
  },
  {
    "id": 7,
    "question": "What is the function of a 'Base Case' in a recursive algorithm?",
    "options": [
      "To optimize memory usage",
      "To stop the recursion and prevent infinite loops",
      "To increase the execution speed",
      "To define the input size"
    ],
    "answer": "To stop the recursion and prevent infinite loops",
    "explanation": "Without a base case, a recursive function would call itself indefinitely, eventually causing a stack overflow. Optimization and speed are secondary concerns to termination.",
    "difficulty": "Beginner"
  },
  {
    "id": 8,
    "question": "Which traversal method visits nodes in a binary tree layer-by-layer (level-order)?",
    "options": [
      "Depth-First Search (DFS)",
      "In-order Traversal",
      "Breadth-First Search (BFS)",
      "Post-order Traversal"
    ],
    "answer": "Breadth-First Search (BFS)",
    "explanation": "BFS uses a queue to explore neighbors before moving deeper, resulting in level-order traversal. DFS variants (In-order, Pre-order, Post-order) go deep before wide.",
    "difficulty": "Beginner"
  },
  {
    "id": 9,
    "question": "Which data structure is most suitable for implementing a Least Recently Used (LRU) Cache?",
    "options": [
      "Array",
      "Hash Map Only",
      "Doubly Linked List + Hash Map",
      "Stack"
    ],
    "answer": "Doubly Linked List + Hash Map",
    "explanation": "A Hash Map provides O(1) access to cache items, while a Doubly Linked List allows O(1) reordering of nodes for recency. Arrays or Stacks cannot efficiently reorder arbitrary elements.",
    "difficulty": "Beginner"
  },
  {
    "id": 10,
    "question": "What is the space complexity of standard Depth-First Search (DFS) on a graph in the worst case?",
    "options": [
      "O(V)",
      "O(E)",
      "O(V + E)",
      "O(log V)"
    ],
    "answer": "O(V)",
    "explanation": "DFS space complexity is determined by the recursion stack or explicit stack storing vertices. In the worst case (a straight line graph), the stack stores all V vertices.",
    "difficulty": "Beginner"
  },
  {
    "id": 11,
    "question": "Which property defines a 'stable' sorting algorithm?",
    "options": [
      "It always runs in O(n log n)",
      "It preserves the relative order of equal elements",
      "It requires no extra memory space",
      "It works on any data type"
    ],
    "answer": "It preserves the relative order of equal elements",
    "explanation": "Stability means that if two elements are equal, their order in the sorted output matches their order in the input. Merge Sort is stable; standard Quick Sort is not.",
    "difficulty": "Beginner"
  },
  {
    "id": 12,
    "question": "Which technique involves storing the results of expensive function calls and returning the cached result when the same inputs occur again?",
    "options": [
      "Greedy Approach",
      "Memoization",
      "Divide and Conquer",
      "Backtracking"
    ],
    "answer": "Memoization",
    "explanation": "Memoization is a top-down optimization technique used primarily in dynamic programming to avoid redundant calculations. Greedy approaches make local choices without storing sub-problems.",
    "difficulty": "Beginner"
  },
  {
    "id": 13,
    "question": "In a Min-Heap data structure, where is the minimum element always located?",
    "options": [
      "At the last leaf node",
      "At the root node",
      "In the leftmost branch",
      "It requires a linear search to find"
    ],
    "answer": "At the root node",
    "explanation": "A Min-Heap maintains the heap property where the parent is always smaller than its children, ensuring the smallest element is at the top (root).",
    "difficulty": "Beginner"
  },
  {
    "id": 14,
    "question": "What is the worst-case time complexity of inserting an element into a standard (unbalanced) Binary Search Tree (BST)?",
    "options": [
      "O(1)",
      "O(log n)",
      "O(n)",
      "O(n log n)"
    ],
    "answer": "O(n)",
    "explanation": "If elements are inserted in sorted order, a BST degenerates into a linked list (skewed tree), requiring O(n) time to traverse the height. Balanced BSTs maintain O(log n).",
    "difficulty": "Beginner"
  },
  {
    "id": 15,
    "question": "Which graph representation is most space-efficient for a sparse graph?",
    "options": [
      "Adjacency Matrix",
      "Adjacency List",
      "Edge List only",
      "Incidence Matrix"
    ],
    "answer": "Adjacency List",
    "explanation": "Adjacency Lists store only existing edges, using space proportional to O(V + E). An Adjacency Matrix uses O(V^2) space regardless of edge count, which is inefficient for sparse graphs.",
    "difficulty": "Beginner"
  },
  {
    "id": 16,
    "question": "What is the primary purpose of Dijkstra's algorithm?",
    "options": [
      "To detect negative weight cycles",
      "To find the shortest path from a single source to all other nodes in a graph with non-negative weights",
      "To find the Minimum Spanning Tree",
      "To traverse a graph in topological order"
    ],
    "answer": "To find the shortest path from a single source to all other nodes in a graph with non-negative weights",
    "explanation": "Dijkstra is a greedy shortest-path algorithm for graphs with non-negative edge weights. Bellman-Ford handles negative weights (if no negative cycles), and Prim/Kruskal find MSTs.",
    "difficulty": "Beginner"
  },
  {
    "id": 17,
    "question": "Which of the following operations is NOT supported efficiently by a standard Singly Linked List?",
    "options": [
      "Insertion at head",
      "Deletion of a node given the pointer to it",
      "Random access to the k-th element",
      "Traversal"
    ],
    "answer": "Random access to the k-th element",
    "explanation": "Linked Lists require sequential traversal from the head to reach the k-th element, taking O(k) or O(n) time. Arrays support O(1) random access.",
    "difficulty": "Beginner"
  },
  {
    "id": 18,
    "question": "In algorithm analysis, what does the term 'Amortized Analysis' refer to?",
    "options": [
      "Analyzing the average performance over all possible inputs",
      "Analyzing the total time per operation averaged over a worst-case sequence of operations",
      "Analyzing the algorithm using random inputs",
      "Analyzing only the best-case scenario"
    ],
    "answer": "Analyzing the total time per operation averaged over a worst-case sequence of operations",
    "explanation": "Amortized analysis guarantees the average performance of each operation in the *worst case* over a sequence (e.g., dynamic array resizing). It differs from average-case analysis which assumes probability distributions.",
    "difficulty": "Beginner"
  },
  {
    "id": 19,
    "question": "Which bit manipulation operation checks if a specific bit is set (1) in an integer?",
    "options": [
      "Bitwise OR",
      "Bitwise AND with a mask",
      "Bitwise XOR",
      "Left Shift"
    ],
    "answer": "Bitwise AND with a mask",
    "explanation": "Performing `n & (1 << k)` isolates the k-th bit. If the result is non-zero, the bit is set. OR and XOR modify bits or toggle them, while Left Shift moves them.",
    "difficulty": "Beginner"
  },
  {
    "id": 20,
    "question": "What is the output of the bitwise operation `5 ^ 3` (5 XOR 3)?",
    "options": [
      "1",
      "2",
      "6",
      "8"
    ],
    "answer": "6",
    "explanation": "5 is 101, 3 is 011. XOR outputs 1 where bits differ: 101 ^ 011 = 110 (which is 6).",
    "difficulty": "Beginner"
  },
  {
    "id": 21,
    "question": "Which sorting algorithm is based on the Divide and Conquer strategy but is NOT a comparison sort?",
    "options": [
      "Merge Sort",
      "Quick Sort",
      "Radix Sort",
      "Heap Sort"
    ],
    "answer": "Radix Sort",
    "explanation": "Radix Sort sorts by processing individual digits or bits, not by comparing elements against each other. Merge, Quick, and Heap sorts rely on comparison operators.",
    "difficulty": "Beginner"
  },
  {
    "id": 22,
    "question": "In the context of recursion, what causes a 'Stack Overflow' error?",
    "options": [
      "Excessive memory usage on the heap",
      "Exceeding the maximum depth of the call stack",
      "Running out of disk space",
      "Integer division by zero"
    ],
    "answer": "Exceeding the maximum depth of the call stack",
    "explanation": "Stack overflow occurs when the memory reserved for the call stack (tracking function calls) is exhausted, typically due to infinite recursion or excessively deep recursion.",
    "difficulty": "Beginner"
  },
  {
    "id": 23,
    "question": "Which algorithm design technique involves exploring all potential solutions by incrementally building candidates and abandoning a candidate ('backtracking') as soon as it cannot lead to a valid solution?",
    "options": [
      "Dynamic Programming",
      "Greedy",
      "Backtracking",
      "Divide and Conquer"
    ],
    "answer": "Backtracking",
    "explanation": "Backtracking is a brute-force refinement that prunes partial solutions that violate constraints (e.g., N-Queens). Dynamic Programming stores solutions to subproblems.",
    "difficulty": "Beginner"
  },
  {
    "id": 24,
    "question": "What is the time complexity of the union operation in a Disjoint Set Union (DSU) data structure using Path Compression and Union by Rank?",
    "options": [
      "O(1)",
      "O(log n)",
      "O(alpha(n))",
      "O(n)"
    ],
    "answer": "O(alpha(n))",
    "explanation": "With these optimizations, the time complexity is the inverse Ackermann function alpha(n), which grows extremely slowly and is effectively constant (O(1)) for all practical values of n.",
    "difficulty": "Beginner"
  },
  {
    "id": 25,
    "question": "Which condition is necessary for a graph to have a valid Topological Sort?",
    "options": [
      "The graph must be fully connected",
      "The graph must be a Directed Acyclic Graph (DAG)",
      "The graph must contain only positive weights",
      "The graph must be undirected"
    ],
    "answer": "The graph must be a Directed Acyclic Graph (DAG)",
    "explanation": "Topological sort orders nodes such that for every directed edge u->v, u comes before v. This is only possible if the graph has no directed cycles.",
    "difficulty": "Beginner"
  },
  {
    "id": 26,
    "question": "What is the primary disadvantage of using an Adjacency Matrix to represent a graph?",
    "options": [
      "Slow edge lookup",
      "High memory usage for sparse graphs",
      "Inability to represent weighted edges",
      "Difficulty in implementation"
    ],
    "answer": "High memory usage for sparse graphs",
    "explanation": "An Adjacency Matrix allocates V^2 space regardless of the number of edges (E). If E is much smaller than V^2 (sparse), the matrix wastes significant memory.",
    "difficulty": "Beginner"
  },
  {
    "id": 27,
    "question": "Which Boolean operator returns `true` only if both operands are `true`?",
    "options": [
      "OR",
      "XOR",
      "AND",
      "NAND"
    ],
    "answer": "AND",
    "explanation": "Logical AND requires both inputs to be true. OR requires at least one, and XOR requires exactly one to be true (for boolean logic).",
    "difficulty": "Beginner"
  },
  {
    "id": 28,
    "question": "What is the result of `1 << 3` in bitwise operations?",
    "options": [
      "3",
      "4",
      "8",
      "2"
    ],
    "answer": "8",
    "explanation": "Left shifting an integer by n positions multiplies it by 2^n. `1 << 3` is equivalent to 1 * 2^3 = 8.",
    "difficulty": "Beginner"
  },
  {
    "id": 29,
    "question": "Which algorithm is typically used to find the Strongly Connected Components in a directed graph?",
    "options": [
      "Dijkstra's Algorithm",
      "Kosaraju's Algorithm",
      "Prim's Algorithm",
      "Ford-Fulkerson Algorithm"
    ],
    "answer": "Kosaraju's Algorithm",
    "explanation": "Kosaraju's (or Tarjan's) algorithm finds SCCs by utilizing DFS on the graph and its transpose. Dijkstra finds paths, Prim finds MSTs, and Ford-Fulkerson computes max flow.",
    "difficulty": "Beginner"
  },
  {
    "id": 30,
    "question": "In a Hash Map, what is the primary purpose of a 'load factor'?",
    "options": [
      "To determine the compression algorithm",
      "To decide when to resize the underlying array",
      "To encrypt the keys",
      "To calculate the hash code"
    ],
    "answer": "To decide when to resize the underlying array",
    "explanation": "The load factor (number of entries / bucket count) triggers a rehashing/resizing operation when exceeded to maintain efficient O(1) access and reduce collisions.",
    "difficulty": "Beginner"
  },
  {
    "id": 31,
    "question": "Which of the following best describes the 'Greedy' algorithm paradigm?",
    "options": [
      "It breaks the problem into overlapping subproblems",
      "It makes the locally optimal choice at each stage hoping for a global optimum",
      "It explores all possible paths using recursion",
      "It sorts the data before processing"
    ],
    "answer": "It makes the locally optimal choice at each stage hoping for a global optimum",
    "explanation": "Greedy algorithms are top-down approaches that pick the best immediate option without revisiting or considering future consequences (e.g., Dijkstra, Huffman Coding).",
    "difficulty": "Beginner"
  },
  {
    "id": 32,
    "question": "What is the worst-case time complexity of Merge Sort?",
    "options": [
      "O(n)",
      "O(n log n)",
      "O(n^2)",
      "O(log n)"
    ],
    "answer": "O(n log n)",
    "explanation": "Merge Sort consistently divides the array in half (log n) and merges the subarrays (n), resulting in O(n log n) in all cases (best, average, worst).",
    "difficulty": "Beginner"
  },
  {
    "id": 33,
    "question": "Which data structure allows access to the median element in O(1) time (assuming implementation details are handled)?",
    "options": [
      "Hash Map",
      "Two Heaps (Max-Heap and Min-Heap)",
      "Queue",
      "Standard Array"
    ],
    "answer": "Two Heaps (Max-Heap and Min-Heap)",
    "explanation": "Maintaining a Max-Heap for the lower half and a Min-Heap for the upper half allows accessing the median(s) at the roots in O(1). Other structures require sorting.",
    "difficulty": "Beginner"
  },
  {
    "id": 34,
    "question": "In the Two Pointers technique, what is a necessary precondition for the standard approach?",
    "options": [
      "The array must be unsorted",
      "The array must be sorted",
      "The array must contain only integers",
      "The array size must be odd"
    ],
    "answer": "The array must be sorted",
    "explanation": "The Two Pointers technique relies on monotonicity to decide whether to move the left or right pointer (based on the sum comparison). This logic fails if the data is unsorted.",
    "difficulty": "Beginner"
  },
  {
    "id": 35,
    "question": "What distinguishes a 'Trie' from a standard Hash Map for storing strings?",
    "options": [
      "Tries are always faster",
      "Tries allow efficient prefix-based searches",
      "Tries use less memory",
      "Tries cannot store duplicate strings"
    ],
    "answer": "Tries allow efficient prefix-based searches",
    "explanation": "A Trie stores characters in a tree structure, enabling efficient retrieval of all keys with a specific prefix (autocomplete). Hash maps map full keys to values without inherent prefix structure.",
    "difficulty": "Beginner"
  },
  {
    "id": 36,
    "question": "Why does Dijkstra's algorithm fail to compute the shortest path when negative edge weights are present in a graph?",
    "options": [
      "The algorithm prioritizes nodes with the smallest tentative distance, assuming it is final, which may not hold if future negative edges reduce it",
      "Negative edges cause the priority queue to enter an infinite loop due to integer underflow",
      "The graph becomes cyclic and unreachable, preventing the termination condition",
      "Relaxation operations require strictly positive values to maintain the triangle inequality property"
    ],
    "answer": "The algorithm prioritizes nodes with the smallest tentative distance, assuming it is final, which may not hold if future negative edges reduce it",
    "explanation": "Dijkstra's greedy strategy assumes that once a node is extracted from the priority queue, the shortest distance to it is found. A negative edge from a node not yet processed could create a shorter path to an already 'finalized' node, violating this assumption. Negative cycles are an issue for Bellman-Ford, but simple negative weights break Dijkstra's greedy logic specifically.",
    "difficulty": "Intermediate"
  },
  {
    "id": 37,
    "question": "What is the primary purpose of Lazy Propagation in a Segment Tree?",
    "options": [
      "To reduce the memory footprint of the tree by storing only leaf nodes",
      "To defer updates to child nodes until they are explicitly queried, achieving O(log N) range updates",
      "To speed up point queries by pre-calculating the sum of all subtree ranges",
      "To balance the tree dynamically during insertion to maintain O(log N) height"
    ],
    "answer": "To defer updates to child nodes until they are explicitly queried, achieving O(log N) range updates",
    "explanation": "Without lazy propagation, a range update requires visiting O(N) leaf nodes. Lazy propagation marks a node as 'dirty' (pending update) and pushes the value down only when a specific child requires access, keeping updates efficient at O(log N).",
    "difficulty": "Intermediate"
  },
  {
    "id": 38,
    "question": "Which technique is essential to ensure the amortized time complexity of Union-Find (Disjoint Set Union) operations is effectively O(α(N)), where α is the inverse Ackermann function?",
    "options": [
      "Path compression and Union by rank/size",
      "Balancing the tree using AVL rotations",
      "Rehashing the underlying array when load factor exceeds 0.75",
      "Maintaining a separate priority queue for element ordering"
    ],
    "answer": "Path compression and Union by rank/size",
    "explanation": "Path compression flattens the structure of the tree during find operations, while Union by rank ensures the tree remains shallow by attaching the smaller tree to the root of the larger tree. Together, they optimize the structure to near-constant time complexity.",
    "difficulty": "Intermediate"
  },
  {
    "id": 39,
    "question": "In the context of Minimum Spanning Trees (MST), how does Kruskal's algorithm differ fundamentally from Prim's algorithm regarding edge processing?",
    "options": [
      "Kruskal's uses a priority queue to select the minimum weight edge connected to the current tree",
      "Kruskal's builds the forest by adding the globally smallest edge that does not form a cycle, whereas Prim's grows a single tree from a starting vertex",
      "Kruskal's requires the graph to be directed, while Prim's works on undirected graphs only",
      "Kruskal's fails on dense graphs, while Prim's is optimized for sparse graphs only"
    ],
    "answer": "Kruskal's builds the forest by adding the globally smallest edge that does not form a cycle, whereas Prim's grows a single tree from a starting vertex",
    "explanation": "Kruskal's is a greedy edge-based algorithm that sorts all edges and adds them if they connect different components. Prim's is a vertex-based algorithm that maintains a set of visited vertices and adds the cheapest edge connecting them to an unvisited vertex.",
    "difficulty": "Intermediate"
  },
  {
    "id": 40,
    "question": "Which property of the Breadth-First Search (BFS) algorithm makes it suitable for finding the shortest path in an unweighted graph?",
    "options": [
      "It uses a stack to process nodes in Last-In-First-Out order",
      "It visits nodes level-by-level, guaranteeing that the first time a node is discovered is via the shortest path",
      "It recursively explores the deepest nodes first before backtracking",
      "It dynamically updates edge weights based on node visitation frequency"
    ],
    "answer": "It visits nodes level-by-level, guaranteeing that the first time a node is discovered is via the shortest path",
    "explanation": "BFS explores all neighbors at the present depth prior to moving on to nodes at the next depth level. In an unweighted graph, each edge has equal cost (1), so the first time a node is reached is necessarily via the minimum number of edges.",
    "difficulty": "Intermediate"
  },
  {
    "id": 41,
    "question": "When solving the Longest Increasing Subsequence (LIS) problem, how does the 'Patience Sorting' / Binary Search approach achieve O(N log N) time complexity compared to the standard O(N²) DP?",
    "options": [
      "By sorting the array first and then applying a standard sliding window",
      "By maintaining a list of the smallest possible tail value for all increasing subsequences of length i",
      "By using a Segment Tree to query the maximum LIS value ending at a specific number",
      "By memoizing the recursive calls to avoid re-calculating suffix sums"
    ],
    "answer": "By maintaining a list of the smallest possible tail value for all increasing subsequences of length i",
    "explanation": "The algorithm maintains an array `tails` where `tails[i]` stores the smallest ending element of an increasing subsequence of length `i+1`. By replacing the first element in `tails` greater than the current number (using binary search), it ensures the array remains sorted and allows extending subsequences efficiently.",
    "difficulty": "Intermediate"
  },
  {
    "id": 42,
    "question": "In Kahn's Algorithm for Topological Sorting, what condition indicates that the input graph contains a cycle?",
    "options": [
      "The adjacency list contains a node with zero outgoing edges",
      "The queue becomes empty before all vertices have been processed",
      "The recursion depth exceeds the maximum stack size during DFS",
      "The resulting sorted order contains duplicate vertices"
    ],
    "answer": "The queue becomes empty before all vertices have been processed",
    "explanation": "Kahn's algorithm works by repeatedly removing nodes with no incoming edges (in-degree 0). If there is a cycle, every node in that cycle will have an in-degree of at least 1, meaning they can never be added to the queue, and the algorithm will halt prematurely.",
    "difficulty": "Intermediate"
  },
  {
    "id": 43,
    "question": "In a Binary Indexed Tree (Fenwick Tree), which bitwise operation is used to move from a current index `i` to its immediate parent in the tree structure?",
    "options": [
      "i + (i & -i)",
      "i - 1",
      "i - (i & -i)",
      "i >> 1"
    ],
    "answer": "i - (i & -i)",
    "explanation": "The term `i & -i` isolates the lowest set bit of `i`. This value represents the range of responsibility for the current node. To move to the parent (which covers a smaller or different range), you subtract this lowest set bit from the current index.",
    "difficulty": "Intermediate"
  },
  {
    "id": 44,
    "question": "How does the Floyd-Warshall algorithm detect the presence of a negative cycle in a graph?",
    "options": [
      "If any diagonal entry `dist[i][i]` becomes negative after the algorithm completes",
      "If the shortest path distance between any two nodes is infinity",
      "If the relaxation step fails to update any distance in the final iteration",
      "If the sum of all edges in the adjacency matrix is negative"
    ],
    "answer": "If any diagonal entry `dist[i][i]` becomes negative after the algorithm completes",
    "explanation": "The distance from a node to itself (`dist[i][i]`) is initialized to 0. If the algorithm finds a path that leaves node `i` and eventually returns to `i` with a total weight less than 0, `dist[i][i]` will become negative, indicating a negative cycle.",
    "difficulty": "Intermediate"
  },
  {
    "id": 45,
    "question": "When implementing the 0/1 Knapsack problem using Dynamic Programming with space optimization (1D array), in which order must the inner loop iterate to avoid using an item multiple times?",
    "options": [
      "Forward (0 to W)",
      "Backward (W down to 0)",
      "Random order",
      "Only over prime weights"
    ],
    "answer": "Backward (W down to 0)",
    "explanation": "Iterating backwards ensures that when computing `dp[w]`, the value of `dp[w - weight]` being read is from the *previous* iteration (items processed so far). Iterating forward would read the updated value of the current item, allowing the item to be counted infinitely (unbounded knapsack).",
    "difficulty": "Intermediate"
  },
  {
    "id": 46,
    "question": "Which of the following sorting algorithms is considered 'stable' and 'comparison-based', and generally performs better than Merge Sort on linked lists?",
    "options": [
      "Quick Sort",
      "Bubble Sort",
      "Merge Sort",
      "Heap Sort"
    ],
    "answer": "Merge Sort",
    "explanation": "Merge Sort is stable and comparison-based. While Quick Sort is comparison-based, it is not stable (standard implementation). Heap Sort is also unstable. Merge Sort is particularly suited for linked lists because it requires only sequential access, avoiding the random access overhead that Quick Sort or Heap Sort might incur. Note: The prompt implies the standard distinction where Merge Sort is the canonical stable efficient sort.",
    "difficulty": "Intermediate"
  },
  {
    "id": 47,
    "question": "What is the time complexity of finding the longest common prefix among a set of strings using a Trie data structure?",
    "options": [
      "O(N * M) where N is string count and M is max length",
      "O(M) where M is the length of the longest common prefix",
      "O(N log N)",
      "O(N + M)"
    ],
    "answer": "O(M) where M is the length of the longest common prefix",
    "explanation": "A Trie stores characters by level. Once the Trie is built, finding the longest common prefix simply involves traversing down from the root until a node has more than one child or the string ends. This depends only on the depth of the traversal (M), not the total number of strings.",
    "difficulty": "Intermediate"
  },
  {
    "id": 48,
    "question": "In Kosaraju's algorithm for finding Strongly Connected Components (SCCs), why is the second Depth First Search (DFS) performed on the transposed graph (edges reversed)?",
    "options": [
      "To reduce the total number of edges processed in the second pass",
      "To ensure that the DFS starts from the node with the lowest finishing time from the first pass",
      "To prevent moving from one SCC into another, effectively isolating SCCs for processing",
      "To convert the directed graph into an undirected graph"
    ],
    "answer": "To prevent moving from one SCC into another, effectively isolating SCCs for processing",
    "explanation": "Reversing edges ensures that if there was a path from SCC A to SCC B in the original graph, there is a path from B to A in the transposed graph. However, crucially, there is no path *out* of the SCC being processed to other SCCs that were 'upstream' in the first pass, preventing the DFS from bleeding into separate components.",
    "difficulty": "Intermediate"
  },
  {
    "id": 49,
    "question": "When applying the Two Pointers technique to solve the 'Two Sum' problem (finding a pair summing to K), what is a prerequisite for the input array?",
    "options": [
      "The array must contain only unique elements",
      "The array must be sorted",
      "The array must be circular",
      "The array size must be a power of two"
    ],
    "answer": "The array must be sorted",
    "explanation": "The Two Pointers technique relies on the monotonic property of a sorted array. By adjusting pointers based on whether the current sum is too high or too low, we can deterministically decide which direction to move. If the array is unsorted, there is no logical basis for moving the pointers.",
    "difficulty": "Intermediate"
  },
  {
    "id": 50,
    "question": "How does the Edmonds-Karp algorithm improve upon the generic Ford-Fulkerson method for computing Max Flow?",
    "options": [
      "By using Depth First Search (DFS) to find augmenting paths",
      "By using Breadth First Search (BFS) to find the shortest augmenting path (in terms of edges)",
      "By dynamically resizing the graph edges based on flow",
      "By preprocessing the graph to remove all negative capacity edges"
    ],
    "answer": "By using Breadth First Search (BFS) to find the shortest augmenting path (in terms of edges)",
    "explanation": "Ford-Fulkerson's implementation dependent on DFS can take exponential time if augmenting paths are long or poorly chosen. Edmonds-Karp restricts the choice to the shortest path (fewest edges) using BFS, guaranteeing a polynomial time complexity of O(VE²).",
    "difficulty": "Intermediate"
  },
  {
    "id": 51,
    "question": "In Bitmask Dynamic Programming, what is the standard state definition used to solve the Traveling Salesperson Problem (TSP) for N cities?",
    "options": [
      "dp[mask] = minimum cost to visit the set of cities in 'mask'",
      "dp[i][j] = minimum cost to travel from city i to city j",
      "dp[mask][i] = minimum cost to visit the set of cities in 'mask' ending at city 'i'",
      "dp[mask] = maximum number of cities visited using 'mask'"
    ],
    "answer": "dp[mask][i] = minimum cost to visit the set of cities in 'mask' ending at city 'i'",
    "explanation": "The state must track which cities have been visited (`mask`) AND where the salesman currently is (`i`), because the cost to return to the start depends on the current location. Without the endpoint index `i`, the state is insufficient to define the next transition.",
    "difficulty": "Intermediate"
  },
  {
    "id": 52,
    "question": "What is the primary mechanism that allows a Treap (Tree + Heap) to maintain balance probabilistically?",
    "options": [
      "Strict color balancing rules (Red-Black properties)",
      "Storing subtree sizes and performing rotations based on rank",
      "Assigning a random priority to nodes and maintaining heap order on priorities",
      "Always splitting the tree at the median index"
    ],
    "answer": "Assigning a random priority to nodes and maintaining heap order on priorities",
    "explanation": "A Treap obeys BST property on keys and Heap property on priorities. Since priorities are assigned randomly, the expected height of the tree is O(log N), similar to a randomly generated BST, preventing the worst-case O(N) behavior of a standard BST on sorted input.",
    "difficulty": "Intermediate"
  },
  {
    "id": 53,
    "question": "In the context of Sliding Window Maximum problems, why is a Deque (Double-Ended Queue) preferred over a Priority Queue?",
    "options": [
      "Deques allow O(1) access to the maximum element, whereas Heaps take O(log N)",
      "Deques naturally keep elements sorted, whereas Heaps do not",
      "Deques allow efficient removal of elements outside the window and elements smaller than the new element from the back, maintaining the monotonic property",
      "Priority Queues cannot handle duplicate values in the window"
    ],
    "answer": "Deques allow efficient removal of elements outside the window and elements smaller than the new element from the back, maintaining the monotonic property",
    "explanation": "A Max Heap takes O(log N) for insertion and deletion. More importantly, a Heap cannot efficiently remove elements that are 'out of the window' but not at the top. A Deque allows O(1) removal from both ends to maintain a decreasing sequence, where the front is always the max.",
    "difficulty": "Intermediate"
  },
  {
    "id": 54,
    "question": "Tarjan's Strongly Connected Components (SCC) algorithm utilizes a 'Lowlink' value. What does `low[u]` represent?",
    "options": [
      "The highest index reachable from node u",
      "The smallest discovery time (index) reachable from u by following zero or more tree edges and then at most one back edge",
      "The discovery time of the parent of u in the DFS tree",
      "The total number of nodes in the subtree rooted at u"
    ],
    "answer": "The smallest discovery time (index) reachable from u by following zero or more tree edges and then at most one back edge",
    "explanation": "`disc[u]` is the discovery time. `low[u]` tracks the earliest node reachable from the subtree of `u`. If `low[u] == disc[u]`, it means `u` cannot reach back to an ancestor, identifying it as the root of an SCC.",
    "difficulty": "Intermediate"
  },
  {
    "id": 55,
    "question": "Which proof technique is most commonly used to establish the optimality of a Greedy algorithm?",
    "options": [
      "Proof by Induction",
      "Proof by Contradiction",
      "Exchange Argument",
      "Reduction to SAT"
    ],
    "answer": "Exchange Argument",
    "explanation": "The Exchange Argument demonstrates that if you have an optimal solution that does not match the greedy choice, you can exchange an element in the optimal solution with the greedy choice without worsening the solution, thereby proving that a greedy solution exists which is optimal.",
    "difficulty": "Intermediate"
  },
  {
    "id": 56,
    "question": "What is the primary function of Coordinate Compression in algorithm optimization?",
    "options": [
      "To convert 3D coordinates into 2D space for faster rendering",
      "To map large or non-contiguous input values to a smaller, contiguous range of indices",
      "To compress the memory footprint of a graph by removing isolated vertices",
      "To sort the coordinates of a convex hull"
    ],
    "answer": "To map large or non-contiguous input values to a smaller, contiguous range of indices",
    "explanation": "Algorithms like Fenwick Trees or Segment Trees require indices to be within a manageable range (e.g., 0 to N). Coordinate compression sorts unique values and assigns them ranks 0 to N-1, preserving relative order while fitting the constraints of index-based data structures.",
    "difficulty": "Intermediate"
  },
  {
    "id": 57,
    "question": "In Red-Black Trees, which property ensures that the longest path from root to leaf is no more than twice the length of the shortest path?",
    "options": [
      "The root is always black",
      "Red nodes cannot have red children (no double red)",
      "Every leaf (NIL) is black",
      "Both a and b"
    ],
    "answer": "Red nodes cannot have red children (no double red)",
    "explanation": "Since red nodes cannot have red children, at least half the nodes on any root-to-leaf path must be black (alternating color rule). Combined with the property that all root-to-leaf paths have the same number of black nodes, this limits the height to roughly 2 * log(N).",
    "difficulty": "Intermediate"
  },
  {
    "id": 58,
    "question": "What is the time complexity of the 'Meet-in-the-Middle' algorithm when applied to a problem like the Partition Sum or Subset Sum for a set of size N=40?",
    "options": [
      "O(2^N)",
      "O(N * 2^(N/2))",
      "O(N^2)",
      "O(N log N)"
    ],
    "answer": "O(N * 2^(N/2))",
    "explanation": "The input is split into two halves of size N/2. We compute all subsets (2^(N/2)) for each half, sort one half, and binary search (or use two pointers) for pairs. The dominant factor is generating subsets: 2 * 2^(N/2) = 2^(N/2 + 1), multiplied by N for sorting/iterating.",
    "difficulty": "Intermediate"
  },
  {
    "id": 59,
    "question": "In Sqrt Decomposition (Mo's Algorithm variant), what is the optimal block size typically used to minimize the total complexity of processing Q queries on an array of size N?",
    "options": [
      "O(log N)",
      "O(N / Q)",
      "O(sqrt(N))",
      "O(2^N)"
    ],
    "answer": "O(sqrt(N))",
    "explanation": "The complexity is roughly O((N/Q * N) + (Q * B)). Balancing the movement of the Left pointer (N jumps) and the Right pointer (Q * B jumps) minimizes the total cost when B is approximately sqrt(N).",
    "difficulty": "Intermediate"
  },
  {
    "id": 60,
    "question": "In the context of Minimum Spanning Trees, the 'Cut Property' states that:",
    "options": [
      "For any cut of the graph, the minimum weight edge crossing the cut is part of the MST",
      "The maximum weight edge in a cycle can be safely removed",
      "All edges in the MST must be of equal weight",
      "The graph can be cut into two trees to verify correctness"
    ],
    "answer": "For any cut of the graph, the minimum weight edge crossing the cut is part of the MST",
    "explanation": "A cut partitions vertices into two sets. The lightest edge crossing this cut must be in the MST because if it weren't, we could add it and remove a heavier edge crossing the same cut, resulting in a lighter spanning tree.",
    "difficulty": "Intermediate"
  },
  {
    "id": 61,
    "question": "Which graph algorithm concept is primarily utilized in the solution to the 'Assignment Problem' (e.g., assigning N jobs to N workers with minimum cost)?",
    "options": [
      "Prim's Algorithm",
      "Hungarian Algorithm (or Min-Cost Max-Flow)",
      "Floyd-Warshall",
      "Dijkstra with a heuristic"
    ],
    "answer": "Hungarian Algorithm (or Min-Cost Max-Flow)",
    "explanation": "The Assignment Problem is a specific type of bipartite matching with weighted edges. The Hungarian Algorithm solves it in O(N³). It can also be modeled as a Min-Cost Max-Flow problem with a source and sink connected to the bipartite sets.",
    "difficulty": "Intermediate"
  },
  {
    "id": 62,
    "question": "The Convex Hull Trick (CHT) optimization is used to speed up specific Dynamic Programming transitions. What is the mathematical form of the DP equation that CHT optimizes?",
    "options": [
      "dp[i] = min(dp[j] + cost(j+1, i)) where cost is complex",
      "dp[i] = min(m * j + c + dp[j]) where m and c are constants derived from i",
      "dp[i][j] = dp[i-1][k] + C[k][j]",
      "dp[i] = dp[i-1] + dp[i-2]"
    ],
    "answer": "dp[i] = min(m * j + c + dp[j]) where m and c are constants derived from i",
    "explanation": "CHT maintains a set of lines (y = mx + c) corresponding to previous states. When querying for state `i`, we treat the variable (often `j` or `dp[j]`) as x and want the minimum y at a specific x coordinate. This effectively replaces the inner loop with a logarithmic query.",
    "difficulty": "Intermediate"
  },
  {
    "id": 63,
    "question": "In Mo's Algorithm, queries are sorted in a specific order to minimize the cost of moving the current window boundaries. Which comparator is used?",
    "options": [
      "Sort by L value ascending, then by R value ascending",
      "Sort by block of L ascending, and by R value ascending if in the same block (alternating R direction for odd/even blocks is an optimization)",
      "Sort by R value descending, then by L value ascending",
      "Sort by the query index"
    ],
    "answer": "Sort by block of L ascending, and by R value ascending if in the same block (alternating R direction for odd/even blocks is an optimization)",
    "explanation": "Standard Mo's sorts by the block of the Left pointer. Within the same block, sorting by the Right pointer groups queries spatially. A common optimization (Hilbert Order or Odd-Even sort) modifies the R-sorting to reduce pointer thrashing.",
    "difficulty": "Intermediate"
  },
  {
    "id": 64,
    "question": "What is the fundamental requirement for applying 'Binary Search on Answer' to a problem?",
    "options": [
      "The input array must be strictly increasing",
      "The problem must have a monotonic predicate (Boolean function) where True follows False (or vice versa)",
      "The complexity of the checking function must be O(1)",
      "The answer must be an integer"
    ],
    "answer": "The problem must have a monotonic predicate (Boolean function) where True follows False (or vice versa)",
    "explanation": "Binary search relies on the ability to discard half of the search space. This is only possible if the property being tested is monotonic (e.g., if a solution of size X is possible, then any size < X is also possible).",
    "difficulty": "Intermediate"
  },
  {
    "id": 65,
    "question": "How does Radix Sort achieve O(N) time complexity compared to O(N log N) comparison sorts?",
    "options": [
      "By partitioning the array around a pivot element",
      "By sorting numbers digit by digit, utilizing a stable subroutine (like Counting Sort) and avoiding element comparisons",
      "By converting numbers to binary and using bitwise operations to swap",
      "By balancing the input into a binary tree"
    ],
    "answer": "By sorting numbers digit by digit, utilizing a stable subroutine (like Counting Sort) and avoiding element comparisons",
    "explanation": "Radix sort is non-comparative. It processes digits (Least Significant Digit first usually). Using a stable sort per digit allows the order from previous digits to be preserved. If the range of digits is constant, complexity becomes O(N * number_of_digits).",
    "difficulty": "Intermediate"
  },
  {
    "id": 66,
    "question": "The concept of 'Bipartite Matching' is often solved using the Ford-Fulkerson algorithm by constructing a flow network. How are the original edges typically handled in this construction?",
    "options": [
      "Directed edges with capacity equal to the edge weight",
      "Undirected edges with infinite capacity",
      "Directed edges from source to one partition and the other partition to sink, with original undirected edges replaced by directed edges of capacity 1",
      "Original edges are removed and replaced by vertices"
    ],
    "answer": "Directed edges from source to one partition and the other partition to sink, with original undirected edges replaced by directed edges of capacity 1",
    "explanation": "Standard construction: Source connects to Left Partition nodes (cap 1). Right Partition nodes connect to Sink (cap 1). Edges between Left and Right are directed L->R (or R->L) with capacity 1. This ensures flow corresponds to a valid matching.",
    "difficulty": "Intermediate"
  },
  {
    "id": 67,
    "question": "In the Bellman-Ford algorithm, why does the standard loop run for exactly V-1 iterations (where V is the number of vertices)?",
    "options": [
      "To ensure all edges have been checked at least twice",
      "Because the longest simple path in a graph can contain at most V-1 edges",
      "To account for the zero-indexing of the adjacency matrix",
      "To detect negative cycles which always appear after V-1 iterations"
    ],
    "answer": "Because the longest simple path in a graph can contain at most V-1 edges",
    "explanation": "In a graph with V vertices, a simple path (no cycles) cannot have more than V-1 edges. Since Bellman-Ford relaxes all edges, after i iterations, it has found the shortest path using at most i edges. After V-1 iterations, it has found the shortest path using at most V-1 edges, which is the theoretical maximum for simple paths.",
    "difficulty": "Intermediate"
  },
  {
    "id": 68,
    "question": "What is a Persistent Data Structure?",
    "options": [
      "A data structure that stores data to a disk database",
      "A data structure that always preserves the previous version of itself when it is modified",
      "A data structure that uses a hash table to ensure O(1) lookups",
      "A data structure that persists after the program terminates (static)"
    ],
    "answer": "A data structure that always preserves the previous version of itself when it is modified",
    "explanation": "Persistence means that when an update is performed, instead of modifying the existing structure in place, a new version of the structure is created, sharing unchanged parts with the old version to save memory (e.g., Persistent Segment Tree).",
    "difficulty": "Intermediate"
  },
  {
    "id": 69,
    "question": "In Heavy-Light Decomposition (HLD), how is the 'Heavy Child' of a node defined?",
    "options": [
      "The child with the largest edge weight connecting to it",
      "The child that is visited first in the DFS traversal",
      "The child whose subtree has the maximum size",
      "The child that is closest to the root of the tree"
    ],
    "answer": "The child whose subtree has the maximum size",
    "explanation": "HLD selects the child with the largest subtree as the 'heavy' child. This ensures that any path from the root to a leaf passes through at most O(log N) 'light' edges, keeping path queries efficient.",
    "difficulty": "Intermediate"
  },
  {
    "id": 70,
    "question": "In the Knuth-Morris-Pratt (KMP) string matching algorithm, what does the 'LPS' (Longest Prefix Suffix) array store?",
    "options": [
      "The length of the longest substring of the text that matches the pattern",
      "The length of the longest proper prefix of the pattern which is also a suffix for every prefix of the pattern",
      "The indices of all occurrences of the pattern in the text",
      "The frequency of each character in the pattern"
    ],
    "answer": "The length of the longest proper prefix of the pattern which is also a suffix for every prefix of the pattern",
    "explanation": "For a prefix ending at index `i`, `lps[i]` stores the length of the longest proper prefix of `pattern[0...i]` that is also a suffix. This allows the algorithm to skip characters already known to match when a mismatch occurs.",
    "difficulty": "Intermediate"
  },
  {
    "id": 71,
    "question": "In the context of Dynamic Programming optimization, what specific condition must the cost function satisfy to apply the Knuth Optimization?",
    "options": [
      "The cost function must satisfy the quadrangle inequality and be monotonic.",
      "The cost function must be convex and the search space must be continuous.",
      "The cost function must be linear and independent of the previous states.",
      "The cost function must be multiplicative and satisfy the triangle inequality."
    ],
    "answer": "The cost function must satisfy the quadrangle inequality and be monotonic.",
    "explanation": "Knuth Optimization requires the quadrangle inequality (also known as the quadrangle inequality) and monotonicity of the decision argument (opt[i][j-1] <= opt[i][j] <= opt[i+1][j]) to reduce the time complexity from O(N^3) to O(N^2).",
    "difficulty": "Advanced"
  },
  {
    "id": 72,
    "question": "Which algorithmic technique is primarily utilized by the 'Segment Tree Beats' data structure to handle range chmin/chmax (assign min/max) updates efficiently?",
    "options": [
      "Lazy propagation applied specifically to break strict monotonicity.",
      "Fractional cascading on the second and maximum values of nodes.",
      "Heavy-light decomposition to flatten the tree structure.",
      "Maintaining a history buffer of all previous updates for rollback."
    ],
    "answer": "Lazy propagation applied specifically to break strict monotonicity.",
    "explanation": "Segment Tree Beats maintains not only the maximum value but also the second maximum value within a segment. This allows it to apply a 'chmax' update efficiently by updating tags only when the max value exceeds the target, breaking the monotonicity constraint of standard segment trees.",
    "difficulty": "Advanced"
  },
  {
    "id": 73,
    "question": "When using Dinic's algorithm for Maximum Flow, what is the worst-case time complexity for a unit capacity graph?",
    "options": [
      "O(min(V^(2/3), sqrt(E)) * E)",
      "O(E * sqrt(V))",
      "O(V^2 * E)",
      "O(E * log V)"
    ],
    "answer": "O(min(V^(2/3), sqrt(E)) * E)",
    "explanation": "In unit capacity graphs, Dinic's algorithm achieves a time complexity of O(min(V^(2/3), sqrt(E)) * E). This is significantly faster than the general O(V^2 * E) bound because the distance between source and sink increases by at least 1 each phase, and each phase is O(E).",
    "difficulty": "Advanced"
  },
  {
    "id": 74,
    "question": "What is the primary theoretical advantage of using a Fibonacci Heap over a Binary Heap in Dijkstra's algorithm?",
    "options": [
      "O(1) amortized time for decrease-key operations.",
      "O(log n) worst-case time for insert operations.",
      "Strict O(1) worst-case time for find-min.",
      "Deterministic O(1) time for delete-min operations."
    ],
    "answer": "O(1) amortized time for decrease-key operations.",
    "explanation": "The Fibonacci Heap improves Dijkstra's complexity from O(E log V) to O(E + V log V) specifically because it supports decrease-key operations in O(1) amortized time, whereas a Binary Heap takes O(log V).",
    "difficulty": "Advanced"
  },
  {
    "id": 75,
    "question": "In string algorithms, the 'Z-function' (Z-array) for a string computes the length of the longest substring starting from position i that matches the prefix. What is the time complexity of the standard linear-time algorithm to compute this array?",
    "options": [
      "O(N) using the 'box' invariant to maintain the rightmost match.",
      "O(N log N) using a divide and conquer strategy.",
      "O(N^2) using dynamic programming.",
      "O(N) using a suffix array and LCP query."
    ],
    "answer": "O(N) using the 'box' invariant to maintain the rightmost match.",
    "explanation": "The linear Z-algorithm maintains an interval [L, R] which is the interval with maximum R that matches the prefix. By leveraging previously computed values within this 'box', each character is compared at most once, resulting in O(N) total time.",
    "difficulty": "Advanced"
  },
  {
    "id": 76,
    "question": "Which property distinguishes a Treap from a standard Binary Search Tree (BST) regarding its balancing mechanism?",
    "options": [
      "It maintains heap priority on nodes randomly to probabilistically ensure balance.",
      "It uses color bits (red/black) and rotations to enforce height balance.",
      "It stores subtree size information to perform path compression.",
      "It splits the tree based on a hash value of the node's key."
    ],
    "answer": "It maintains heap priority on nodes randomly to probabilistically ensure balance.",
    "explanation": "A Treap (Tree + Heap) assigns a random priority to each node and maintains the heap property on priorities alongside the BST property on keys. This randomization ensures expected O(log n) height without complex deterministic balancing logic.",
    "difficulty": "Advanced"
  },
  {
    "id": 77,
    "question": "In the context of 2-Satisfiability (2-SAT) problems, which graph algorithmic concept is used to determine if a solution exists?",
    "options": [
      "Finding Strongly Connected Components (SCC) in the implication graph.",
      "Finding the Minimum Spanning Tree (MST) of the variable graph.",
      "Computing the Maximum Bipartite Matching on literals.",
      "Running a Breadth-First Search (BFS) from the source node."
    ],
    "answer": "Finding Strongly Connected Components (SCC) in the implication graph.",
    "explanation": "A 2-SAT instance is unsatisfiable if and only if there is a variable x such that x and NOT x are in the same Strongly Connected Component of the implication graph. If no such component exists, a valid assignment can be derived from the topological order of SCCs.",
    "difficulty": "Advanced"
  },
  {
    "id": 78,
    "question": "What is the specific time complexity of the 'Small-to-Large' merging technique when processing DSU (Disjoint Set Union) on trees or merging heaps/maps?",
    "options": [
      "O(N log N)",
      "O(N)",
      "O(N sqrt(N))",
      "O(N log^2 N)"
    ],
    "answer": "O(N log N)",
    "explanation": "Small-to-Large ensures that each element is moved or processed at most O(log N) times, because it only moves from a smaller set to a larger set, at least doubling the size of the set it belongs to each time. Total complexity is O(N log N).",
    "difficulty": "Advanced"
  },
  {
    "id": 79,
    "question": "Which technique allows the 'Sum over Subsets' (SOS) Dynamic Programming to calculate the sum of values for all submasks of a mask in O(N * 2^N) time?",
    "options": [
      "Iterating over bits and updating only masks where that bit is set.",
      "Using a Fast Fourier Transform (FFT) on the mask values.",
      "Performing a Depth First Search (DFS) on the subset lattice.",
      "Applying Memoization recursively for every mask."
    ],
    "answer": "Iterating over bits and updating only masks where that bit is set.",
    "explanation": "The SOS DP iterates through each bit position i and for every mask, if the i-th bit is set, it adds the DP value of `mask ^ (1<<i)` to `mask`. This iteratively aggregates sums from smaller submasks to larger ones.",
    "difficulty": "Advanced"
  },
  {
    "id": 80,
    "question": "What is the defining feature of a 'Persistent' data structure (e.g., Persistent Segment Tree)?",
    "options": [
      "It retains access to all previous versions of the data structure after updates.",
      "It stores data on non-volatile memory to survive system crashes.",
      "It uses a B-Tree structure to minimize disk I/O.",
      "It automatically re-balances itself after every read operation."
    ],
    "answer": "It retains access to all previous versions of the data structure after updates.",
    "explanation": "Persistence means that any modification to the data structure creates a new version, while nodes from the old version that were not changed remain shared between the versions, allowing O(log N) access to historical states.",
    "difficulty": "Advanced"
  },
  {
    "id": 81,
    "question": "When implementing the 'Minimum Cost Maximum Flow' algorithm using the Successive Shortest Path approach with potentials (Johnson's trick), what is the primary purpose of the potentials?",
    "options": [
      "To eliminate negative edge weights, allowing the use of Dijkstra's algorithm.",
      "To reduce the total cost of the flow by applying a discount factor.",
      "To convert the capacity constraints from integral to fractional.",
      "To find the augmenting path with the fewest number of edges."
    ],
    "answer": "To eliminate negative edge weights, allowing the use of Dijkstra's algorithm.",
    "explanation": "Potentials (or reduced costs) adjust the edge weights such that all non-saturated edges have non-negative reduced costs. This allows the algorithm to use the more efficient Dijkstra's algorithm (with a priority queue) instead of Bellman-Ford to find shortest paths in each iteration.",
    "difficulty": "Advanced"
  },
  {
    "id": 82,
    "question": "In the 'KMP' (Knuth-Morris-Pratt) algorithm, what information is stored in the 'LPS' (Longest Prefix Suffix) array?",
    "options": [
      "The length of the longest proper prefix of the sub-pattern which is also a suffix.",
      "The list of indices where the pattern matches the text.",
      "The frequency of each character in the pattern.",
      "The number of mismatch comparisons required for each position."
    ],
    "answer": "The length of the longest proper prefix of the sub-pattern which is also a suffix.",
    "explanation": "LPS[i] stores the length of the longest proper prefix of the pattern substring `pattern[0...i]` that is also a suffix of this substring. This allows the algorithm to skip redundant comparisons when a mismatch occurs.",
    "difficulty": "Advanced"
  },
  {
    "id": 83,
    "question": "Which graph traversal technique is the foundation for the 'Tarjan' algorithm for finding Strongly Connected Components (SCC)?",
    "options": [
      "Depth First Search (DFS) using 'disc' (discovery time) and 'low' (low-link) values.",
      "Breadth First Search (BFS) using distance tracking.",
      "Dijkstra's algorithm using priority queues.",
      "Prim's algorithm using edge weight sorting."
    ],
    "answer": "Depth First Search (DFS) using 'disc' (discovery time) and 'low' (low-link) values.",
    "explanation": "Tarjan's algorithm performs a single DFS pass. It maintains `low` values (the smallest discovery time reachable) to identify the root of an SCC when `low[u] == disc[u]`, then pops the stack to form the component.",
    "difficulty": "Advanced"
  },
  {
    "id": 84,
    "question": "What is the time complexity of finding the 'Articulation Points' (or Cut Vertices) in an undirected graph using Tarjan's algorithm?",
    "options": [
      "O(V + E)",
      "O(V * E)",
      "O(V^2)",
      "O(E log V)"
    ],
    "answer": "O(V + E)",
    "explanation": "Similar to finding SCCs or Bridges, finding Articulation Points requires a single DFS traversal to calculate discovery times and low values, resulting in linear time complexity O(V + E).",
    "difficulty": "Advanced"
  },
  {
    "id": 85,
    "question": "In the context of number theory and algorithms, what is the 'Fermat Primality Test' based on?",
    "options": [
      "Fermat's Little Theorem regarding modular inverses.",
      "The Euclidean algorithm for greatest common divisors.",
      "The Chinese Remainder Theorem.",
      "Wilson's Theorem regarding factorials."
    ],
    "answer": "Fermat's Little Theorem regarding modular inverses.",
    "explanation": "The test relies on the property that if $p$ is prime, then for any integer $a$, $a^{p-1} \\equiv 1 \\pmod p$. However, it is vulnerable to Carmichael numbers, so Miller-Rabin is preferred for deterministic checks.",
    "difficulty": "Advanced"
  },
  {
    "id": 86,
    "question": "What is the core idea behind 'Meet-in-the-Middle' optimization?",
    "options": [
      "Splitting the problem in half, solving both halves independently via brute force, and combining results.",
      "Dividing the input array into two halves and sorting them.",
      "Meeting the search pointer in the middle of a sorted array.",
      "Using a binary search tree to split data."
    ],
    "answer": "Splitting the problem in half, solving both halves independently via brute force, and combining results.",
    "explanation": "Meet-in-the-Middle reduces exponential time complexity from O(2^N) to O(2^{N/2}) by enumerating all possibilities for the first half of the input, storing them, and then matching them against enumerated possibilities for the second half (e.g., subset sum).",
    "difficulty": "Advanced"
  },
  {
    "id": 87,
    "question": "In a 'Binary Indexed Tree' (Fenwick Tree), which operation corresponds to retrieving the prefix sum from index 0 to i?",
    "options": [
      "Moving up the tree by repeatedly subtracting the lowest set bit (i -= i & -i).",
      "Moving down the tree by repeatedly adding the lowest set bit (i += i & -i).",
      "Performing a standard traversal from the root to the leaf.",
      "Calculating the bitwise XOR of all ancestors."
    ],
    "answer": "Moving up the tree by repeatedly subtracting the lowest set bit (i -= i & -i).",
    "explanation": "The prefix sum query involves summing the value at index `i`, then jumping to the parent `i -= (i & -i)` (removing the lowest set bit) until `i` becomes 0. Update operations move in the opposite direction (`i += i & -i`).",
    "difficulty": "Advanced"
  },
  {
    "id": 88,
    "question": "Which algorithm is most efficient for finding the 'Convex Hull' of a set of points in 2D when the points are already sorted by one coordinate?",
    "options": [
      "Monotone Chain algorithm.",
      "Graham Scan.",
      "Jarvis March (Gift Wrapping).",
      "QuickHull."
    ],
    "answer": "Monotone Chain algorithm.",
    "explanation": "While Graham Scan is O(N log N) due to sorting, Monotone Chain also requires sorting initially. However, the question implies sorted input; given that, Monotone Chain constructs the hull (upper and lower) in linear scan O(N) efficiently using a stack.",
    "difficulty": "Advanced"
  },
  {
    "id": 89,
    "question": "What is the 'Rollback' technique in the context of Union-Find (DSU)?",
    "options": [
      "Reverting DSU operations to a previous state using a stack to restore connectivity.",
      "Re-computing the DSU from scratch for every query.",
      "Using path compression to optimize future rollbacks.",
      "Merging sets based on rank to avoid rollbacks."
    ],
    "answer": "Reverting DSU operations to a previous state using a stack to restore connectivity.",
    "explanation": "Standard DSU with path compression is hard to rollback. The rollback variant avoids path compression and records changes (parent/size changes) on a stack during `union`, allowing `undo` operations to pop the stack and revert to the exact previous state, useful in offline divide-and-conquer queries.",
    "difficulty": "Advanced"
  },
  {
    "id": 90,
    "question": "Which sorting algorithm is conceptually the basis for the 'Burrows-Wheeler Transform' (BWT) construction?",
    "options": [
      "Sorting all cyclic rotations of the string.",
      "Sorting all suffixes of the string.",
      "Sorting all prefixes of the string.",
      "Counting sort based on character frequency."
    ],
    "answer": "Sorting all cyclic rotations of the string.",
    "explanation": "BWT is constructed by taking all cyclic rotations of the input string, sorting them lexicographically, and taking the last column of the sorted matrix. Suffix Arrays are a related but distinct structure used to accelerate this.",
    "difficulty": "Advanced"
  },
  {
    "id": 91,
    "question": "In computational geometry, what determines the orientation of an ordered triplet of points (p, q, r)?",
    "options": [
      "The sign of the slope of the cross product of vectors PQ and QR.",
      "The Euclidean distance between p and r.",
      "The angle of the vector PQ relative to the x-axis.",
      "The sum of the coordinates of the three points."
    ],
    "answer": "The sign of the slope of the cross product of vectors PQ and QR.",
    "explanation": "Orientation (Counter-clockwise, Clockwise, or Collinear) is determined by the sign of the cross product `(q.x - p.x)*(r.y - p.y) - (q.y - p.y)*(r.x - p.x)`. This is fundamental for convex hull and line intersection algorithms.",
    "difficulty": "Advanced"
  },
  {
    "id": 92,
    "question": "What is the primary limitation of the 'Bellman-Ford' algorithm compared to 'Dijkstra's'?",
    "options": [
      "Bellman-Ford has a worse time complexity (O(VE) vs O(E + V log V)).",
      "Bellman-Ford cannot handle graphs with negative edge weights.",
      "Bellman-Ford requires a Fibonacci Heap to function correctly.",
      "Bellman-Ford only works on Directed Acyclic Graphs (DAGs)."
    ],
    "answer": "Bellman-Ford has a worse time complexity (O(VE) vs O(E + V log V)).",
    "explanation": "While Bellman-Ford is more versatile because it can detect negative cycles and handle negative weights, its time complexity of O(V*E) is significantly slower than Dijkstra's O(E + V log V) (or similar), making it inefficient for large graphs without negative weights.",
    "difficulty": "Advanced"
  },
  {
    "id": 93,
    "question": "What distinguishes the 'Edmonds-Karp' algorithm from the generic Ford-Fulkerson method?",
    "options": [
      "It uses Breadth-First Search (BFS) to find the shortest augmenting path.",
      "It uses Depth-First Search (DFS) to find any augmenting path.",
      "It uses a priority queue to find the path with maximum flow.",
      "It implements the push-relabel heuristic instead of augmenting paths."
    ],
    "answer": "It uses Breadth-First Search (BFS) to find the shortest augmenting path.",
    "explanation": "Edmonds-Karp is a specific implementation of Ford-Fulkerson where BFS is used to find the augmenting path with the fewest edges. This bounds the number of augmentations to O(VE), ensuring a polynomial time complexity of O(V E^2).",
    "difficulty": "Advanced"
  },
  {
    "id": 94,
    "question": "In the 'A*' search algorithm, what is the role of the 'heuristic' function h(n)?",
    "options": [
      "It estimates the cost of the cheapest path from node n to the goal.",
      "It calculates the exact distance from the start node to node n.",
      "It determines the branching factor of the search tree.",
      "It sorts the open list primarily by the path length so far."
    ],
    "answer": "It estimates the cost of the cheapest path from node n to the goal.",
    "explanation": "A* minimizes `f(n) = g(n) + h(n)`, where `g(n)` is the cost from the start to `n`, and `h(n)` is the heuristic estimate from `n` to the goal. If `h(n)` is admissible (never overestimates), A* is optimal.",
    "difficulty": "Advanced"
  },
  {
    "id": 95,
    "question": "Which data structure is essential for implementing the 'Sieve of Eratosthenes' to generate primes up to N with O(N) complexity (the linear sieve)?",
    "options": [
      "An array to store the smallest prime factor (SPF) for each number.",
      "A binary search tree to store composite numbers.",
      "A stack to manage the current prime being processed.",
      "A hash set to filter out multiples."
    ],
    "answer": "An array to store the smallest prime factor (SPF) for each number.",
    "explanation": "The linear sieve ensures each composite number is marked exactly once by iterating through primes and breaking the loop when `p % SPF[i] == 0`. The SPF array allows efficient factorization of numbers after sieving.",
    "difficulty": "Advanced"
  },
  {
    "id": 96,
    "question": "What does the 'Master Theorem' solve for?",
    "options": [
      "The time complexity of divide-and-conquer recurrences of the form T(n) = aT(n/b) + f(n).",
      "The space complexity of recursive algorithms.",
      "The correctness of greedy algorithms.",
      "The maximum flow in a network."
    ],
    "answer": "The time complexity of divide-and-conquer recurrences of the form T(n) = aT(n/b) + f(n).",
    "explanation": "The Master Theorem provides a cookbook solution for asymptotic analysis (Big O) of recurrence relations where a problem of size n is broken down into a subproblems of size n/b.",
    "difficulty": "Advanced"
  },
  {
    "id": 97,
    "question": "In 'Hashing' algorithms (e.g., Rabin-Karp), what is the primary danger of using a fixed modulo (like 2^64) without careful distribution handling?",
    "options": [
      "Hash collisions leading to false positive matches.",
      "Stack overflow during recursive calls.",
      "Integer overflow slowing down the CPU.",
      "Time limit exceeded due to modulo operations."
    ],
    "answer": "Hash collisions leading to false positive matches.",
    "explanation": "While modulo 2^64 (using unsigned integer overflow) is fast, it maps a huge space of inputs into a relatively small space. It is possible for distinct strings to generate the same hash value, creating 'collisions' that the algorithm must verify (e.g., by direct string comparison).",
    "difficulty": "Advanced"
  },
  {
    "id": 98,
    "question": "What is the time complexity of the 'Hopcroft-Karp' algorithm for finding maximum matching in bipartite graphs?",
    "options": [
      "O(E sqrt(V))",
      "O(VE)",
      "O(V^3)",
      "O(E log V)"
    ],
    "answer": "O(E sqrt(V))",
    "explanation": "Hopcroft-Karp improves upon simple DFS augmenting path algorithms (O(VE)) by finding multiple shortest augmenting paths simultaneously in phases. This reduces the number of phases to O(sqrt(V)), resulting in O(E sqrt(V)) complexity.",
    "difficulty": "Advanced"
  },
  {
    "id": 99,
    "question": "The 'Convex Hull Trick' (or Convex Hull Optimization) is used to optimize DP problems where the transition involves lines. Which condition makes the 'Li Chao Tree' variant preferable over a standard deque-based CHT?",
    "options": [
      "When the query points are not monotonic (sorted) or lines are added arbitrarily.",
      "When the lines all have the same slope.",
      "When the number of queries is extremely small.",
      "When the DP transition only involves minimum operations."
    ],
    "answer": "When the query points are not monotonic (sorted) or lines are added arbitrarily.",
    "explanation": "The deque-based CHT relies on monotonicity of lines (by slope) and queries (by x-coordinate) to efficiently prune lines. The Li Chao Tree inserts lines into a binary search tree structure, allowing insertion and query in O(log C) regardless of order.",
    "difficulty": "Advanced"
  },
  {
    "id": 100,
    "question": "In 'Sqrt Decomposition', what is the ideal size of the blocks to achieve O(sqrt(N)) time complexity for range queries?",
    "options": [
      "Approximately sqrt(N)",
      "Approximately N/2",
      "Approximately log N",
      "Approximately N / log N"
    ],
    "answer": "Approximately sqrt(N)",
    "explanation": "Sqrt Decomposition divides the array of size N into blocks of size B. There are roughly N/B blocks. An operation typically scans O(B) elements inside one partial block and O(N/B) blocks in the middle. Balancing these terms (B = N/B) yields B = sqrt(N).",
    "difficulty": "Advanced"
  }
]