[
  {
    "id": 1,
    "question": "What is the primary difference between Vertical Scaling (Scale Up) and Horizontal Scaling (Scale Out)?",
    "options": [
      "Vertical scaling adds more machines to the pool, while horizontal scaling adds more power to a single machine.",
      "Vertical scaling increases the capacity of a single machine (CPU/RAM), while horizontal scaling adds more nodes to the pool.",
      "Vertical scaling is only possible in cloud environments, while horizontal scaling requires on-premise hardware.",
      "Vertical scaling handles read traffic, while horizontal scaling handles write traffic."
    ],
    "answer": "Vertical scaling increases the capacity of a single machine (CPU/RAM), while horizontal scaling adds more nodes to the pool.",
    "explanation": "Vertical scaling involves upgrading existing hardware resources, limited to the maximum capacity of a single machine. Horizontal scaling distributes the load across multiple machines, offering theoretically unlimited expansion.",
    "difficulty": "Beginner"
  },
  {
    "id": 2,
    "question": "In the CAP Theorem, what does the 'A' stand for, and what does it guarantee?",
    "options": [
      "Atomicity: Ensuring that transactions are all-or-nothing.",
      "Availability: Every request receives a (non-error) response, without the guarantee that it contains the most recent write.",
      "Asynchrony: Processes can execute at different speeds without communication.",
      "Authentication: Verifying the identity of users making requests."
    ],
    "answer": "Availability: Every request receives a (non-error) response, without the guarantee that it contains the most recent write.",
    "explanation": "In CAP, Availability means the system remains operational and responsive even during failures, though it may return stale data. Atomicity is an ACID property, not a CAP theorem component.",
    "difficulty": "Beginner"
  },
  {
    "id": 3,
    "question": "Which database property is defined by the principle that 'a transaction is a unit of work that is either performed completely or not performed at all'?",
    "options": [
      "Consistency",
      "Isolation",
      "Durability",
      "Atomicity"
    ],
    "answer": "Atomicity",
    "explanation": "Atomicity ensures that if one part of a transaction fails, the entire transaction fails and the database state is left unchanged. This prevents partial updates that could corrupt data.",
    "difficulty": "Beginner"
  },
  {
    "id": 4,
    "question": "What is the primary function of a Load Balancer in a distributed system?",
    "options": [
      "To store static assets like images and CSS files.",
      "To encrypt all incoming traffic using SSL/TLS.",
      "To distribute incoming network traffic across multiple servers.",
      "To translate domain names into IP addresses."
    ],
    "answer": "To distribute incoming network traffic across multiple servers.",
    "explanation": "Load balancers act as the traffic manager, routing client requests across all servers to ensure no single server becomes overloaded. SSL termination is a separate feature, and DNS is handled by specific name servers.",
    "difficulty": "Beginner"
  },
  {
    "id": 5,
    "question": "Why are Microservices generally considered more scalable than Monolithic architectures?",
    "options": [
      "Microservices share a single database, allowing for faster data retrieval.",
      "Each microservice can be scaled independently based on its specific load requirements.",
      "Monolithic architectures are limited to the HTTP protocol, whereas microservices use TCP.",
      "Microservices eliminate the need for load balancers."
    ],
    "answer": "Each microservice can be scaled independently based on its specific load requirements.",
    "explanation": "In a microservices architecture, you can deploy more instances of only the service experiencing high load, rather than scaling the entire application. Monoliths require scaling the whole application even if only one feature is under load.",
    "difficulty": "Beginner"
  },
  {
    "id": 6,
    "question": "What is the main trade-off when adding a database index to a table?",
    "options": [
      "Read performance decreases while write performance increases.",
      "Write performance decreases and storage increases, while read performance increases.",
      "Data consistency is sacrificed for improved latency.",
      "The database becomes unable to handle horizontal sharding."
    ],
    "answer": "Write performance decreases and storage increases, while read performance increases.",
    "explanation": "Indexes speed up data retrieval but require additional storage space and slow down write operations (INSERT/UPDATE/DELETE) because the index must also be updated.",
    "difficulty": "Beginner"
  },
  {
    "id": 7,
    "question": "Which strategy involves breaking a database into smaller chunks (shards) and distributing them across different servers?",
    "options": [
      "Replication",
      "Sharding",
      "Caching",
      "Partitioning (on a single disk)"
    ],
    "answer": "Sharding",
    "explanation": "Sharding is a horizontal scaling method where data is partitioned across multiple database instances (nodes). Replication copies data to multiple nodes for redundancy, not distribution.",
    "difficulty": "Beginner"
  },
  {
    "id": 8,
    "question": "In the context of caching, what is a 'Cache Hit'?",
    "options": [
      "When the cache server fails and requests are routed to the database.",
      "When the requested data is found in the cache, avoiding a database query.",
      "When the cache is full and evicts old data to make space.",
      "When the data in the cache is inconsistent with the database."
    ],
    "answer": "When the requested data is found in the cache, avoiding a database query.",
    "explanation": "A cache hit occurs when the client requests data that is already stored in the high-speed cache memory. This results in faster response times and reduced load on the primary database.",
    "difficulty": "Beginner"
  },
  {
    "id": 9,
    "question": "What is the primary purpose of a Content Delivery Network (CDN)?",
    "options": [
      "To dynamically generate HTML content based on user behavior.",
      "To execute server-side business logic closer to the user.",
      "To distribute static content (media, CSS, JS) geographically closer to end-users.",
      "To provide a secure tunnel for database replication."
    ],
    "answer": "To distribute static content (media, CSS, JS) geographically closer to end-users.",
    "explanation": "CDNs cache content in edge servers located around the world to reduce latency by serving content from a location physically closer to the user. They are not used for dynamic logic execution.",
    "difficulty": "Beginner"
  },
  {
    "id": 10,
    "question": "What distinguishes a Stateful service from a Stateless service?",
    "options": [
      "Stateful services store client session data locally between requests, while stateless services do not.",
      "Stateless services are faster because they use complex algorithms to predict state.",
      "Stateful services cannot be load balanced, while stateless services must always use UDP.",
      "Stateful services are only used for databases, while stateless services are used for APIs."
    ],
    "answer": "Stateful services store client session data locally between requests, while stateless services do not.",
    "explanation": "Stateless services treat every request independently, allowing any server to handle any request, which simplifies scaling. Stateful services require sticky sessions or complex state management to route requests to the specific server holding the session.",
    "difficulty": "Beginner"
  },
  {
    "id": 11,
    "question": "Which load balancing algorithm simply distributes requests in a circular sequence, regardless of server load or response time?",
    "options": [
      "Least Connections",
      "Round Robin",
      "IP Hash",
      "Weighted Response Time"
    ],
    "answer": "Round Robin",
    "explanation": "Round Robin iterates through a list of servers in order, distributing requests one by one. Least Connections is more intelligent, routing to the server with the fewest active connections.",
    "difficulty": "Beginner"
  },
  {
    "id": 12,
    "question": "What is the primary advantage of using Read Replicas in a database architecture?",
    "options": [
      "They allow for stronger data consistency guarantees for write operations.",
      "They enable horizontal scaling of read traffic to reduce load on the primary database.",
      "They eliminate the need for database backups.",
      "They automatically shard the database based on the primary key."
    ],
    "answer": "They enable horizontal scaling of read traffic to reduce load on the primary database.",
    "explanation": "Read replicas are copies of the primary database that handle read-only queries, offloading traffic from the primary node which handles writes. They introduce replication lag but improve read throughput.",
    "difficulty": "Beginner"
  },
  {
    "id": 13,
    "question": "Which of the following best describes 'Eventual Consistency' in a distributed database?",
    "options": [
      "All nodes acknowledge a write immediately before it is considered successful.",
      "The system guarantees that reads will always return the most recent write.",
      "Given enough time and no new updates, all accesses to the data will return the last updated value.",
      "Data is only consistent if the network never partitions."
    ],
    "answer": "Given enough time and no new updates, all accesses to the data will return the last updated value.",
    "explanation": "Eventual consistency guarantees that if no new updates are made, eventually all accesses will return the last written value. It prioritizes availability (AP in CAP) over immediate consistency.",
    "difficulty": "Beginner"
  },
  {
    "id": 14,
    "question": "What is the primary function of an API Gateway?",
    "options": [
      "To act as a reverse proxy, routing requests, handling SSL termination, and enforcing rate limiting.",
      "To store the permanent data records for all microservices.",
      "To replace the need for a load balancer in a monolithic application.",
      "To compile source code into machine executable binaries."
    ],
    "answer": "To act as a reverse proxy, routing requests, handling SSL termination, and enforcing rate limiting.",
    "explanation": "An API Gateway is the single entry point for all clients, handling cross-cutting concerns like authentication, routing, and throttling before forwarding requests to backend services.",
    "difficulty": "Beginner"
  },
  {
    "id": 15,
    "question": "In the context of system design, what does 'Idempotency' mean?",
    "options": [
      "An operation can be performed multiple times without changing the result beyond the initial application.",
      "An operation runs exactly once and then stops automatically.",
      "An operation is guaranteed to execute in less than 100ms.",
      "An operation requires two separate confirmation steps to complete."
    ],
    "answer": "An operation can be performed multiple times without changing the result beyond the initial application.",
    "explanation": "Idempotency ensures that making the same request multiple times has the same effect as making it once. This is crucial for handling network retries and failures safely.",
    "difficulty": "Beginner"
  },
  {
    "id": 16,
    "question": "What is the primary benefit of using a Message Queue (e.g., RabbitMQ, SQS) over direct synchronous HTTP calls?",
    "options": [
      "Message queues guarantee that data is never lost, even if the server burns down.",
      "It allows for asynchronous processing, decoupling the sender from the receiver and improving resilience to spikes.",
      "Message queues are faster than TCP connections because they use UDP.",
      "It eliminates the need for a database."
    ],
    "answer": "It allows for asynchronous processing, decoupling the sender from the receiver and improving resilience to spikes.",
    "explanation": "Message queues buffer requests, allowing the producer to continue without waiting for the consumer. This decoupling handles traffic spikes by processing tasks in the background.",
    "difficulty": "Beginner"
  },
  {
    "id": 17,
    "question": "What is the 'P' in the CAP Theorem?",
    "options": [
      "Performance",
      "Partition Tolerance",
      "Persistence",
      "Parallelism"
    ],
    "answer": "Partition Tolerance",
    "explanation": "Partition Tolerance refers to the system's ability to continue operating despite arbitrary message loss or failure of part of the system. In distributed systems, network partitions are a reality, forcing a trade-off between Consistency and Availability.",
    "difficulty": "Beginner"
  },
  {
    "id": 18,
    "question": "Which caching strategy involves checking the cache for data; if missing, fetching from DB, updating cache, and returning data?",
    "options": [
      "Write-Through",
      "Write-Back",
      "Cache-Aside (Lazy Loading)",
      "Write-Around"
    ],
    "answer": "Cache-Aside (Lazy Loading)",
    "explanation": "In Cache-Aside, the application looks in the cache first. On a miss, it retrieves data from the database, populates the cache, and returns it. The cache maintains only the data that is actually read.",
    "difficulty": "Beginner"
  },
  {
    "id": 19,
    "question": "What is a 'Long Polling' technique used for in real-time web applications?",
    "options": [
      "The server sends data to the client continuously until the connection times out.",
      "The client requests data and holds the connection open until the server has new data to send.",
      "The client polls the server every 10 seconds regardless of updates.",
      "The server keeps the connection open indefinitely and never closes it."
    ],
    "answer": "The client requests data and holds the connection open until the server has new data to send.",
    "explanation": "Long polling reduces the latency of standard polling by having the server wait until an event occurs before responding. If no event occurs, it holds the request until a timeout.",
    "difficulty": "Beginner"
  },
  {
    "id": 20,
    "question": "What is the main purpose of the Circuit Breaker pattern?",
    "options": [
      "To route requests to the nearest data center.",
      "To prevent cascading failures by stopping requests to a failing service after a threshold is reached.",
      "To encrypt data between microservices.",
      "To automatically scale up the number of service instances."
    ],
    "answer": "To prevent cascading failures by stopping requests to a failing service after a threshold is reached.",
    "explanation": "A Circuit Breaker detects when a remote service is failing (throwing exceptions/timing out) and trips, preventing the client from waiting or overwhelming the failing service further.",
    "difficulty": "Beginner"
  },
  {
    "id": 21,
    "question": "Which technique distributes data across servers based on a hash of the key to minimize reorganization when nodes are added?",
    "options": [
      "Random Distribution",
      "Consistent Hashing",
      "Round Robin Partitioning",
      "Sequential Sharding"
    ],
    "answer": "Consistent Hashing",
    "explanation": "Consistent hashing maps keys to specific nodes on a ring. When a node is added, only the keys for that node are remapped, whereas standard hashing would require remapping all keys.",
    "difficulty": "Beginner"
  },
  {
    "id": 22,
    "question": "What is the primary role of DNS (Domain Name System) in system architecture?",
    "options": [
      "To assign IP addresses to hardware devices via DHCP.",
      "To translate human-readable domain names into machine-readable IP addresses.",
      "To route traffic between different subnets within a VPC.",
      "To filter malicious traffic before it reaches the load balancer."
    ],
    "answer": "To translate human-readable domain names into machine-readable IP addresses.",
    "explanation": "DNS acts as the internet's phonebook, resolving domain names (e.g., google.com) to IP addresses (e.g., 142.250.xxx.xxx) so browsers can load resources.",
    "difficulty": "Beginner"
  },
  {
    "id": 23,
    "question": "What defines 'Throughput' in a system performance context?",
    "options": [
      "The time taken to process a single request.",
      "The number of requests processed successfully within a specific time frame.",
      "The amount of RAM used by the application.",
      "The physical distance between the client and the server."
    ],
    "answer": "The number of requests processed successfully within a specific time frame.",
    "explanation": "Throughput measures the rate of processing (e.g., requests per second). Latency measures the time taken for a single request.",
    "difficulty": "Beginner"
  },
  {
    "id": 24,
    "question": "What is a 'Hot Spot' in the context of database sharding?",
    "options": [
      "A server that has overheated due to high CPU usage.",
      "A specific shard that receives significantly more traffic or data than others, creating an imbalance.",
      "A firewall rule that blocks incoming traffic.",
      "A backup server that is kept in standby mode."
    ],
    "answer": "A specific shard that receives significantly more traffic or data than others, creating an imbalance.",
    "explanation": "Hot spots occur when a sharding key is chosen poorly (e.g., a popular user ID), causing one shard to handle a disproportionate load and negating the benefits of horizontal scaling.",
    "difficulty": "Beginner"
  },
  {
    "id": 25,
    "question": "Which RAID level uses disk mirroring to provide redundancy?",
    "options": [
      "RAID 0",
      "RAID 1",
      "RAID 5",
      "RAID 10"
    ],
    "answer": "RAID 1",
    "explanation": "RAID 1 writes identical data to two drives simultaneously. If one drive fails, the data is preserved on the other. RAID 0 stripes data for speed but has no redundancy.",
    "difficulty": "Beginner"
  },
  {
    "id": 26,
    "question": "What is 'Latency' in the context of web performance?",
    "options": [
      "The total number of users currently on the website.",
      "The delay before a transfer of data begins following an instruction.",
      "The total bandwidth of the network connection.",
      "The speed at which the database executes a query."
    ],
    "answer": "The delay before a transfer of data begins following an instruction.",
    "explanation": "Latency refers to the time interval between the initiation of a request and the start of the response (often measured as Round Trip Time). It is distinct from bandwidth (capacity).",
    "difficulty": "Beginner"
  },
  {
    "id": 27,
    "question": "What is the primary function of a 'Reverse Proxy'?",
    "options": [
      "To protect the identity of the client making the request.",
      "To sit in front of web servers and forward client requests to those servers, providing load balancing and security.",
      "To cache responses from the internet for local ISP clients.",
      "To convert database queries into SQL code."
    ],
    "answer": "To sit in front of web servers and forward client requests to those servers, providing load balancing and security.",
    "explanation": "A reverse proxy accepts requests from clients, forwards them to backend servers, and returns the server's response to the client, hiding the backend topology.",
    "difficulty": "Beginner"
  },
  {
    "id": 28,
    "question": "In an Event-Driven Architecture, what is an 'Event'?",
    "options": [
      "A scheduled cron job that runs every night.",
      "A significant change in state or an action that has occurred within the system.",
      "A user interface element like a button click.",
      "A specific type of database transaction."
    ],
    "answer": "A significant change in state or an action that has occurred within the system.",
    "explanation": "In event-driven systems, events are messages that notify other components about state changes (e.g., 'OrderPlaced'), triggering downstream workflows asynchronously.",
    "difficulty": "Beginner"
  },
  {
    "id": 29,
    "question": "Which HTTP status code should be returned when a rate limit is exceeded?",
    "options": [
      "200 OK",
      "401 Unauthorized",
      "429 Too Many Requests",
      "500 Internal Server Error"
    ],
    "answer": "429 Too Many Requests",
    "explanation": "HTTP 429 indicates the user has sent too many requests in a given amount of time. It is the standard response for rate limiting mechanisms.",
    "difficulty": "Beginner"
  },
  {
    "id": 30,
    "question": "What is 'Connection Pooling' used for?",
    "options": [
      "To keep multiple database connections open and reused to minimize the overhead of establishing connections.",
      "To balance traffic between different geographical regions.",
      "To store SQL queries in memory for faster execution.",
      "To compress data before sending it over the network."
    ],
    "answer": "To keep multiple database connections open and reused to minimize the overhead of establishing connections.",
    "explanation": "Creating connections is expensive. Pooling maintains a cache of open connections so threads can reuse them without the latency of setting up a new connection for every request.",
    "difficulty": "Beginner"
  },
  {
    "id": 31,
    "question": "What is the 'Fan-out' pattern in data processing?",
    "options": [
      "Sending a single message to one specific consumer.",
      "Sending a single message to multiple recipients or processing channels.",
      "Aggregating multiple messages into one.",
      "Stopping a message from being delivered."
    ],
    "answer": "Sending a single message to multiple recipients or processing channels.",
    "explanation": "Fan-out describes distributing a message (e.g., a new post on a social network) to multiple services (e.g., notification system, analytics feed) simultaneously.",
    "difficulty": "Beginner"
  },
  {
    "id": 32,
    "question": "Why is 'Exponential Backoff' used in retry logic?",
    "options": [
      "To ensure the data is encrypted.",
      "To increase the wait time between retries exponentially to prevent overwhelming a failing service.",
      "To decrease the wait time to get the request processed faster.",
      "To verify the user's identity before retrying."
    ],
    "answer": "To increase the wait time between retries exponentially to prevent overwhelming a failing service.",
    "explanation": "When a service fails, retrying immediately can cause a thundering herd. Exponential backoff increases the delay (e.g., 1s, 2s, 4s) between retries to give the service time to recover.",
    "difficulty": "Beginner"
  },
  {
    "id": 33,
    "question": "What is a 'Pub/Sub' (Publish-Subscribe) model?",
    "options": [
      "A direct 1-to-1 communication between client and server.",
      "A messaging pattern where senders (publishers) broadcast messages to receivers (subscribers) via a topic.",
      "A database backup strategy.",
      "A network topology for LAN connections."
    ],
    "answer": "A messaging pattern where senders (publishers) broadcast messages to receivers (subscribers) via a topic.",
    "explanation": "Pub/Sub decouples senders from receivers. Publishers send messages to a topic, and the system delivers them to all subscribers interested in that topic.",
    "difficulty": "Beginner"
  },
  {
    "id": 34,
    "question": "What is the 'Leader-Follower' (or Master-Slave) replication model?",
    "options": [
      "All nodes can accept writes simultaneously.",
      "One node (Leader) handles writes, and other nodes (Followers) replicate data and handle reads.",
      "Data is written to the followers first, then the leader.",
      "There is no distinction between nodes; they are all identical."
    ],
    "answer": "One node (Leader) handles writes, and other nodes (Followers) replicate data and handle reads.",
    "explanation": "In this model, the single Leader/Node ensures data consistency for writes, while Followers replicate the data to provide read scalability and redundancy.",
    "difficulty": "Beginner"
  },
  {
    "id": 35,
    "question": "What does the term 'Serverless' computing imply in system design?",
    "options": [
      "That there are no physical servers involved.",
      "That developers do not need to manage infrastructure provisioning and scaling, paying only for execution time.",
      "That the application runs entirely on the client's browser.",
      "That the database is removed from the architecture."
    ],
    "answer": "That developers do not need to manage infrastructure provisioning and scaling, paying only for execution time.",
    "explanation": "Serverless (e.g., AWS Lambda) abstracts server management. It auto-scales and bills based on actual compute usage, though physical servers still run the code.",
    "difficulty": "Beginner"
  },
  {
    "id": 36,
    "question": "In a distributed system using consistent hashing, why is a virtual node (vnode) strategy often employed instead of assigning a single hash range per physical node?",
    "options": [
      "To eliminate the need for a replication factor",
      "To ensure a more uniform distribution of data and minimize rebalancing overhead during scaling events",
      "To allow the system to function without a hash function",
      "To guarantee strong consistency over eventual consistency"
    ],
    "answer": "To ensure a more uniform distribution of data and minimize rebalancing overhead during scaling events",
    "explanation": "Virtual nodes randomize the data distribution by creating multiple hash tokens for each server, preventing hotspots caused by uneven hashing. This technique ensures that when a node is added or removed, only a proportional fraction of keys (specifically 1/N) needs to be remapped, rather than reshuffling the entire keyspace.",
    "difficulty": "Intermediate"
  },
  {
    "id": 37,
    "question": "When designing a write-heavy database system, why might you choose an LSM-Tree (Log-Structured Merge-Tree) structure over a standard B-Tree index?",
    "options": [
      "LSM-Trees offer faster read performance for random data access",
      "B-Trees do not support range queries",
      "LSM-Trees convert random writes into sequential writes, reducing I/O overhead and write amplification",
      "B-Trees cannot handle in-memory buffering"
    ],
    "answer": "LSM-Trees convert random writes into sequential writes, reducing I/O overhead and write amplification",
    "explanation": "LSM-Trees buffer writes in memory (MemTable) and flush them as immutable sorted files (SSTables), turning random disk writes into sequential appends which are much faster on spinning disks and SSDs. B-Trees require random disk I/O for writes to update specific locations in the tree, leading to higher write amplification.",
    "difficulty": "Intermediate"
  },
  {
    "id": 38,
    "question": "What is the primary technical trade-off when choosing a Write-Through caching strategy over a Write-Back (Write-Behind) caching strategy?",
    "options": [
      "Write-Through offers higher write latency but ensures data consistency between cache and persistent storage",
      "Write-Through increases read latency compared to Write-Back",
      "Write-Back guarantees that no data is ever lost if the cache fails",
      "Write-Through requires significantly more cache memory than Write-Back"
    ],
    "answer": "Write-Through offers higher write latency but ensures data consistency between cache and persistent storage",
    "explanation": "Write-Through synchronously writes data to both the cache and the backing database before confirming the transaction, ensuring consistency but incurring higher write latency. Write-Back writes only to the cache and lazily syncs to the database, offering lower latency but introducing a risk of data loss if the cache fails before persistence.",
    "difficulty": "Intermediate"
  },
  {
    "id": 39,
    "question": "In the context of the CAP Theorem, which of the following statements accurately describes a system that prioritizes 'AP' (Availability and Partition Tolerance) over Consistency?",
    "options": [
      "The system guarantees that all nodes see the same data at the same time",
      "The system stops accepting requests during a network partition to prevent divergence",
      "The system continues processing reads and writes despite network partitions, potentially returning stale data",
      "The system uses a two-phase commit to lock resources during updates"
    ],
    "answer": "The system continues processing reads and writes despite network partitions, potentially returning stale data",
    "explanation": "An AP system sacrifices consistency for availability, meaning it remains operational during a partition (P) and accepts requests (A), but the data returned may not be the most recent version across all nodes. CP systems would reject requests to maintain consistency, while CA systems generally cannot tolerate partitions.",
    "difficulty": "Intermediate"
  },
  {
    "id": 40,
    "question": "What is the specific role of the 'Lease' mechanism in a distributed locking system or leader election protocol (like those used in etcd or Zookeeper)?",
    "options": [
      "To encrypt the communication channel between the leader and followers",
      "To act as a time-to-live (TTL) that automatically expires the lock/leadership if the holder crashes, preventing deadlocks",
      "To permanently assign a specific shard to a specific node",
      "To increase the throughput of the database connection pool"
    ],
    "answer": "To act as a time-to-live (TTL) that automatically expires the lock/leadership if the holder crashes, preventing deadlocks",
    "explanation": "A lease grants exclusive rights for a specific duration; if the holder fails to renew (heartbeat) due to a crash, the lease expires, allowing other nodes to acquire the lock. Without a TTL mechanism, a crashed leader could hold a lock indefinitely, blocking the system indefinitely (deadlock).",
    "difficulty": "Intermediate"
  },
  {
    "id": 41,
    "question": "When building an API gateway, what is the primary purpose of implementing rate limiting using the 'Token Bucket' algorithm versus a 'Fixed Window' counter?",
    "options": [
      "Token Bucket prevents bursts of traffic entirely, ensuring a flat rate of requests",
      "Fixed Window is more computationally expensive than Token Bucket",
      "Token Bucket allows for short bursts of traffic while maintaining a defined long-term rate",
      "Fixed Window guarantees strictly better distribution of requests over time"
    ],
    "answer": "Token Bucket allows for short bursts of traffic while maintaining a defined long-term rate",
    "explanation": "The Token Bucket algorithm fills tokens at a fixed rate but allows consuming them instantly up to the bucket's capacity, accommodating traffic bursts. Fixed Window counters simply reset at intervals, which can cause 'thundering herd' spikes at the boundary of the time window (e.g., start of the minute).",
    "difficulty": "Intermediate"
  },
  {
    "id": 42,
    "question": "Why are UUIDs (Universally Unique Identifiers) often problematic as primary keys in high-scale, relational databases (like PostgreSQL/MySQL) using B-Tree indexes?",
    "options": [
      "UUIDs run out of unique values faster than integer auto-increments",
      "UUIDs are sequential, causing hotspot contention on the index leaf nodes",
      "Randomly generated UUIDs cause high index fragmentation and random disk I/O during inserts",
      "UUIDs cannot be indexed by standard relational databases"
    ],
    "answer": "Randomly generated UUIDs cause high index fragmentation and random disk I/O during inserts",
    "explanation": "B-Trees rely on sequential insertion for optimal page filling; random UUIDs require inserting rows at random locations in the index tree, leading to poor page utilization, fragmentation, and expensive random disk writes. Auto-incrementing integers append to the end of the index, which is I/O efficient.",
    "difficulty": "Intermediate"
  },
  {
    "id": 43,
    "question": "In a microservices architecture, how does the 'Sidecar' pattern facilitate cross-cutting concerns?",
    "options": [
      "By bundling all cross-cutting logic into a single monolithic library shared by all services",
      "By deploying a separate helper process alongside the main service to handle features like logging, monitoring, or networking",
      "By running a dedicated proxy server for every single user request",
      "By compiling the cross-cutting concerns directly into the operating system kernel"
    ],
    "answer": "By deploying a separate helper process alongside the main service to handle features like logging, monitoring, or networking",
    "explanation": "The Sidecar pattern abstracts infrastructure logic (like service discovery, retries, or telemetry) into a separate process running in the same container/pod. This decouples the business logic code from the platform-specific code, allowing language-agnostic feature updates (e.g., using Envoy proxy).",
    "difficulty": "Intermediate"
  },
  {
    "id": 44,
    "question": "What is the primary advantage of using a 'Circuit Breaker' in a distributed system?",
    "options": [
      "To permanently disable a failing service to save resources",
      "To allow a service to fail fast and prevent cascading failures by stopping requests to a downstream dependency that is known to be down",
      "To automatically retry failed requests infinitely until they succeed",
      "To increase the bandwidth of the network connection between services"
    ],
    "answer": "To allow a service to fail fast and prevent cascading failures by stopping requests to a downstream dependency that is known to be down",
    "explanation": "When a dependency fails repeatedly, the Circuit Breaker trips (opens) and immediately rejects subsequent calls, preventing the network or thread pools from saturating with waiting requests. It protects the caller and gives the downstream service time to recover, rather than overwhelming it with retries.",
    "difficulty": "Intermediate"
  },
  {
    "id": 45,
    "question": "How does a 'Read Replica' in a database system generally differ from a 'Leader' (Master) node in terms of data availability?",
    "options": [
      "The Read Replica handles all write operations, while the Leader handles reads",
      "The Read Replica contains a complete copy of the data but is typically read-only and handles traffic with slight replication lag",
      "The Read Replica stores only the schema and not the actual data rows",
      "The Read Replica is the only node that can perform database backups"
    ],
    "answer": "The Read Replica contains a complete copy of the data but is typically read-only and handles traffic with slight replication lag",
    "explanation": "Read replicas replicate changes from the Leader asynchronously (or semi-synchronously). While they offer a full dataset for querying to reduce load on the Leader, they exhibit 'replication lag,' meaning they may not immediately reflect the most recent write committed to the Leader.",
    "difficulty": "Intermediate"
  },
  {
    "id": 46,
    "question": "In the context of API design, what is the specific technical benefit of making a REST API operation 'Idempotent'?",
    "options": [
      "The operation returns the result faster on the second attempt",
      "The operation can be safely retried multiple times without changing the result beyond the first attempt",
      "The operation uses less bandwidth than non-idempotent operations",
      "The operation automatically authenticates the user for future requests"
    ],
    "answer": "The operation can be safely retried multiple times without changing the result beyond the first attempt",
    "explanation": "Idempotency ensures that N > 0 identical requests have the same side effect as a single request. This is critical for distributed systems where network errors may cause clients to retry requests (like payment processing) without accidentally charging a user twice.",
    "difficulty": "Intermediate"
  },
  {
    "id": 47,
    "question": "What is the main purpose of the 'Exponential Backoff' algorithm in retry logic for network communications?",
    "options": [
      "To prioritize certain users over others during high load",
      "To progressively increase the wait time between retries to reduce congestion and give the service time to recover",
      "To decrease the wait time between retries to resolve the error as quickly as possible",
      "To ensure the request fails immediately if the first try fails"
    ],
    "answer": "To progressively increase the wait time between retries to reduce congestion and give the service time to recover",
    "explanation": "Exponential backoff increases the delay (e.g., 1s, 2s, 4s, 8s) between retry attempts. This prevents 'thundering herd' effects where many clients simultaneously overwhelm a recovering service, unlike fixed-interval retries.",
    "difficulty": "Intermediate"
  },
  {
    "id": 48,
    "question": "When designing a system for 'Eventual Consistency', which mechanism is typically used to resolve conflicts when two geographically separated users update the same record simultaneously?",
    "options": [
      "Two-Phase Commit (2PC)",
      "ACID transactions",
      "Vector Clocks or Last-Write-Wins (LWW) timestamps",
      "Immediate locking of the database row"
    ],
    "answer": "Vector Clocks or Last-Write-Wins (LWW) timestamps",
    "explanation": "Since synchronous locking violates high availability in distributed systems, eventual consistency relies on conflict resolution strategies. Vector clocks capture causality to detect concurrent versions, while LWW uses timestamps to heuristically declare the 'latest' update the winner.",
    "difficulty": "Intermediate"
  },
  {
    "id": 49,
    "question": "What is the primary function of a 'Bloom Filter' in a database or storage system context?",
    "options": [
      "To compress the data before storage",
      "To efficiently check if an element is definitely NOT in a set, avoiding expensive disk lookups",
      "To encrypt the data at rest",
      "To sort the query results by relevance"
    ],
    "answer": "To efficiently check if an element is definitely NOT in a set, avoiding expensive disk lookups",
    "explanation": "A Bloom Filter is a probabilistic memory structure that allows for fast negative lookups (false positives are possible, false negatives are not). Systems like HBase or Cassandra use it to avoid seeking data on disk for keys that do not exist, significantly improving read performance.",
    "difficulty": "Intermediate"
  },
  {
    "id": 50,
    "question": "Why is a 'Long-Polling' mechanism often preferred over standard 'Short-Polling' for real-time notifications?",
    "options": [
      "Long-Polling maintains a constant connection to the server, never closing the HTTP request",
      "Long-Polling reduces latency and server load by keeping the request open until the server has data or a timeout occurs",
      "Short-Polling requires the use of WebSockets",
      "Long-Polling uses UDP instead of TCP for faster transmission"
    ],
    "answer": "Long-Polling reduces latency and server load by keeping the request open until the server has data or a timeout occurs",
    "explanation": "Short-polling requires the client to repeatedly hit the server at high frequency (chatter), which wastes resources and adds lag. Long-polling holds the connection open, allowing the server to respond immediately when data arrives, mimicking 'push' behavior over HTTP.",
    "difficulty": "Intermediate"
  },
  {
    "id": 51,
    "question": "In a distributed hash table (DHT) like Chord, what is the primary purpose of a 'Finger Table'?",
    "options": [
      "To store the actual data values",
      "To accelerate lookups by maintaining shortcuts to nodes at logarithmic intervals in the key space",
      "To authenticate users accessing the data",
      "To backup data to a remote location"
    ],
    "answer": "To accelerate lookups by maintaining shortcuts to nodes at logarithmic intervals in the key space",
    "explanation": "Without Finger Tables, a node would only know its immediate successor, resulting in O(N) lookup time. Finger Tables store references to nodes at distances that are powers of two, enabling O(log N) lookup efficiency by allowing the query to 'jump' large distances across the ring.",
    "difficulty": "Intermediate"
  },
  {
    "id": 52,
    "question": "What is the defining characteristic of a 'Chatty' or 'Conversational' API in a microservices architecture, and why is it generally considered an anti-pattern?",
    "options": [
      "It uses GraphQL for all communications",
      "It makes multiple synchronous network calls to complete a single business transaction, increasing latency and coupling",
      "It communicates asynchronously using message queues",
      "It caches all responses on the client side"
    ],
    "answer": "It makes multiple synchronous network calls to complete a single business transaction, increasing latency and coupling",
    "explanation": "A chatty API requires the client to make many small calls (e.g., get user, then get profile, then get settings) sequentially. This drastically increases cumulative latency due to network round-trips and creates tight coupling, making the system brittle.",
    "difficulty": "Intermediate"
  },
  {
    "id": 53,
    "question": "In the context of Domain-Driven Design (DDD) within microservices, what is a 'Bounded Context'?",
    "options": [
      "A database schema that limits the size of a table",
      "A specific part of the domain logic where a particular term (like 'Order') has a specific, unambiguous meaning",
      "A firewall rule that restricts API access",
      "A limit on the number of concurrent users"
    ],
    "answer": "A specific part of the domain logic where a particular term (like 'Order') has a specific, unambiguous meaning",
    "explanation": "A Bounded Context delineates the boundaries of a specific domain model or sub-domain. Inside a context, 'User' might mean an entity in a database; in another, 'User' might be a read-model projection. This prevents ambiguity and coupling between different business capabilities.",
    "difficulty": "Intermediate"
  },
  {
    "id": 54,
    "question": "How does a 'Forward Proxy' differ from a 'Reverse Proxy'?",
    "options": [
      "A Forward Proxy protects the server; a Reverse Proxy protects the client",
      "A Forward Proxy sits in front of the client to handle requests to the internet; a Reverse Proxy sits in front of the server to handle requests from clients",
      "A Forward Proxy uses HTTPS; a Reverse Proxy uses HTTP",
      "A Forward Proxy caches server responses; a Reverse Proxy caches client requests"
    ],
    "answer": "A Forward Proxy sits in front of the client to handle requests to the internet; a Reverse Proxy sits in front of the server to handle requests from clients",
    "explanation": "A Forward Proxy acts on behalf of the client (often used to bypass corporate firewalls or anonymize traffic). A Reverse Proxy acts on behalf of the server (used for load balancing, security, and caching) and accepts traffic from the internet before routing it to internal services.",
    "difficulty": "Intermediate"
  },
  {
    "id": 55,
    "question": "What is the 'Shard' in a database architecture?",
    "options": [
      "A specific type of SQL query optimizer",
      "A horizontal partition of data that contains a subset of the total dataset, distributed across nodes",
      "A backup of the primary database",
      "The connection string used to access the database"
    ],
    "answer": "A horizontal partition of data that contains a subset of the total dataset, distributed across nodes",
    "explanation": "Sharding involves splitting a large database into smaller, faster, more easily managed parts called data shards. It is a horizontal partition (splitting rows by range or hash) to distribute the load across multiple machines, unlike vertical partitioning which splits columns.",
    "difficulty": "Intermediate"
  },
  {
    "id": 56,
    "question": "Why is the 'Two-Phase Commit' (2PC) protocol rarely used in high-latency, large-scale distributed systems?",
    "options": [
      "It is too fast for modern networks",
      "It is a blocking protocol that locks resources during the prepare phase, reducing availability and throughput",
      "It does not guarantee ACID properties",
      "It requires all nodes to be located in the same data center"
    ],
    "answer": "It is a blocking protocol that locks resources during the prepare phase, reducing availability and throughput",
    "explanation": "2PC requires all participants to vote 'Yes' and lock their resources before committing. If a single node fails, the entire transaction hangs or rolls back, making the system fragile and unavailable during partitions. Modern systems prefer Saga patterns (compensating transactions) for better availability.",
    "difficulty": "Intermediate"
  },
  {
    "id": 57,
    "question": "In system design, what does 'Backpressure' refer to?",
    "options": [
      "The latency introduced by compressing data",
      "A mechanism to manage the flow of data when a producer generates data faster than a consumer can process it",
      "The stress on the network router",
      "The authentication token expiration time"
    ],
    "answer": "A mechanism to manage the flow of data when a producer generates data faster than a consumer can process it",
    "explanation": "Backpressure prevents a fast producer from overwhelming a slow consumer, which would lead to resource exhaustion (OOM) or crashes. It is implemented via flow control protocols (like TCP windowing) or buffering strategies (blocking, dropping, or signaling load to the producer).",
    "difficulty": "Intermediate"
  },
  {
    "id": 58,
    "question": "What is the primary benefit of using 'CRDTs' (Conflict-free Replicated Data Types) in collaborative distributed applications?",
    "options": [
      "They require a central authority to validate all changes",
      "They allow concurrent updates to replicated data without coordination, resolving conflicts mathematically",
      "They enforce strong consistency at the cost of availability",
      "They reduce the overall data size by 50%"
    ],
    "answer": "They allow concurrent updates to replicated data without coordination, resolving conflicts mathematically",
    "explanation": "CRDTs are data structures designed to be replicated across nodes, allowing updates to occur independently and concurrently without locking. When updates are synced, the mathematical properties of the CRDT guarantee convergence to a consistent state without conflict resolution meetings.",
    "difficulty": "Intermediate"
  },
  {
    "id": 59,
    "question": "What is the function of a 'Warm Standby' in disaster recovery?",
    "options": [
      "Data is mirrored in real-time, and the backup system is turned on and ready to take over instantly",
      "Data is mirrored, but the backup system is powered off and requires manual intervention to start",
      "Data is backed up to tape once a week",
      "The system operates without any backup at all"
    ],
    "answer": "Data is mirrored, but the backup system is powered off and requires manual intervention to start",
    "explanation": "A Warm Standby implies the hardware is running and data is synchronized (or near synchronized)",
    "difficulty": "Intermediate"
  },
  {
    "id": 60,
    "question": "In the context of SQL databases, what is a 'Clustered Index'?",
    "options": [
      "A separate data structure that holds pointers to the actual data rows",
      "An index where the data rows themselves are stored in the leaf nodes of the index, sorted by the key",
      "An index that spans multiple tables",
      "A redundant copy of the data used only for auditing"
    ],
    "answer": "An index where the data rows themselves are stored in the leaf nodes of the index, sorted by the key",
    "explanation": "In a Clustered Index, the table data is physically sorted and stored on disk based on the index key. Because you can physically sort data only one way, a table can have only one clustered index (usually the primary key), unlike non-clustered indexes which point to the data.",
    "difficulty": "Intermediate"
  },
  {
    "id": 61,
    "question": "What is the main advantage of 'Column-Oriented Storage' (like Parquet or BigTable) over 'Row-Oriented Storage' for analytical workloads?",
    "options": [
      "Column-oriented storage is faster for inserting new rows",
      "Column-oriented storage allows efficient compression and I/O by reading only the specific columns needed for a query",
      "Column-oriented storage enforces ACID transactions better",
      "Column-oriented storage is required for primary keys"
    ],
    "answer": "Column-oriented storage allows efficient compression and I/O by reading only the specific columns needed for a query",
    "explanation": "Analytical queries often aggregate a few columns across millions of rows. Columnar storage stores data values of the same column contiguously on disk, enabling high compression ratios and reading only the required columns (projection) instead of skipping over irrelevant row data.",
    "difficulty": "Intermediate"
  },
  {
    "id": 62,
    "question": "How does 'Graph Database' architecture (like Neo4j) fundamentally differ from a Relational Database architecture?",
    "options": [
      "Graph databases use tables; Relational databases use documents",
      "Graph databases store nodes and edges as first-class citizens, optimizing for traversing relationships (joins)",
      "Relational databases are only for small datasets",
      "Graph databases cannot be indexed"
    ],
    "answer": "Graph databases store nodes and edges as first-class citizens, optimizing for traversing relationships (joins)",
    "explanation": "Relational databases compute relationships (JOINs) at query time via index lookups, which becomes expensive for deep queries (O(M*N)). Graph databases use 'index-free adjacency' where a node has direct pointers to its neighbors, allowing traversal with O(1) per hop, making them ideal for social networks or fraud detection.",
    "difficulty": "Intermediate"
  },
  {
    "id": 63,
    "question": "What is the 'Thundering Herd' problem in the context of high-scale caching?",
    "options": [
      "When too many users download the application at once",
      "When a cached object expires, causing a massive number of simultaneous requests to hit the origin database simultaneously to regenerate the cache",
      "When the database locks up due to too many writes",
      "When the cache server runs out of RAM"
    ],
    "answer": "When a cached object expires, causing a massive number of simultaneous requests to hit the origin database simultaneously to regenerate the cache",
    "explanation": "If a heavy cache key expires, hundreds/thousands of concurrent requests might detect a cache miss and all trigger the expensive database query. This can overload the database. It is solved by mechanisms like 'Locking' or 'Probabilistic Early Expiration'.",
    "difficulty": "Intermediate"
  },
  {
    "id": 64,
    "question": "In the 'Saga' pattern for distributed transactions, what is a 'Compensating Transaction'?",
    "options": [
      "A transaction that commits the data permanently",
      "A transaction that reverses the effects of a previously committed transaction in case of a failure in the workflow",
      "A transaction that queries the state of the database",
      "A transaction that optimizes the database index"
    ],
    "answer": "A transaction that reverses the effects of a previously committed transaction in case of a failure in the workflow",
    "explanation": "Since distributed locks (2PC) are avoided in microservices, Sagas break a transaction into a sequence of local transactions. If a step fails, compensating transactions (e.g., 'CancelOrder') are executed to roll back the effects of previously completed steps to restore consistency.",
    "difficulty": "Intermediate"
  },
  {
    "id": 65,
    "question": "What is the primary technical limitation of a 'Monolithic' architecture that drives the adoption of microservices?",
    "options": [
      "Monoliths cannot use SQL databases",
      "Monoliths couple all functionality into a single deployment unit, preventing independent scaling and deployment of components",
      "Monoliths are always slower than microservices for small workloads",
      "Monoliths cannot be written in Java"
    ],
    "answer": "Monoliths couple all functionality into a single deployment unit, preventing independent scaling and deployment of components",
    "explanation": "In a monolith, a bug in a minor feature or high load in one module can crash the entire application, and the entire app must be redeployed for a single line change. Microservices isolate failures and allow scaling specific services (e.g., Scale Image Processing, not Login Service).",
    "difficulty": "Intermediate"
  },
  {
    "id": 66,
    "question": "What is 'Geographic Sharding' (or Geo-Partitioning)?",
    "options": [
      "Replicating the entire database to every region in the world",
      "Distributing data across regions based on a specific attribute (like 'user_country') to keep data local to the user",
      "Storing backups in a mountain bunker",
      "Using a single global database server"
    ],
    "answer": "Distributing data across regions based on a specific attribute (like 'user_country') to keep data local to the user",
    "explanation": "Geo-sharding directs traffic and data storage based on location (e.g., European users stored in EU DB, US users in US DB). This reduces latency for reads/writes and helps with data sovereignty laws (GDPR), unlike replication where every node has all data.",
    "difficulty": "Intermediate"
  },
  {
    "id": 67,
    "question": "Why is 'Jitter' (randomization) added to the retry interval timeout in exponential backoff mechanisms?",
    "options": [
      "To slow down the client intentionally",
      "To synchronize retries from multiple clients to ensure they hit the server at the same time",
      "To prevent 'Thundering Herd' or correlated retries where clients retry in lockstep, overwhelming the recovering service",
      "To encrypt the retry packet"
    ],
    "answer": "To prevent 'Thundering Herd' or correlated retries where clients retry in lockstep, overwhelming the recovering service",
    "explanation": "If all clients experience the same failure and retry at exactly the same calculated intervals (e.g., 2s, 4s), they remain synchronized. Adding random jitter (e.g., 2s +/- random(0-1s)) spreads out the retries, preventing the server from being spiked by simultaneous retry waves.",
    "difficulty": "Intermediate"
  },
  {
    "id": 68,
    "question": "In a Content Delivery Network (CDN), what is an 'Edge Location'?",
    "options": [
      "The main data center where the origin server resides",
      "A physically distributed point-of-presence (POP) located closer to the end-user to cache and deliver content with lower latency",
      "The network boundary firewall",
      "The last mile of the user's home internet connection"
    ],
    "answer": "A physically distributed point-of-presence (POP) located closer to the end-user to cache and deliver content with lower latency",
    "explanation": "Edge locations are the peripheral nodes of the CDN. By caching static assets (images, CSS, video) geographically close to the user, they reduce the number of network hops and RTT (Round Trip Time) required to fetch data, significantly improving load times.",
    "difficulty": "Intermediate"
  },
  {
    "id": 69,
    "question": "What is the 'Split-Brain' problem in distributed databases?",
    "options": [
      "When the database query optimizer gets confused",
      "When a network partition causes two distinct subsets of nodes to believe they are the primary leader, leading to data divergence",
      "When the user interface fails to load",
      "When the database runs out of allocated disk space"
    ],
    "answer": "When a network partition causes two distinct subsets of nodes to believe they are the primary leader, leading to data divergence",
    "explanation": "If the cluster cannot establish a quorum (e.g., due to network failure), both sides might continue accepting writes assuming the other side is dead. This results in two conflicting 'truths' that are difficult or impossible to reconcile later.",
    "difficulty": "Intermediate"
  },
  {
    "id": 70,
    "question": "In the context of the CAP theorem, what behavior defines a 'CP' (Consistency and Partition Tolerance) system during a network partition?",
    "options": [
      "The system continues to accept reads and writes, serving potentially stale data to maintain availability.",
      "The system rejects all write operations to prevent divergence and may return errors until the partition resolves.",
      "The system automatically routes traffic to the nearest datacenter to minimize latency, sacrificing strong consistency.",
      "The system uses a gossip protocol to asynchronously reconcile state once the network partition heals."
    ],
    "answer": "The system rejects all write operations to prevent divergence and may return errors until the partition resolves.",
    "explanation": "A CP system prioritizes data consistency over availability; it must block or abort writes that cannot be synchronized with the quorum to prevent split-brain. An AP system (Option A) would serve stale data, while Option C describes latency optimization strategies often found in eventually consistent systems.",
    "difficulty": "Advanced"
  },
  {
    "id": 71,
    "question": "How does the 'Leaky Bucket' algorithm differ from the 'Token Bucket' algorithm regarding traffic shaping?",
    "options": [
      "Leaky Bucket smooths traffic output to a fixed rate, discarding excess packets, while Token Bucket allows bursts up to a accumulated capacity.",
      "Leaky Bucket allows for large bursts of traffic immediately if the bucket is full, while Token Bucket strictly paces packets.",
      "Leaky Bucket relies on client-side throttling, whereas Token Bucket is exclusively a server-side congestion control mechanism.",
      "Leaky Bucket is only used for TCP traffic, while Token Bucket is designed for UDP packet streams."
    ],
    "answer": "Leaky Bucket smooths traffic output to a fixed rate, discarding excess packets, while Token Bucket allows bursts up to a accumulated capacity.",
    "explanation": "The Leaky Bucket enforces a strict, uniform outflow rate, acting like a queue where overflow results in packet drops. The Token Bucket permits traffic to burst up to the size of the bucket (tokens) as long as the long-term average rate is maintained.",
    "difficulty": "Advanced"
  },
  {
    "id": 72,
    "question": "What is the primary technical disadvantage of using a 'Write-Through' caching strategy?",
    "options": [
      "Data in the cache is permanently lost if the database fails.",
      "Write latency is higher because data must be written to both the cache and the persistent storage before confirmation.",
      "The cache is frequently populated with data that is never read, leading to 'cache pollution'.",
      "It requires complex conflict resolution mechanisms to handle concurrent write operations."
    ],
    "answer": "Write latency is higher because data must be written to both the cache and the persistent storage before confirmation.",
    "explanation": "Write-Through requires synchronously updating the backing store and the cache, ensuring consistency but adding the disk I/O latency to the critical path of the write operation. Write-Back (Option A) carries the risk of data loss, while Option B refers to Read-Through.",
    "difficulty": "Advanced"
  },
  {
    "id": 73,
    "question": "In a distributed system using Chord or similar DHTs, why are 'Virtual Nodes' (vnodes) used on the hash ring?",
    "options": [
      "To ensure that the hash function produces a uniform distribution regardless of the input key space.",
      "To allow physical nodes to hold multiple segments of the hash ring, ensuring a more even distribution of data and load.",
      "To encrypt the data keys before they are stored in the distributed hash table.",
      "To enable automatic replication of data across different geographic availability zones."
    ],
    "answer": "To allow physical nodes to hold multiple segments of the hash ring, ensuring a more even distribution of data and load.",
    "explanation": "Without vnodes, a small number of nodes might happen to own a large portion of the key space (non-uniform distribution). Vnodes allow each physical server to scatter its presence around the ring, statistically balancing the load and minimizing the impact of node churn.",
    "difficulty": "Advanced"
  },
  {
    "id": 74,
    "question": "What specific problem does 'Consistent Hashing' solve in a distributed cache cluster that standard modulo hashing does not?",
    "options": [
      "It prevents the 'Thundering Herd' problem by locking cache keys during updates.",
      "It minimizes the number of keys that need to be remapped when a node is added or removed from the cluster.",
      "It ensures strong consistency between the cache and the primary database.",
      "It automatically shards the database based on the query patterns."
    ],
    "answer": "It minimizes the number of keys that need to be remapped when a node is added or removed from the cluster.",
    "explanation": "In modulo hashing ($node = hash(key) \\% N$), changing $N$ (number of nodes) destroys the entire cache mapping. Consistent Hashing maps both nodes and keys to a ring, ensuring that adding/removing a node only affects the keys adjacent to it on the ring.",
    "difficulty": "Advanced"
  },
  {
    "id": 75,
    "question": "In the context of SQL database isolation levels, what phenomenon does the 'Serializable' isolation level prevent that 'Repeatable Read' does not?",
    "options": [
      "Non-repeatable reads",
      "Dirty reads",
      "Phantoms (new rows appearing in a range query upon re-execution)",
      "Lost updates"
    ],
    "answer": "Phantoms (new rows appearing in a range query upon re-execution)",
    "explanation": "Repeatable Read prevents non-repeatable reads and dirty reads by locking read rows, but it does not prevent new rows from being inserted into a range, causing Phantoms. Serializable isolation is the strictest level, preventing phantoms by typically locking ranges or using predicate locking.",
    "difficulty": "Advanced"
  },
  {
    "id": 76,
    "question": "When designing a REST API, what is the primary function of the 'ETag' header?",
    "options": [
      "To uniquely identify the user session and prevent CSRF attacks.",
      "To enable conditional requests and cache validation using a digest of the resource representation.",
      "To specify the format of the response payload (e.g., JSON vs XML).",
      "To compress the response body to reduce network bandwidth usage."
    ],
    "answer": "To enable conditional requests and cache validation using a digest of the resource representation.",
    "explanation": "The ETag (Entity Tag) is a fingerprint of the content. Clients use it with `If-None-Match` headers to ask the server to return the resource only if it has changed (304 Not Modified), saving bandwidth. Option A refers to Session IDs/Cookies.",
    "difficulty": "Advanced"
  },
  {
    "id": 77,
    "question": "What is the 'Split-Brain' problem in distributed databases?",
    "options": [
      "A situation where two independent nodes or clusters both believe they are the active leader, potentially causing data corruption or conflicts.",
      "A deadlock scenario where two transactions wait indefinitely for each other to release locks.",
      "A network latency issue where replicas are so out of sync that the database performs a full table scan.",
      "A memory leak caused by excessive spawning of child processes during a failure recovery."
    ],
    "answer": "A situation where two independent nodes or clusters both believe they are the active leader, potentially causing data corruption or conflicts.",
    "explanation": "Split-Brain occurs when the network partitions and the quorum mechanism fails, allowing disjointed sections of the cluster to operate autonomously. This leads to diverging data histories (inconsistent writes) that are difficult or impossible to reconcile later.",
    "difficulty": "Advanced"
  },
  {
    "id": 78,
    "question": "Why is a B-Tree typically preferred over an LSM-Tree (Log-Structured Merge-tree) for a read-heavy, write-once database workload?",
    "options": [
      "LSM-Trees do not support secondary indexes, whereas B-Trees do.",
      "B-Trees offer lower read amplification because the data is stored in a single, sorted structure without requiring merging of multiple files.",
      "LSM-Trees require random disk I/O for writes, which is slower than the sequential I/O used by B-Trees.",
      "B-Trees compress data more efficiently than LSM-Trees, reducing storage costs."
    ],
    "answer": "B-Trees offer lower read amplification because the data is stored in a single, sorted structure without requiring merging of multiple files.",
    "explanation": "LSM-Trees optimize writes by buffering them in memory (MemTable) and flushing to SSTables, scattering reads across multiple sorted files (high read amplification). B-Trees keep data keyed in one place (the tree nodes), allowing faster lookups at the cost of slower random writes.",
    "difficulty": "Advanced"
  },
  {
    "id": 79,
    "question": "What is the purpose of a 'Sidecar Proxy' in a Service Mesh architecture (e.g., Istio, Linkerd)?",
    "options": [
      "To act as a centralized load balancer that routes all external traffic into the cluster.",
      "To handle service-to-service communication logic (retry, circuit breaking, mTLS) abstracted from the application code.",
      "To compile the microservice source code into a binary executable for faster deployment.",
      "To store the persistent state of the microservice to ensure data durability."
    ],
    "answer": "To handle service-to-service communication logic (retry, circuit breaking, mTLS) abstracted from the application code.",
    "explanation": "The sidecar proxy runs alongside the application instance and intercepts all network traffic in and out. This offloads complex networking and reliability features from the application code, allowing for centralized control (observability and security) via the control plane.",
    "difficulty": "Advanced"
  },
  {
    "id": 80,
    "question": "In a message queue system (e.g., Kafka, RabbitMQ), what distinguishes 'Idempotence' from 'Exactly-Once' processing?",
    "options": [
      "Idempotence ensures an operation is performed only once, while Exactly-Once ensures the outcome is as if it happened once, even if retried.",
      "Exactly-Once guarantees message delivery, while Idempotence guarantees the order of messages.",
      "Idempotence refers to the producer, while Exactly-Once refers only to the consumer.",
      "There is no difference; the terms are interchangeable in distributed systems theory."
    ],
    "answer": "Idempotence ensures an operation is performed only once, while Exactly-Once ensures the outcome is as if it happened once, even if retried.",
    "explanation": "Idempotence means applying the same operation multiple times yields the same result (e.g., `x = 5`). Exactly-Once is a delivery semantic involving coordination between producer and broker (transactions/ids) to ensure a message is neither lost nor duplicated, effectively handling retries so they appear idempotent.",
    "difficulty": "Advanced"
  },
  {
    "id": 81,
    "question": "Which consistency model allows reads to return any value, including stale ones, but guarantees that if no new updates are made, eventually all reads will return the latest write?",
    "options": [
      "Strong Consistency",
      "Causal Consistency",
      "Eventual Consistency",
      "Read-Your-Writes Consistency"
    ],
    "answer": "Eventual Consistency",
    "explanation": "Eventual Consistency (Option C) explicitly trades immediate consistency for high availability, guaranteeing only that the system will converge to a consistent state *if* updates stop. Strong consistency guarantees immediate up-to-date reads, while Causal consistency preserves the order of related operations.",
    "difficulty": "Advanced"
  },
  {
    "id": 82,
    "question": "In a distributed system, what is the 'Phi Accrual Failure Detector' designed to improve upon compared to traditional heartbeat failure detectors?",
    "options": [
      "It reduces the bandwidth usage by compressing the heartbeat packets.",
      "It calculates a suspicion level based on the distribution of heartbeat arrival times, adapting to network fluctuations rather than using a fixed timeout.",
      "It guarantees zero false positives in failure detection.",
      "It eliminates the need for a consensus protocol like Raft or Paxos."
    ],
    "answer": "It calculates a suspicion level based on the distribution of heartbeat arrival times, adapting to network fluctuations rather than using a fixed timeout.",
    "explanation": "Traditional detectors use a fixed timeout which can cause false positives during network congestion. Phi Accrual uses a statistical window of arrival times to compute a dynamic 'Phi' value representing the likelihood of failure, allowing the system to be more robust to temporary network latency.",
    "difficulty": "Advanced"
  },
  {
    "id": 83,
    "question": "When designing a system for high write throughput, why might you choose 'Write-Ahead Logging' (WAL) even though it adds disk I/O?",
    "options": [
      "It allows the database to perform random writes to the data file immediately, bypassing the buffer pool.",
      "It ensures atomicity and durability by writing changes to a sequential log file before applying them to the main storage, preventing data loss on crash.",
      "It compresses the data pages to reduce the overall storage footprint on the disk.",
      "It enables multi-master replication by logging every write operation to a distributed ledger."
    ],
    "answer": "It ensures atomicity and durability by writing changes to a sequential log file before applying them to the main storage, preventing data loss on crash.",
    "explanation": "WAL provides crash recovery (atomicity/durability). By appending changes to a log sequentially before modifying the actual data pages (which might be random I/O), the system can restore the state to the last consistent point after a crash by simply replaying the log.",
    "difficulty": "Advanced"
  },
  {
    "id": 84,
    "question": "What is the 'Thundering Herd' problem in the context of load balancing and caching?",
    "options": [
      "A DDoS attack that overwhelms the load balancer with SYN packets.",
      "A situation where a massive number of clients or workers wake up simultaneously to access a resource that just became available, causing a spike in load.",
      "When a cache server fails and all traffic is immediately redirected to the origin database without throttling.",
      "The lag time between scaling up a VM and it being ready to receive traffic."
    ],
    "answer": "A situation where a massive number of clients or workers wake up simultaneously to access a resource that just became available, causing a spike in load.",
    "explanation": "This typically occurs when a cached resource expires (or a lock is released), and thousands of waiting processes all simultaneously attempt to re-fetch it or acquire the lock. This creates a massive load spike that can crash the origin server or lock service.",
    "difficulty": "Advanced"
  },
  {
    "id": 85,
    "question": "In the 'Two-Phase Commit' (2PC) protocol, what happens if the 'Coordinator' crashes after sending a 'Prepare' message but before sending a 'Commit' message?",
    "options": [
      "All participants immediately commit the transaction to ensure data availability.",
      "All participants abort the transaction because they cannot confirm the decision.",
      "Participants are blocked and must hold their locks until the coordinator recovers to resolve the transaction state.",
      "The transaction is automatically rolled back by the transaction monitor."
    ],
    "answer": "Participants are blocked and must hold their locks until the coordinator recovers to resolve the transaction state.",
    "explanation": "This is the blocking nature of 2PC. Participants have voted 'Yes' (promised to commit) but do not know the final outcome. They cannot unilaterally commit or abort and must maintain locks on resources, leading to potential deadlock or resource starvation until the coordinator recovers.",
    "difficulty": "Advanced"
  },
  {
    "id": 86,
    "question": "How does 'Range Sharding' differ from 'Hash Sharding' regarding query efficiency?",
    "options": [
      "Range sharding allows efficient range queries (e.g., 'find names between A and C') on a single shard, whereas hash sharding scatters data, requiring global queries.",
      "Hash sharding requires a full table scan for any lookup, while range sharding uses index lookups.",
      "Range sharding automatically re-balances data when nodes are added, while hash sharding requires manual intervention.",
      "Hash sharding supports composite primary keys efficiently, while range sharding does not."
    ],
    "answer": "Range sharding allows efficient range queries (e.g., 'find names between A and C') on a single shard, whereas hash sharding scatters data, requiring global queries.",
    "explanation": "Range sharding keeps data that is close in key space on the same shard, making range scans fast. Hash sharding distributes data uniformly based on a hash, meaning keys that are logically close are likely on different shards, making range queries expensive (scatter-gather).",
    "difficulty": "Advanced"
  },
  {
    "id": 87,
    "question": "What is the 'Write Skew' anomaly in database concurrency control?",
    "options": [
      "Two transactions update the same row simultaneously, resulting in a lost update.",
      "Two transactions read overlapping data sets and make disjoint updates that, when combined, violate a database constraint.",
      "A transaction reads uncommitted data from a concurrent transaction, leading to dirty reads.",
      "A transaction reads the same row twice and gets different values due to an uncommitted intermediate state."
    ],
    "answer": "Two transactions read overlapping data sets and make disjoint updates that, when combined, violate a database constraint.",
    "explanation": "Write Skew occurs in Snapshot Isolation when two transactions read the same consistent snapshot, see 'gaps', and update different rows simultaneously. The individual updates are valid, but the combined result is inconsistent (e.g., two on-call doctors both resigning simultaneously because they see the other is still on-call).",
    "difficulty": "Advanced"
  },
  {
    "id": 88,
    "question": "In Kafka, why are messages within a partition strictly ordered, but not across partitions?",
    "options": [
      "Because Kafka stores messages in a global commit log that is partitioned by topic, preventing order guarantees.",
      "Because partitions are distributed across different brokers, allowing parallel processing, which makes cross-partition ordering impossible without a global coordinator.",
      "Because producers write to partitions based on a round-robin strategy, stripping the original timestamp.",
      "Because consumers read from partitions using a random polling mechanism."
    ],
    "answer": "Because partitions are distributed across different brokers, allowing parallel processing, which makes cross-partition ordering impossible without a global coordinator.",
    "explanation": "Kafka provides ordering guarantees only within a single partition ID. Multiple partitions are handled by separate threads/brokers to increase throughput and parallelism, meaning there is no global timestamp or clock to guarantee order across them.",
    "difficulty": "Advanced"
  },
  {
    "id": 89,
    "question": "What is the primary benefit of using 'Application-Level Sharding' over a generic 'Database Sharding' solution?",
    "options": [
      "Application-level sharding allows developers to implement complex custom routing logic and data-aware affinity that generic databases might not support.",
      "Application-level sharding eliminates the need for a load balancer.",
      "Generic database sharding does not support horizontal scaling, whereas application-level sharding does.",
      "Application-level sharding automatically handles distributed transactions (ACID) across nodes."
    ],
    "answer": "Application-level sharding allows developers to implement complex custom routing logic and data-aware affinity that generic databases might not support.",
    "explanation": "By handling sharding at the application layer, developers can optimize for specific query patterns (e.g., collocating a user's profile and messages on the same shard). Generic DB sharding offers ease of use but may be less flexible for highly specific, multi-tenant, or relationship-heavy data models.",
    "difficulty": "Advanced"
  },
  {
    "id": 90,
    "question": "What is the 'Stop-the-World' phenomenon in Garbage Collection (GC) and how does it impact a high-throughput system?",
    "options": [
      "It refers to the JVM pausing all application threads to perform heap defragmentation and object collection, causing latency spikes.",
      "It is a mechanism where the database locks all tables to prevent writes during a backup.",
      "It occurs when the network fails and all microservices stop processing requests.",
      "It is a synchronization point where all threads wait for a global lock to be released."
    ],
    "answer": "It refers to the JVM pausing all application threads to perform heap defragmentation and object collection, causing latency spikes.",
    "explanation": "During a stop-the-world pause, the JVM suspends all non-GC threads to identify and reclaim unused memory safely. In high-throughput, low-latency systems, these pauses can cause timeouts, increased p99 latency, and SLA violations.",
    "difficulty": "Advanced"
  },
  {
    "id": 91,
    "question": "Why are 'Bloom Filters' frequently used in conjunction with LSM-Tree based databases like RocksDB or Cassandra?",
    "options": [
      "To compress the SSTables before they are written to disk.",
      "To quickly check if a key might exist in a Sorted String Table (SSTable) on disk, avoiding expensive I/O for non-existent keys.",
      "To ensure that all writes are persisted to the commit log before being acknowledged.",
      "To encrypt the keys stored in the MemTable."
    ],
    "answer": "To quickly check if a key might exist in a Sorted String Table (SSTable) on disk, avoiding expensive I/O for non-existent keys.",
    "explanation": "Bloom filters are memory-efficient probabilistic structures. They allow the database to check if a key *definitely does not* exist in an SSTable, skipping the disk read entirely if the filter returns negative. This significantly reduces read amplification for miss operations.",
    "difficulty": "Advanced"
  },
  {
    "id": 92,
    "question": "In the context of distributed consensus (Raft), what is a 'Log Replication'?",
    "options": [
      "The process of backing up the data from the Leader to an S3 bucket.",
      "The Leader accepting entries from clients, appending them to its log, and sending AppendEntries RPCs to followers to copy the log entries.",
      "The Followers creating a snapshot of their state and sending it to the Leader.",
      "The mechanism used to elect a new Leader when the current one crashes."
    ],
    "answer": "The Leader accepting entries from clients, appending them to its log, and sending AppendEntries RPCs to followers to copy the log entries.",
    "explanation": "In Raft, the Leader handles all client requests. It writes the request to its log (uncommitted) and then replicates this entry to Followers in parallel using `AppendEntries` RPCs. Once a majority acknowledges the entry, it is committed.",
    "difficulty": "Advanced"
  },
  {
    "id": 93,
    "question": "What is 'Read Repair' in a Dynamo-style distributed database?",
    "options": [
      "A process that runs in the background to synchronize data between replicas by comparing version vectors and updating outdated nodes.",
      "A mechanism to roll back the database to a previous state if a write fails.",
      "An index rebuilding operation that fixes corrupted B-Trees.",
      "The act of redirecting a read request to a node with lower latency."
    ],
    "answer": "A process that runs in the background to synchronize data between replicas by comparing version vectors and updating outdated nodes.",
    "explanation": "Dynamo-style systems are often Eventually Consistent. Read Repair (triggered during a read if inconsistency is detected, or as a background task) ensures that all replicas holding stale data are updated to the latest version to achieve eventual consistency.",
    "difficulty": "Advanced"
  },
  {
    "id": 94,
    "question": "How does the 'Circuit Breaker' pattern improve system resilience?",
    "options": [
      "By automatically increasing the resources (CPU/RAM) allocated to a failing service.",
      "By failing fast and blocking outbound calls to a failing service for a set duration, preventing resource exhaustion and cascading failures.",
      "By retrying failed requests indefinitely until the service recovers.",
      "By restarting the failing service automatically."
    ],
    "answer": "By failing fast and blocking outbound calls to a failing service for a set duration, preventing resource exhaustion and cascading failures.",
    "explanation": "When a service fails repeatedly (threshold exceeded), the Circuit Breaker trips (opens). It immediately rejects subsequent calls, preventing the client from waiting on timeouts (which consumes threads/memory). It periodically allows a test request to check if the service has recovered.",
    "difficulty": "Advanced"
  },
  {
    "id": 95,
    "question": "What is the main trade-off when using 'Chatty' (fine-grained) microservices versus 'Coarse-grained' services?",
    "options": [
      "Chatty services are easier to deploy but harder to test.",
      "Coarse-grained services increase network latency due to protocol overhead.",
      "Chatty services lead to high network overhead and latency due to numerous inter-service calls, whereas coarse-grained services might suffer from monolithic deployment coupling.",
      "Coarse-grained services cannot scale independently."
    ],
    "answer": "Chatty services lead to high network overhead and latency due to numerous inter-service calls, whereas coarse-grained services might suffer from monolithic deployment coupling.",
    "explanation": "Fine-grained decomposition improves isolation and scalability but introduces performance penalties (serialization, network round-trips). Coarse-grained services reduce this overhead but reintroduce tighter coupling between domain concepts and deployment lifecycles.",
    "difficulty": "Advanced"
  },
  {
    "id": 96,
    "question": "In HTTP/2, what is 'Multiplexing' and how does it affect web performance?",
    "options": [
      "It allows a single TCP connection to carry multiple streams of requests and responses concurrently, eliminating Head-of-Line (HOL) blocking.",
      "It compresses HTTP headers to reduce bandwidth usage.",
      "It forces the server to push resources to the client before they are requested.",
      "It encrypts the payload using TLS 1.3."
    ],
    "answer": "It allows a single TCP connection to carry multiple streams of requests and responses concurrently, eliminating Head-of-Line (HOL) blocking.",
    "explanation": "In HTTP/1.1, only one request could be in-flight at a time per connection (without pipelining). HTTP/2 multiplexing interleaves frames from multiple requests, removing the HOL blocking where one large slow request blocked subsequent requests.",
    "difficulty": "Advanced"
  },
  {
    "id": 97,
    "question": "What is the 'Gossip Protocol' (or Epidemic Protocol) used for in large-scale distributed systems?",
    "options": [
      "To securely exchange encryption keys between nodes.",
      "To disseminate state information (e.g., membership, failure detection) periodically to random peers, ensuring eventual consistency across the cluster.",
      "To order messages in a distributed queue.",
      "To elect the master node in a centralized database."
    ],
    "answer": "To disseminate state information (e.g., membership, failure detection) periodically to random peers, ensuring eventual consistency across the cluster.",
    "explanation": "Gossip protocols are robust and scalable. Each node periodically picks a random peer to exchange state with with. Information spreads exponentially (like an epidemic), ensuring the entire cluster eventually converges on the same state without a central coordinator.",
    "difficulty": "Advanced"
  },
  {
    "id": 98,
    "question": "What is 'Sticky Session' (Session Affinity) in a load balancer context?",
    "options": [
      "Assigning a specific user to a specific server for the duration of their session to maintain local state.",
      "Keeping a TCP connection open indefinitely to reduce handshake overhead.",
      "Redirecting a user to a different server based on their geographic location.",
      "Encrypting the session cookie to prevent man-in-the-middle attacks."
    ],
    "answer": "Assigning a specific user to a specific server for the duration of their session to maintain local state.",
    "explanation": "If a server stores user state locally (in-memory), subsequent requests must go to *that* specific server. The Load Balancer achieves this via Sticky Sessions (often using a cookie). This complicates horizontal scaling and fault tolerance (if that server dies, the state is lost).",
    "difficulty": "Advanced"
  },
  {
    "id": 99,
    "question": "Why is the 'SYN Cookie' mechanism used by servers under SYN Flood attacks?",
    "options": [
      "To encrypt the SYN packet payload.",
      "To validate the client's IP address against a whitelist.",
      "To allocate connection state only after the client completes the handshake, avoiding resource exhaustion during the initial connection setup.",
      "To drop all incoming SYN packets to filter out malicious traffic."
    ],
    "answer": "To allocate connection state only after the client completes the handshake, avoiding resource exhaustion during the initial connection setup.",
    "explanation": "In a SYN flood, the attacker sends many SYN packets but doesn't complete the handshake, filling the server's backlog queue. SYN Cookies allow the server to respond without keeping state; the server encodes the state info in the initial sequence number (cookie) and validates it only when the ACK returns.",
    "difficulty": "Advanced"
  }
]