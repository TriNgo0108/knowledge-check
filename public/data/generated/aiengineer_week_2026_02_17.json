[
  {
    "id": 1,
    "question": "What is the primary purpose of MLOps in a machine learning lifecycle?",
    "options": [
      "To replace data scientists with automated algorithms",
      "To streamline the deployment, monitoring, and maintenance of models in production",
      "To focus exclusively on the mathematical proofs of model convergence",
      "To eliminate the need for training data"
    ],
    "answer": "To streamline the deployment, monitoring, and maintenance of models in production",
    "explanation": "MLOps applies DevOps principles to machine learning, aiming to automate and reliable the process of taking models from development to production and maintaining them.",
    "difficulty": "Beginner"
  },
  {
    "id": 2,
    "question": "What occurs during 'training-serving skew'?",
    "options": [
      "The model performs better in production than during training due to hardware upgrades",
      "The model learns to ignore the features used during serving",
      "The data distribution or feature engineering in production differs from the training environment",
      "The training dataset is accidentally deleted after deployment"
    ],
    "answer": "The data distribution or feature engineering in production differs from the training environment",
    "explanation": "Training-serving skew happens when the input data or feature processing logic used to generate predictions in production diverges from what the model was trained on, leading to performance degradation.",
    "difficulty": "Beginner"
  },
  {
    "id": 3,
    "question": "In MLOps maturity models, what distinguishes Level 1 (Continuous Training) from Level 0 (Manual)?",
    "options": [
      "Level 1 removes the need for human intervention by deploying the entire training pipeline",
      "Level 1 relies entirely on manual scripts for model retraining",
      "Level 0 uses a fully automated CI/CD pipeline for model deployment",
      "Level 1 requires the data scientist to manually trigger model retraining for each release"
    ],
    "answer": "Level 1 removes the need for human intervention by deploying the entire training pipeline",
    "explanation": "While Level 0 focuses on manually deploying a trained model as a prediction service, Level 1 automates the process by deploying the training pipeline itself to automatically retrain and deploy models.",
    "difficulty": "Beginner"
  },
  {
    "id": 4,
    "question": "Why is model monitoring critical after deployment?",
    "options": [
      "To check if the server hardware is functioning correctly",
      "To track data drift and ensure model performance remains within acceptable bounds",
      "To verify that the code was written in Python",
      "To reduce the cost of electricity in the data center"
    ],
    "answer": "To track data drift and ensure model performance remains within acceptable bounds",
    "explanation": "Real-world data changes over time (data drift), and monitoring is required to detect when a model's performance degrades so it can be retrained or updated.",
    "difficulty": "Beginner"
  },
  {
    "id": 5,
    "question": "What is the function of a Model Registry in MLOps?",
    "options": [
      "To act as a database for raw training logs",
      "To store, version, and manage trained model artifacts",
      "To automatically write code for data scientists",
      "To replace the need for a Git repository"
    ],
    "answer": "To store, version, and manage trained model artifacts",
    "explanation": "A Model Registry is a centralized repository that stores versioned trained models, making it easy to track which model artifacts are deployed and enabling reproducibility.",
    "difficulty": "Beginner"
  },
  {
    "id": 6,
    "question": "What is the primary benefit of using containerization (e.g., Docker) in model deployment?",
    "options": [
      "It guarantees that the model will have 100% accuracy",
      "It ensures the model runs consistently across different computing environments",
      "It automatically reduces the size of the training dataset",
      "It eliminates the need for a version control system"
    ],
    "answer": "It ensures the model runs consistently across different computing environments",
    "explanation": "Containers package the model, dependencies, and runtime environment together, ensuring that the software behaves identically regardless of the underlying infrastructure.",
    "difficulty": "Beginner"
  },
  {
    "id": 7,
    "question": "Which of the following best describes 'Batch Inference'?",
    "options": [
      "Generating predictions for a group of data points on a schedule rather than in real-time",
      "Training a model in small batches to reduce memory usage",
      "Processing a single user request instantly",
      "Updating model weights continuously during production"
    ],
    "answer": "Generating predictions for a group of data points on a schedule rather than in real-time",
    "explanation": "Batch inference involves accumulating data over a period and running inference jobs to generate predictions for all data points at once, often used when low latency is not critical.",
    "difficulty": "Beginner"
  },
  {
    "id": 8,
    "question": "What is the concept of 'Drift Detection' in the context of MLOps?",
    "options": [
      "Monitoring the physical location of servers in a data center",
      "Identifying changes in the statistical properties of input data or predictions over time",
      "Detecting unauthorized access to the model repository",
      "Measuring the speed of data ingestion pipelines"
    ],
    "answer": "Identifying changes in the statistical properties of input data or predictions over time",
    "explanation": "Drift detection alerts engineers when the distribution of production data shifts away from the training data, which usually signals that the model's accuracy may be dropping.",
    "difficulty": "Beginner"
  },
  {
    "id": 9,
    "question": "In Large Language Model (LLM) deployment, what does 'Quantization' refer to?",
    "options": [
      "Increasing the model size to improve reasoning capabilities",
      "Reducing the precision of model weights to decrease memory usage and increase speed",
      "Splitting the model into multiple distinct shards",
      "The process of gathering user feedback for retraining"
    ],
    "answer": "Reducing the precision of model weights to decrease memory usage and increase speed",
    "explanation": "Quantization converts model weights from higher precision (e.g., 32-bit float) to lower precision (e.g., 4-bit integer), significantly reducing memory footprint and accelerating inference with minimal accuracy loss.",
    "difficulty": "Beginner"
  },
  {
    "id": 10,
    "question": "What is a 'Feature Store' in a modern machine learning architecture?",
    "options": [
      "A database used exclusively for storing raw images",
      "A centralized repository for storing, serving, and managing features for models",
      "A hardware accelerator for neural network computations",
      "A library for visualizing neural network layers"
    ],
    "answer": "A centralized repository for storing, serving, and managing features for models",
    "explanation": "A Feature Store ensures consistency between training and serving by providing a unified source for transformed features, preventing training-serving skew.",
    "difficulty": "Beginner"
  },
  {
    "id": 11,
    "question": "What is the primary function of 'Continuous Integration (CI)' in an MLOps pipeline?",
    "options": [
      "To automatically deploy the model to production every hour",
      "To automatically test code and model validation whenever changes are made",
      "To manually approve all code changes before merging",
      "To archive old training datasets"
    ],
    "answer": "To automatically test code and model validation whenever changes are made",
    "explanation": "CI in MLOps automates the testing of code and data integrity checks (e.g., unit tests, data validation) whenever a developer commits changes, ensuring broken code does not proceed to deployment.",
    "difficulty": "Beginner"
  },
  {
    "id": 12,
    "question": "Which deployment strategy involves routing a small percentage of live traffic to a new model for testing?",
    "options": [
      "Blue/Green Deployment",
      "Shadow Deployment",
      "Canary Deployment",
      "Big Bang Deployment"
    ],
    "answer": "Canary Deployment",
    "explanation": "Canary deployment slowly rolls out a new model to a small subset of users to monitor for errors or performance issues before rolling it out to the entire infrastructure.",
    "difficulty": "Beginner"
  },
  {
    "id": 13,
    "question": "What is 'Shadow Deployment'?",
    "options": [
      "Deploying the model to a public server for anyone to access",
      "Running a new model alongside the old one on live traffic without exposing results to users",
      "Deleting the old model immediately after the new one is trained",
      "Training a model on a dark dataset without labels"
    ],
    "answer": "Running a new model alongside the old one on live traffic without exposing results to users",
    "explanation": "Shadow deployment allows engineers to compare the performance and behavior of a new candidate model against the production model using real data, without affecting user experience.",
    "difficulty": "Beginner"
  },
  {
    "id": 14,
    "question": "What is the role of a 'Data Version Control' system (e.g., DVC) in ML?",
    "options": [
      "To manage the source code versions only",
      "To version large datasets, model files, and metrics similar to how Git manages code",
      "To replace the need for SQL databases",
      "To automatically generate synthetic data"
    ],
    "answer": "To version large datasets, model files, and metrics similar to how Git manages code",
    "explanation": "Standard Git cannot handle large binary files efficiently; Data Version Control systems are designed to track changes in datasets and models to ensure experiment reproducibility.",
    "difficulty": "Beginner"
  },
  {
    "id": 15,
    "question": "What is the 'Mixture of Experts (MoE)' architecture in AI models?",
    "options": [
      "A model that uses a single neural network for all tasks",
      "An architecture that activates only a sparse subset of its parameters for each input",
      "A training technique that uses only the top 1% of data experts",
      "A method for compressing models into smaller files"
    ],
    "answer": "An architecture that activates only a sparse subset of its parameters for each input",
    "explanation": "MoE models contain multiple sub-networks (experts) and a gating network that routes inputs to the relevant experts, allowing for massive scale without activating all parameters for every inference.",
    "difficulty": "Beginner"
  },
  {
    "id": 16,
    "question": "What is the primary benefit of using a 'Vector Database' in Retrieval-Augmented Generation (RAG)?",
    "options": [
      "It stores user passwords securely",
      "It enables efficient semantic search and retrieval of unstructured data based on vector embeddings",
      "It acts as the primary training ground for LLMs",
      "It replaces the need for an API gateway"
    ],
    "answer": "It enables efficient semantic search and retrieval of unstructured data based on vector embeddings",
    "explanation": "Vector databases index and store embeddings, allowing systems to retrieve relevant documents or context based on semantic similarity to provide to the LLM.",
    "difficulty": "Beginner"
  },
  {
    "id": 17,
    "question": "In the context of LLMs, what is 'Fine-tuning'?",
    "options": [
      "Reducing the temperature of the server hardware",
      "The process of adjusting a pre-trained model's weights on a specific dataset to adapt its behavior",
      "Deleting the last layer of a neural network",
      "The initial training phase of a model from random weights"
    ],
    "answer": "The process of adjusting a pre-trained model's weights on a specific dataset to adapt its behavior",
    "explanation": "Fine-tuning takes a general-purpose foundation model and trains it further on a narrower, task-specific dataset to improve its performance in that domain.",
    "difficulty": "Beginner"
  },
  {
    "id": 18,
    "question": "What is 'Hallucination' in Large Language Models?",
    "options": [
      "The model correctly identifying a complex object in an image",
      "The model generating confident but factually incorrect or nonsensical information",
      "The hardware overheating due to excessive computation",
      "The model failing to load into memory"
    ],
    "answer": "The model generating confident but factually incorrect or nonsensical information",
    "explanation": "Hallucination occurs when an LLM generates plausible-sounding text that is not grounded in reality or the provided context, a key challenge in production AI.",
    "difficulty": "Beginner"
  },
  {
    "id": 19,
    "question": "What is the primary goal of 'Prompt Engineering'?",
    "options": [
      "To modify the underlying source code of the model",
      "To design input prompts that effectively guide the model to produce the desired output",
      "To increase the RAM requirements of the inference server",
      "To bypass the need for model training entirely"
    ],
    "answer": "To design input prompts that effectively guide the model to produce the desired output",
    "explanation": "Prompt engineering is the practice of refining the input text (context, instructions, examples) given to an LLM to maximize the accuracy and relevance of its response without changing the model weights.",
    "difficulty": "Beginner"
  },
  {
    "id": 20,
    "question": "What is the 'Token' in the context of Large Language Models?",
    "options": [
      "A physical coin used to pay for GPU usage",
      "The smallest unit of text or data that a model processes (e.g., a word or part of a word)",
      "The encryption key used to secure the model",
      "A digital certificate for authenticating users"
    ],
    "answer": "The smallest unit of text or data that a model processes (e.g., a word or part of a word)",
    "explanation": "LLMs process text by breaking it down into tokens, which can be whole words, sub-words, or characters, representing the fundamental input unit for the model.",
    "difficulty": "Beginner"
  },
  {
    "id": 21,
    "question": "Why is 'Foundation Model' a significant term in AI Engineering?",
    "options": [
      "It refers to the physical concrete base of a data center",
      "It describes a large-scale model trained on broad data that can be adapted to many different tasks",
      "It is a specific type of database schema",
      "It refers to the first version of a software release"
    ],
    "answer": "It describes a large-scale model trained on broad data that can be adapted to many different tasks",
    "explanation": "Foundation models (like GPT or BERT) are pre-trained on vast corpora and serve as a general-purpose base that can be fine-tuned or prompted for a wide variety of downstream applications.",
    "difficulty": "Beginner"
  },
  {
    "id": 22,
    "question": "What does 'Human-in-the-Loop' (HITL) refer to in active learning or data pipelines?",
    "options": [
      "A person physically holding the network cable",
      "Involving human experts to label data, correct model errors, or provide feedback during training or inference",
      "A backup system where a person replaces the GPU if it fails",
      "The process of a human reviewing code before it is written"
    ],
    "answer": "Involving human experts to label data, correct model errors, or provide feedback during training or inference",
    "explanation": "HITL recognizes that models are imperfect; incorporating human feedback (e.g., for labeling or reinforcement learning) improves accuracy and safety.",
    "difficulty": "Beginner"
  },
  {
    "id": 23,
    "question": "What is the main purpose of the 'Attention Mechanism' in Transformer models?",
    "options": [
      "To reduce the file size of the model",
      "To allow the model to focus on specific parts of the input sequence when producing an output",
      "To speed up the internet connection",
      "To divide the model into smaller unrelated pieces"
    ],
    "answer": "To allow the model to focus on specific parts of the input sequence when producing an output",
    "explanation": "The attention mechanism weighs the importance of different input tokens relative to each other, allowing the model to capture long-range dependencies and context in text.",
    "difficulty": "Beginner"
  },
  {
    "id": 24,
    "question": "What is 'One-shot Inference'?",
    "options": [
      "Training a model on a single example",
      "A deployment strategy where the model crashes after one request",
      "Providing a single example in the prompt to demonstrate the desired task to the model",
      "A hardware configuration with a single CPU"
    ],
    "answer": "Providing a single example in the prompt to demonstrate the desired task to the model",
    "explanation": "In prompt engineering, one-shot (and few-shot) learning involves giving the LLM one (or a few) examples of input-output pairs within the prompt to improve performance on the task.",
    "difficulty": "Beginner"
  },
  {
    "id": 25,
    "question": "What is 'Latency' in the context of AI inference?",
    "options": [
      "The total amount of data a model can process in a day",
      "The time delay between sending a request to the model and receiving the response",
      "The speed at which the model downloads updates",
      "The reliability of the network connection"
    ],
    "answer": "The time delay between sending a request to the model and receiving the response",
    "explanation": "Low latency is critical for real-time applications (like chatbots), measuring how long the user has to wait for the model to generate a prediction.",
    "difficulty": "Beginner"
  },
  {
    "id": 26,
    "question": "What is 'Throughput' regarding model serving?",
    "options": [
      "The number of inference requests a system can handle in a given time period",
      "The physical thickness of the server rack",
      "The amount of electricity consumed by the GPU",
      "The error rate of the model predictions"
    ],
    "answer": "The number of inference requests a system can handle in a given time period",
    "explanation": "Throughput measures the capacity of the inference infrastructure, indicating how many predictions (or tokens) can be generated per second, which is vital for cost and scalability.",
    "difficulty": "Beginner"
  },
  {
    "id": 27,
    "question": "Which component is responsible for managing the lifecycle of a containerized application (e.g., Kubernetes)?",
    "options": [
      "Container Registry",
      "Orchestration Platform",
      "Model Registry",
      "Feature Store"
    ],
    "answer": "Orchestration Platform",
    "explanation": "Orchestration platforms like Kubernetes automate the deployment, scaling, and management of containerized applications, ensuring high availability for ML services.",
    "difficulty": "Beginner"
  },
  {
    "id": 28,
    "question": "What is 'Overfitting' in machine learning?",
    "options": [
      "When a model is too simple to capture the underlying pattern of the data",
      "When a model performs well on training data but poorly on unseen test data",
      "When a model runs too fast on the hardware",
      "When the model weights are initialized to zero"
    ],
    "answer": "When a model performs well on training data but poorly on unseen test data",
    "explanation": "Overfitting occurs when a model learns the noise and specific details of the training set to the extent that it negatively impacts its ability to generalize to new data.",
    "difficulty": "Beginner"
  },
  {
    "id": 29,
    "question": "What is the function of 'Hyperparameters' in a model?",
    "options": [
      "The internal weights learned by the model during training",
      "Configuration settings set before training that control the learning process",
      "The input data fed into the model",
      "The output predictions generated by the model"
    ],
    "answer": "Configuration settings set before training that control the learning process",
    "explanation": "Hyperparameters (e.g., learning rate, batch size) are external configuration variables whose values cannot be estimated from data and must be set prior to the training process.",
    "difficulty": "Beginner"
  },
  {
    "id": 30,
    "question": "What is 'RAG' (Retrieval-Augmented Generation)?",
    "options": [
      "A technique for cleaning noisy text data",
      "A method of augmenting LLM prompts with relevant external data retrieved from a database",
      "A type of neural network architecture only for images",
      "A protocol for encrypting model weights"
    ],
    "answer": "A method of augmenting LLM prompts with relevant external data retrieved from a database",
    "explanation": "RAG combines the generative capabilities of LLMs with external knowledge retrieval, allowing the model to answer questions based on specific, up-to-date, or private data.",
    "difficulty": "Beginner"
  },
  {
    "id": 31,
    "question": "In the context of inference optimization, what is 'Distillation'?",
    "options": [
      "Compressing data into a smaller file format",
      "Training a smaller 'student' model to mimic the behavior of a larger 'teacher' model",
      "Removing water from the liquid cooling system",
      "Separating the model into different languages"
    ],
    "answer": "Training a smaller 'student' model to mimic the behavior of a larger 'teacher' model",
    "explanation": "Knowledge distillation transfers the knowledge from a large, complex model to a smaller, faster one, enabling efficient deployment on resource-constrained hardware.",
    "difficulty": "Beginner"
  },
  {
    "id": 32,
    "question": "What is 'A/B Testing' in the context of deployed models?",
    "options": [
      "Testing the model with two different programming languages",
      "Comparing two versions of a model to see which one performs better on live traffic",
      "Testing the model on two different operating systems",
      "Alternating between training and serving phases"
    ],
    "answer": "Comparing two versions of a model to see which one performs better on live traffic",
    "explanation": "A/B testing serves different versions of a model to different user segments simultaneously to statistically compare their performance and business metrics.",
    "difficulty": "Beginner"
  },
  {
    "id": 33,
    "question": "Why are GPUs preferred over CPUs for training and inference of Deep Learning models?",
    "options": [
      "GPUs have faster clock speeds for single-threaded tasks",
      "GPUs have thousands of cores optimized for the parallel matrix operations required by neural networks",
      "CPUs cannot perform floating-point arithmetic",
      "GPUs are cheaper to manufacture"
    ],
    "answer": "GPUs have thousands of cores optimized for the parallel matrix operations required by neural networks",
    "explanation": "Deep learning involves massive matrix multiplications; GPUs excel here due to their highly parallel architecture, whereas CPUs are designed for sequential processing.",
    "difficulty": "Beginner"
  },
  {
    "id": 34,
    "question": "What is the 'Context Window' of an LLM?",
    "options": [
      "The time delay before the model starts generating text",
      "The maximum length of the input sequence (tokens) the model can process at one time",
      "The physical window of the server room",
      "The specific region in memory where the operating system runs"
    ],
    "answer": "The maximum length of the input sequence (tokens) the model can process at one time",
    "explanation": "The context window defines the limit of text (including the prompt and the generated response) that the model can 'remember' or attend to in a single session.",
    "difficulty": "Beginner"
  },
  {
    "id": 35,
    "question": "What is the primary function of 'vLLM' in model serving?",
    "options": [
      "To provide a visual user interface for data labeling",
      "To manage the storage of training datasets",
      "To optimize inference throughput using techniques like PagedAttention",
      "To act as a replacement for Python code"
    ],
    "answer": "To optimize inference throughput using techniques like PagedAttention",
    "explanation": "vLLM is a high-throughput and memory-efficient inference engine that uses PagedAttention to manage KV cache, significantly speeding up LLM serving compared to traditional engines.",
    "difficulty": "Beginner"
  },
  {
    "id": 36,
    "question": "In the context of MLOps maturity models, what primarily distinguishes 'Continuous Training' (Level 1) from 'Manual Pipeline' (Level 0)?",
    "options": [
      "The use of deep learning algorithms instead of statistical models",
      "The automation of the model retraining process using new data",
      "The deployment of the model to a cloud-based serverless function",
      "The implementation of real-time streaming inference"
    ],
    "answer": "The automation of the model retraining process using new data",
    "explanation": "Level 0 involves manual steps for retraining and deployment. Level 1 introduces a robust, automated CI/CD pipeline where the model is automatically retrained and validated when new data arrives.",
    "difficulty": "Intermediate"
  },
  {
    "id": 37,
    "question": "What is 'training-serving skew', and why is it a critical concern in MLOps?",
    "options": [
      "The difference in latency between the training environment and the production serving environment",
      "The divergence between the statistical properties of data used during training and data seen in production",
      "The inconsistency in code versions between the data scientist's laptop and the production server",
      "The delay caused by the model serving layer waiting for the training layer to finish"
    ],
    "answer": "The divergence between the statistical properties of data used during training and data seen in production",
    "explanation": "Training-serving skew occurs when the feature distribution or data format in production differs from the training data. This leads to performance degradation because the model encounters patterns it was not trained on.",
    "difficulty": "Intermediate"
  },
  {
    "id": 38,
    "question": "When deploying Large Language Models (LLMs), what is the primary benefit of using quantization techniques such as INT8 conversion?",
    "options": [
      "It increases the logical reasoning capabilities of the model",
      "It reduces the memory footprint and increases inference speed with minimal accuracy loss",
      "It automatically compresses the input prompt to fit within the context window",
      "It eliminates the need for a GPU by allowing the model to run on a standard CPU"
    ],
    "answer": "It reduces the memory footprint and increases inference speed with minimal accuracy loss",
    "explanation": "Quantization reduces the precision of the model's weights (e.g., from FP32 to INT8). This shrinks the model size, allowing more of it to fit in faster memory (VRAM) and enabling faster mathematical operations.",
    "difficulty": "Intermediate"
  },
  {
    "id": 39,
    "question": "What architectural innovation utilized by the vLLM inference engine significantly improves throughput for LLMs by managing KV Cache memory?",
    "options": [
      "PagedAttention",
      "FlashAttention",
      "Multi-Query Attention",
      "Cross-Attention"
    ],
    "answer": "PagedAttention",
    "explanation": "PagedAttention, inspired by operating system virtual memory, partitions the KV cache into non-contiguous blocks. This eliminates memory fragmentation and allows efficient sharing of KV cache between different requests.",
    "difficulty": "Intermediate"
  },
  {
    "id": 40,
    "question": "In the context of Retrieval-Augmented Generation (RAG), what is the purpose of a 'Vector Database'?",
    "options": [
      "To store the raw text files for the LLM to read sequentially",
      "To store and retrieve high-dimensional embeddings based on semantic similarity",
      "To manage the version control of the model weights",
      "To log user interactions for later analysis"
    ],
    "answer": "To store and retrieve high-dimensional embeddings based on semantic similarity",
    "explanation": "Vector databases index embeddings (vector representations of text). During RAG, they allow the system to quickly query and retrieve the most relevant documents (chunks) based on vector proximity to the user's query.",
    "difficulty": "Intermediate"
  },
  {
    "id": 41,
    "question": "What is 'Speculative Decoding' in the context of LLM inference optimization?",
    "options": [
      "Using a smaller model to draft tokens, which are then verified in parallel by a larger model",
      "Predicting the next user prompt to preload the relevant data into memory",
      "Distributing a single inference request across multiple GPUs simultaneously",
      "Compressing the model weights after the inference is complete"
    ],
    "answer": "Using a smaller model to draft tokens, which are then verified in parallel by a larger model",
    "explanation": "Speculative decoding uses a small, fast 'draft' model to generate candidate tokens. The large 'target' model then verifies these candidates in a single batch pass, significantly speeding up generation.",
    "difficulty": "Intermediate"
  },
  {
    "id": 42,
    "question": "Which component of an MLOps pipeline is specifically responsible for ensuring that input features adhere to the expected schema and statistical range before prediction?",
    "options": [
      "Model Registry",
      "Continuous Integration (CI) Runner",
      "Data Validation / Feature Validation Service",
      "Experiment Tracking Server"
    ],
    "answer": "Data Validation / Feature Validation Service",
    "explanation": "While training uses validation, the production serving layer requires a service to validate incoming request data (schema, type, range) to prevent crashes or nonsensical predictions caused by bad input.",
    "difficulty": "Intermediate"
  },
  {
    "id": 43,
    "question": "In Transformer models, what specific mechanism allows the model to process different parts of a sequence in parallel rather than sequentially?",
    "options": [
      "Recurrent connections",
      "Positional encoding",
      "Self-attention",
      "Masked Language Modeling (MLM)"
    ],
    "answer": "Self-attention",
    "explanation": "Self-attention computes relationships between all tokens in a sequence simultaneously. Unlike RNNs/LSTMs, which process sequentially, Transformers can compute attention for all positions in parallel, drastically improving training speed.",
    "difficulty": "Intermediate"
  },
  {
    "id": 44,
    "question": "When implementing a 'Mixture of Experts' (MoE) architecture, what is the primary advantage over a dense model of equivalent parameter count?",
    "options": [
      "The model requires less training data to converge",
      "Only a subset of parameters is activated per input token, reducing inference compute cost",
      "It eliminates the need for an attention mechanism",
      "The model is guaranteed to have zero hallucinations"
    ],
    "answer": "Only a subset of parameters is activated per input token, reducing inference compute cost",
    "explanation": "MoE models consist of many 'expert' sub-networks. A 'gating network' selects a small number of experts to process each token, allowing the total parameter count to be large while keeping actual compute (FLOPs) low.",
    "difficulty": "Intermediate"
  },
  {
    "id": 45,
    "question": "What is the function of the 'KV Cache' during the inference generation phase of a Transformer model?",
    "options": [
      "To store the model weights in a compressed format",
      "To cache the Key and Value vectors of previous tokens to avoid re-computation in subsequent steps",
      "To keep a history of all user prompts for privacy auditing",
      "To buffer the output tokens before they are sent to the client"
    ],
    "answer": "To cache the Key and Value vectors of previous tokens to avoid re-computation in subsequent steps",
    "explanation": "In autoregressive generation, tokens already processed cannot change. Caching their Key and Value matrices prevents the model from having to re-compute the attention context for the entire sequence at every new token generation step.",
    "difficulty": "Intermediate"
  },
  {
    "id": 46,
    "question": "What is 'Fine-Tuning' in the context of Foundation Models, and how does it differ from 'Prompt Engineering'?",
    "options": [
      "Fine-tuning updates the model weights on a specific dataset, while prompt engineering relies only on input manipulation",
      "Fine-tuning is done at inference time, while prompt engineering is done at training time",
      "Fine-tuning requires no data, while prompt engineering requires thousands of examples",
      "Fine-tuning is only possible for image models, while prompt engineering is for text models"
    ],
    "answer": "Fine-tuning updates the model weights on a specific dataset, while prompt engineering relies only on input manipulation",
    "explanation": "Fine-tuning involves backpropagation and weight updates to specialize a model. Prompt engineering crafts the input context to guide the pre-existing weights without altering the model parameters.",
    "difficulty": "Intermediate"
  },
  {
    "id": 47,
    "question": "In MLOps, what is the primary purpose of a 'Model Registry'?",
    "options": [
      "To store the raw training data for compliance purposes",
      "To act as a central repository for versioning, storing, and managing trained model artifacts",
      "To automatically generate documentation for the source code",
      "To monitor the CPU usage of the inference servers"
    ],
    "answer": "To act as a central repository for versioning, storing, and managing trained model artifacts",
    "explanation": "A Model Registry tracks the lineage of models (which code/data created them). It ensures reproducibility and provides a controlled interface for deploying specific model versions to production.",
    "difficulty": "Intermediate"
  },
  {
    "id": 48,
    "question": "What does 'Parameter-Efficient Fine-Tuning' (PEFT), such as LoRA or Adapters, aim to achieve?",
    "options": [
      "Training a model from scratch using fewer parameters",
      "Modifying only a small subset of weights or adding small layers to adapt a large model efficiently",
      "Reducing the size of the training dataset required",
      "Compressing a model post-training for faster mobile inference"
    ],
    "answer": "Modifying only a small subset of weights or adding small layers to adapt a large model efficiently",
    "explanation": "PEFT methods freeze most of the pre-trained model's parameters and train a small number of additional parameters (or low-rank matrices). This drastically reduces memory and compute requirements for adaptation.",
    "difficulty": "Intermediate"
  },
  {
    "id": 49,
    "question": "What is the role of 'RLHF' (Reinforcement Learning from Human Feedback) in LLM development?",
    "options": [
      "To pre-train the model on a large corpus of text",
      "To align the model's outputs with human preferences and safety guidelines",
      "To compress the model size for deployment on edge devices",
      "To retrieve relevant documents from a database"
    ],
    "answer": "To align the model's outputs with human preferences and safety guidelines",
    "explanation": "RLHF involves training a 'reward model' on human rankings of model outputs and then optimizing the LLM to maximize this reward. It helps reduce toxicity, hallucinations, and improve helpfulness.",
    "difficulty": "Intermediate"
  },
  {
    "id": 50,
    "question": "In a RAG (Retrieval-Augmented Generation) pipeline, what is the risk of 'Chunking' text too aggressively into small pieces?",
    "options": [
      "The vector database will run out of memory",
      "Loss of semantic context leading to vague or irrelevant retrieval",
      "The LLM will refuse to answer the question",
      "The embedding model will fail to generate vectors"
    ],
    "answer": "Loss of semantic context leading to vague or irrelevant retrieval",
    "explanation": "If chunks are too small, they may lack sufficient surrounding context to match the meaning of a query effectively. For example, a chunk mentioning only 'he' or 'it' without the antecedent makes retrieval difficult.",
    "difficulty": "Intermediate"
  },
  {
    "id": 51,
    "question": "Which deployment strategy involves sending a small percentage of production traffic to a new model to validate its performance before a full rollout?",
    "options": [
      "Blue/Green Deployment",
      "Canary Deployment",
      "Shadow Deployment",
      "A/B Testing"
    ],
    "answer": "Canary Deployment",
    "explanation": "Canary deployment exposes the new model version to a small subset of users (the 'canaries') to detect errors or performance regressions in a real environment before routing all traffic to it.",
    "difficulty": "Intermediate"
  },
  {
    "id": 52,
    "question": "What is the distinction between 'Shadow Deployment' and standard production deployment?",
    "options": [
      "Shadow deployment serves traffic to the new model while the old model is deleted",
      "Shadow deployment runs the new model alongside the old one, but users only see responses from the old model",
      "Shadow deployment requires a dedicated database for the new model",
      "Shadow deployment is used exclusively for unit testing"
    ],
    "answer": "Shadow deployment runs the new model alongside the old one, but users only see responses from the old model",
    "explanation": "In shadow mode, the new model processes real requests and generates outputs (for logging/metrics), but these outputs are discarded. The user receives the response from the existing model, allowing risk-free validation.",
    "difficulty": "Intermediate"
  },
  {
    "id": 53,
    "question": "What is the 'Temperature' parameter in LLM generation controlling?",
    "options": [
      "The speed of the inference process",
      "The randomness or creativity of the token selection probability distribution",
      "The maximum length of the generated text",
      "The memory usage of the GPU"
    ],
    "answer": "The randomness or creativity of the token selection probability distribution",
    "explanation": "Temperature scales the logits before softmax. A higher temperature flattens the distribution, making low-probability tokens more likely (more random/creative). Lower temperature sharpens the distribution, focusing on high-probability tokens.",
    "difficulty": "Intermediate"
  },
  {
    "id": 54,
    "question": "What is 'Top-k Sampling' during text generation?",
    "options": [
      "Selecting the most probable token only",
      "Limiting the next token selection to the k most probable tokens and rescaling probabilities",
      "Sampling from the bottom k tokens to increase diversity",
      "Stopping generation after k tokens"
    ],
    "answer": "Limiting the next token selection to the k most probable tokens and rescaling probabilities",
    "explanation": "Top-k sampling filters the vocabulary to keep only the top k most likely tokens. The model then samples from this restricted subset, preventing the generation of nonsense (long-tail) tokens while maintaining variety.",
    "difficulty": "Intermediate"
  },
  {
    "id": 55,
    "question": "Which metric is commonly used to evaluate the performance of a RAG system by measuring the relevance of the retrieved documents?",
    "options": [
      "BLEU Score",
      "Perplexity",
      "Hit Rate / NDCG",
      "Accuracy"
    ],
    "answer": "Hit Rate / NDCG",
    "explanation": "Hit Rate measures if the correct document is present in the top-k retrieved results. NDCG (Normalized Discounted Cumulative Gain) measures the ranking quality of those retrieved documents. BLEU and Perplexity are for generation quality.",
    "difficulty": "Intermediate"
  },
  {
    "id": 56,
    "question": "What is the primary function of an 'Orchestrator' in a production LLM pipeline (e.g., LangChain, LangGraph)?",
    "options": [
      "To train the neural network weights",
      "To manage the flow of data between prompts, models, retrieval tools, and memory",
      "To store the vector embeddings",
      "To provide the hardware infrastructure"
    ],
    "answer": "To manage the flow of data between prompts, models, retrieval tools, and memory",
    "explanation": "Orchestrators define the logic of an LLM application. They handle routing (which agent/tool to use), memory management (state), and connecting the LLM to external data sources.",
    "difficulty": "Intermediate"
  },
  {
    "id": 57,
    "question": "Why is 'FlashAttention' significant for optimizing LLM training and inference?",
    "options": [
      "It compresses the model weights to 4-bit integers",
      "It makes the attention mechanism IO-aware by tiling the attention matrix to fit in SRAM",
      "It removes the need for positional encodings",
      "It parallelizes the data loading process"
    ],
    "answer": "It makes the attention mechanism IO-aware by tiling the attention matrix to fit in SRAM",
    "explanation": "Standard attention is memory-bandwidth bound. FlashAttention algorithmically reorders the attention computation to keep data in fast GPU SRAM (on-chip memory) rather than HBM (high bandwidth memory), drastically speeding up the process.",
    "difficulty": "Intermediate"
  },
  {
    "id": 58,
    "question": "What is 'Data Drift' in the context of model monitoring?",
    "options": [
      "The model's predictions gradually becoming more random",
      "A change in the statistical distribution of the input data (features) over time",
      "The accidental deletion of training data",
      "The latency of the API increasing over time"
    ],
    "answer": "A change in the statistical distribution of the input data (features) over time",
    "explanation": "Data drift occurs when the real-world data the model encounters in production changes (e.g., seasonality, user behavior changes) compared to the data the model was trained on, often leading to performance degradation.",
    "difficulty": "Intermediate"
  },
  {
    "id": 59,
    "question": "In a MLOps pipeline, what is 'Feature Store' used for?",
    "options": [
      "Storing Python feature engineering code",
      "Serving computed features consistently for both training and inference",
      "Storing raw data logs",
      "Visualizing model metrics"
    ],
    "answer": "Serving computed features consistently for both training and inference",
    "explanation": "Feature stores provide a centralized way to store, manage, and serve feature values. This ensures point-in-time correctness and prevents 'training-serving skew' by ensuring the model uses the exact same features in both phases.",
    "difficulty": "Intermediate"
  },
  {
    "id": 60,
    "question": "What is the primary challenge when performing 'In-context Learning' with very long context windows?",
    "options": [
      "The model cannot handle more than 50 tokens",
      "The 'Lost in the Middle' phenomenon where the model struggles to retrieve information located in the middle of the context",
      "The model automatically switches to a different language",
      "The inference speed becomes faster than real-time"
    ],
    "answer": "The 'Lost in the Middle' phenomenon where the model struggles to retrieve information located in the middle of the context",
    "explanation": "Studies show that LLMs perform best at retrieving information when it is at the beginning or end of the context. As context windows grow, models often fail to utilize information buried in the middle.",
    "difficulty": "Intermediate"
  },
  {
    "id": 61,
    "question": "What is the role of a 'Stopping Criteria' or 'Stop Token' in an LLM inference loop?",
    "options": [
      "To prevent the model from hallucinating",
      "To signal to the generation loop that the output is complete",
      "To filter out toxic words before they are generated",
      "To compress the output before transmission"
    ],
    "answer": "To signal to the generation loop that the output is complete",
    "explanation": "LLMs are autoregressive; they generate tokens one by one. Without a stopping criteria (like an EOS token or a character limit), the model would continue generating text indefinitely or until a hard time limit.",
    "difficulty": "Intermediate"
  },
  {
    "id": 62,
    "question": "How does 'Dynamic Batching' improve throughput in an inference server?",
    "options": [
      "By grouping incoming requests into batches and waiting for a fixed time window to maximize batch size",
      "By processing one request at a time to ensure low latency",
      "By changing the model architecture based on the input size",
      "By compressing the input prompts before processing"
    ],
    "answer": "By grouping incoming requests into batches and waiting for a fixed time window to maximize batch size",
    "explanation": "Dynamic batching holds a batch open for a short duration (e.g., 10ms) to collect multiple incoming requests. This allows the GPU to process a larger batch size, improving hardware utilization and overall throughput.",
    "difficulty": "Intermediate"
  },
  {
    "id": 63,
    "question": "What is the purpose of 'Gradient Accumulation' during model training when GPU memory is limited?",
    "options": [
      "To reduce the size of the dataset",
      "To simulate a larger batch size by averaging gradients over multiple training steps",
      "To increase the learning rate",
      "To compress the model weights"
    ],
    "answer": "To simulate a larger batch size by averaging gradients over multiple training steps",
    "explanation": "If a GPU cannot fit a large batch in memory, gradient accumulation runs smaller mini-batches, accumulating the gradients without updating weights, and only updates weights after the equivalent of the large batch is processed.",
    "difficulty": "Intermediate"
  },
  {
    "id": 64,
    "question": "In the context of System Design for AI, what is 'Semantic Caching'?",
    "options": [
      "Caching the exact raw text of user queries",
      "Caching the results of semantically similar queries to avoid redundant inference",
      "Storing the model weights in a semantic layer",
      "Compressing the database indexes"
    ],
    "answer": "Caching the results of semantically similar queries to avoid redundant inference",
    "explanation": "Instead of exact string matching, semantic caching checks the vector similarity of a new prompt against a cache of previous prompts/answers. If a similar prompt is found, the cached answer is returned, saving compute cost.",
    "difficulty": "Intermediate"
  },
  {
    "id": 65,
    "question": "What is the primary difference between 'Instruction Tuning' and 'Continued Pre-training'?",
    "options": [
      "Instruction tuning trains on raw text corpora; continued pre-training trains on formatted examples",
      "Instruction tuning trains on formatted prompt-response pairs to improve task-following; continued pre-training trains on domain-specific raw text",
      "Instruction tuning is for images; continued pre-training is for text",
      "There is no difference; they are synonyms"
    ],
    "answer": "Instruction tuning trains on formatted prompt-response pairs to improve task-following; continued pre-training trains on domain-specific raw text",
    "explanation": "Continued pre-training (or domain adaptation) exposes the model to new knowledge (raw text). Instruction tuning teaches the model *how to behave* (follow instructions) using NLP tasks formatted as conversations.",
    "difficulty": "Intermediate"
  },
  {
    "id": 66,
    "question": "Why is the 'F1 Score' often preferred over 'Accuracy' for evaluating classification models in imbalanced datasets?",
    "options": [
      "F1 Score is easier to calculate",
      "Accuracy can be misleadingly high if the model predicts the majority class for all samples",
      "F1 Score measures only false positives",
      "Accuracy requires more data to compute"
    ],
    "answer": "Accuracy can be misleadingly high if the model predicts the majority class for all samples",
    "explanation": "In skewed datasets (e.g., 99% negative, 1% positive), a dumb model predicting 'negative' always gets 99% accuracy. F1 Score considers both Precision and Recall, giving a true picture of performance on the minority class.",
    "difficulty": "Intermediate"
  },
  {
    "id": 67,
    "question": "What is the role of 'Token Probability' (Logprobs) in debugging LLM applications?",
    "options": [
      "To count the number of words in the input",
      "To assess the model's confidence in its generation, identifying potential hallucinations",
      "To calculate the cost of the API call",
      "To encrypt the output data"
    ],
    "answer": "To assess the model's confidence in its generation, identifying potential hallucinations",
    "explanation": "High logprobs indicate the model was confident in its token selection. Low logprobs suggest the model was 'unsure' or guessing, which is often a signal for hallucination or poor performance.",
    "difficulty": "Intermediate"
  },
  {
    "id": 68,
    "question": "What is 'Collaborative Filtering' in the context of Recommender Systems?",
    "options": [
      "Filtering out users who have not collaborated",
      "Making automatic predictions about the interests of a user by collecting preferences from many users",
      "Using Deep Learning to filter content",
      "Manually curating lists of items"
    ],
    "answer": "Making automatic predictions about the interests of a user by collecting preferences from many users",
    "explanation": "Collaborative filtering assumes that if a user A has the same opinion as a user B on an issue, A is more likely to have B's opinion on a different issue. It relies on user-item interaction matrices.",
    "difficulty": "Intermediate"
  },
  {
    "id": 69,
    "question": "In A/B testing for ML models, what is 'Statistical Power'?",
    "options": [
      "The probability of correctly rejecting a false null hypothesis (detecting a true difference)",
      "The probability that the model makes a correct prediction",
      "The computational speed of the model",
      "The percentage of users in the test group"
    ],
    "answer": "The probability of correctly rejecting a false null hypothesis (detecting a true difference)",
    "explanation": "Statistical power (1 - Beta) measures the sensitivity of an A/B test. High power means the test is likely to detect an improvement if one actually exists, avoiding false negatives (failed experiments).",
    "difficulty": "Intermediate"
  },
  {
    "id": 70,
    "question": "What is the 'Curse of Dimensionality' in the context of high-dimensional data and vector search?",
    "options": [
      "The data becomes too large to store on a disk",
      "Distance metrics lose their meaning and data becomes sparse, making retrieval difficult",
      "The model requires more training time",
      "The dimensions of the vector increase linearly with data size"
    ],
    "answer": "Distance metrics lose their meaning and data becomes sparse, making retrieval difficult",
    "explanation": "In high-dimensional spaces, all points tend to become equidistant from each other. This makes similarity search less effective because 'nearest neighbor' becomes statistically indistinguishable from any other neighbor.",
    "difficulty": "Intermediate"
  },
  {
    "id": 71,
    "question": "In the context of Large Language Model (LLM) serving, what is the primary function of PagedAttention as implemented in vLLM?",
    "options": [
      "Compressing model weights to 8-bit integers to reduce VRAM usage",
      "Managing KV cache fragmentation by treating cache blocks as virtual memory pages",
      "Distributing model layers across multiple GPUs using tensor parallelism",
      "Predicting the next N tokens using a smaller draft model to reduce latency"
    ],
    "answer": "Managing KV cache fragmentation by treating cache blocks as virtual memory pages",
    "explanation": "PagedAttention, inspired by operating system virtual memory, divides the KV cache into fixed-size blocks to mitigate memory fragmentation and allow efficient non-contiguous memory allocation.",
    "difficulty": "Advanced"
  },
  {
    "id": 72,
    "question": "What distinguishes Direct Preference Optimization (DPO) from Reinforcement Learning from Human Feedback (RLHF) using PPO?",
    "options": [
      "DPO requires a separate reward model to calculate gradients for the policy",
      "DPO implicitly solves the RL problem without an explicit value function or reward model",
      "DPO relies on a KL-constrained objective to prevent mode collapse",
      "DPO only works with generative adversarial networks (GANs) rather than transformers"
    ],
    "answer": "DPO implicitly solves the RL problem without an explicit value function or reward model",
    "explanation": "DPO formulates a stable optimization objective that solves the RLHF problem purely with classification on preference data, bypassing the need to fit a separate reward model or perform online sampling.",
    "difficulty": "Advanced"
  },
  {
    "id": 73,
    "question": "In Mixture of Experts (MoE) models, what is the purpose of the 'load balancing loss' during training?",
    "options": [
      "To prevent all experts from processing the same tokens simultaneously",
      "To distribute tokens evenly across experts and avoid expert collapse",
      "To minimize the communication overhead between GPU devices",
      "To ensure that the gating network outputs sum to exactly 1.0"
    ],
    "answer": "To distribute tokens evenly across experts and avoid expert collapse",
    "explanation": "Without a load balancing loss, the gating network might route all tokens to a small subset of experts, causing the others to receive few or no updates (expert collapse).",
    "difficulty": "Advanced"
  },
  {
    "id": 74,
    "question": "Which quantization technique minimizes the mean squared error (MSE) of the weights by second-order approximation and is commonly used for LLM post-training quantization?",
    "options": [
      "GPTQ (Generative Pre-trained Transformer Quantization)",
      "LoRA (Low-Rank Adaptation)",
      "AWQ (Activation-aware Weight Quantization)",
      "QLoRA (Quantized Low-Rank Adaptation)"
    ],
    "answer": "GPTQ (Generative Pre-trained Transformer Quantization)",
    "explanation": "GPTQ updates weights in an order that minimizes the squared error impact of previous weight quantizations, achieving high-quality 4-bit quantization.",
    "difficulty": "Advanced"
  },
  {
    "id": 75,
    "question": "What is the fundamental difference between Tensor Parallelism and Pipeline Parallelism in distributed model training?",
    "options": [
      "Tensor Parallelism splits the model across layers, while Pipeline Parallelism splits within individual weight matrices",
      "Tensor Parallelism splits individual weight matrices across devices, while Pipeline Parallelism distributes sequential layers",
      "Tensor Parallelism operates on the data dimension, while Pipeline Parallelism operates on the model dimension",
      "Tensor Parallelism requires synchronous SGD, while Pipeline Parallelism is purely asynchronous"
    ],
    "answer": "Tensor Parallelism splits individual weight matrices across devices, while Pipeline Parallelism distributes sequential layers",
    "explanation": "Tensor Parallelism involves breaking down matrix multiplications (like in the attention layer) horizontally or vertically, whereas Pipeline Parallelism assigns different layers of the network to different GPUs.",
    "difficulty": "Advanced"
  },
  {
    "id": 76,
    "question": "When deploying an LLM, what is 'Speculative Decoding' designed to optimize?",
    "options": [
      "Model accuracy by analyzing multiple potential future token sequences",
      "Inference latency by using a smaller draft model to predict tokens verified by the larger model",
      "Memory bandwidth by quantizing activations on the fly",
      "Training throughput by accumulating gradients over micro-batches"
    ],
    "answer": "Inference latency by using a smaller draft model to predict tokens verified by the larger model",
    "explanation": "Speculative decoding uses a small, fast model to generate a sequence of tokens, which the large model verifies in parallel, effectively increasing the generation speed without changing the output distribution.",
    "difficulty": "Advanced"
  },
  {
    "id": 77,
    "question": "In the context of Feature Stores, what is 'Point-in-Time Correctness' and why is it critical?",
    "options": [
      "It ensures that all features are computed in real-time regardless of data source latency",
      "It prevents data leakage by ensuring features used for training correspond to the historical state of data at that timestamp",
      "It guarantees that the feature store schema is immutable and version-controlled",
      "It aligns the timezone of data ingestion from global distributed sources"
    ],
    "answer": "It prevents data leakage by ensuring features used for training correspond to the historical state of data at that timestamp",
    "explanation": "Without point-in-time joins, a training dataset might inadvertently include 'future' information (data that became available after the prediction target time), leading to unrealistically good model performance that fails in production.",
    "difficulty": "Advanced"
  },
  {
    "id": 78,
    "question": "What is 'Training-Serving Skew' in the context of MLOps?",
    "options": [
      "The difference in latency between online inference and offline batch processing",
      "The discrepancy between the model's performance on the validation set and the test set",
      "The inconsistency in feature computation or data handling between the training environment and the production environment",
      "The divergence in resource consumption between training on a GPU cluster and serving on a CPU instance"
    ],
    "answer": "The inconsistency in feature computation or data handling between the training environment and the production environment",
    "explanation": "Skew occurs when the logic used to preprocess data or engineer features during training differs from the logic used during inference, causing model degradation.",
    "difficulty": "Advanced"
  },
  {
    "id": 79,
    "question": "Why is Grouped Query Attention (GQA) preferred over Multi-Query Attention (MQA) in modern architectures like Llama-3?",
    "options": [
      "MQA reduces model size too significantly, leading to a loss of model capacity",
      "GQA offers a better trade-off by splitting attention heads into groups sharing a single key-value head, balancing quality and speed",
      "MQA cannot be efficiently implemented on GPU hardware due to memory coalescing issues",
      "GQA allows for variable sequence lengths, whereas MQA requires fixed padding"
    ],
    "answer": "GQA offers a better trade-off by splitting attention heads into groups sharing a single key-value head, balancing quality and speed",
    "explanation": "While MQA (all heads share one KV pair) is fastest, it hurts quality. GQA (groups share KV pairs) significantly reduces decoder inference memory bandwidth compared to standard Multi-Head Attention while preserving more model expressiveness.",
    "difficulty": "Advanced"
  },
  {
    "id": 80,
    "question": "What is the primary mechanism of FlashAttention that improves GPU utilization during Transformer inference?",
    "options": [
      "Quantizing the attention weights to FP16 to reduce memory footprint",
      "Using tiling to compute attention block-by-block from High Bandwidth Memory (HBM) without materializing the full attention matrix",
      "Removing the softmax operation to reduce the computational complexity from O(N^2) to O(N)",
      "Pruning the least important attention heads to reduce the number of floating point operations"
    ],
    "answer": "Using tiling to compute attention block-by-block from High Bandwidth Memory (HBM) without materializing the full attention matrix",
    "explanation": "FlashAttention minimizes HBM accesses by performing the attention computation in SRAM (on-chip memory) via tiling, which is the bottleneck for long sequences.",
    "difficulty": "Advanced"
  },
  {
    "id": 81,
    "question": "What is 'Rotary Positional Embeddings' (RoPE) and how does it differ from learned absolute embeddings?",
    "options": [
      "RoPE adds a scalar value to the logits based on the token's absolute position index",
      "RoPE encodes position by rotating the key and query vectors in geometric space, providing relative position awareness",
      "RoPE utilizes a separate embedding matrix that is added to the token embeddings before the first layer",
      "RoPE compresses the sequence length by downsampling the hidden states based on position"
    ],
    "answer": "RoPE encodes position by rotating the key and query vectors in geometric space, providing relative position awareness",
    "explanation": "RoPE applies a rotation transformation to query and key vectors proportional to their position index m, allowing the model to naturally capture relative positional relationships (m - n).",
    "difficulty": "Advanced"
  },
  {
    "id": 82,
    "question": "Which technique allows fine-tuning of large models on a single GPU with significantly less VRAM by freezing main weights and injecting trainable rank decomposition matrices?",
    "options": [
      "Quantization-Aware Training (QAT)",
      "Low-Rank Adaptation (LoRA)",
      "Knowledge Distillation",
      "Gradient Checkpointing"
    ],
    "answer": "Low-Rank Adaptation (LoRA)",
    "explanation": "LoRA freezes the pre-trained weights and injects trainable rank decomposition matrices into each layer, drastically reducing the number of trainable parameters and VRAM requirements.",
    "difficulty": "Advanced"
  },
  {
    "id": 83,
    "question": "In the context of continuous model monitoring, what is 'Concept Drift'?",
    "options": [
      "A change in the statistical distribution of the input data (P(X)) over time",
      "A change in the relationship between input data and the target variable (P(Y|X)) over time",
      "A sudden failure in the underlying hardware infrastructure serving the model",
      "The introduction of new features in the training pipeline that the model has not seen before"
    ],
    "answer": "A change in the relationship between input data and the target variable (P(Y|X)) over time",
    "explanation": "Concept drift means the fundamental rules the model learned have changed; for example, a model predicting housing prices may become invalid after a market crash, even if the input features (sq ft, location) remain statistically similar.",
    "difficulty": "Advanced"
  },
  {
    "id": 84,
    "question": "What distinguishes 'SLERP' (Spherical Linear Interpolation) from standard linear interpolation when merging LoRA adapters?",
    "options": [
      "SLERP interpolates along the hyperbolic manifold to preserve semantic distance",
      "SLERP maintains the geometric structure of the high-dimensional latent space on the hypersphere",
      "SLERP performs a weighted average of the singular values of the weight matrices",
      "SLERP reduces the rank of the resulting merged model to prevent overfitting"
    ],
    "answer": "SLERP maintains the geometric structure of the high-dimensional latent space on the hypersphere",
    "explanation": "Unlike linear interpolation which cuts across the chord of the sphere, SLERP interpolates along the great arc (surface of the hypersphere), preserving the magnitude and semantic direction of the vectors being merged.",
    "difficulty": "Advanced"
  },
  {
    "id": 85,
    "question": "What is the primary advantage of using Bfloat16 (BF16) over Float16 (FP16) for LLM training?",
    "options": [
      "BF16 has a larger exponent range, reducing the risk of underflow/overflow during complex calculations",
      "BF16 uses half the memory bandwidth of FP16",
      "BF16 supports subnormal numbers for extremely small gradients",
      "BF16 increases the effective batch size by 2x without code changes"
    ],
    "answer": "BF16 has a larger exponent range, reducing the risk of underflow/overflow during complex calculations",
    "explanation": "BF16 retains the 8-bit exponent size of FP32 (truncating the mantissa), whereas FP16 reduces the exponent to 5 bits. This makes BF16 much more stable for the deep stacks found in large models.",
    "difficulty": "Advanced"
  },
  {
    "id": 86,
    "question": "In retrieval-augmented generation (RAG), what is the 'Reciprocal Rank Fusion' (RRF) algorithm used for?",
    "options": [
      "Re-ranking the top-k documents based on their cross-encoder scores",
      "Combining ranked results from multiple different retrieval sources into a single ranked list",
      "Fusing the query and document embeddings into a single vector for the decoder",
      "Calculating the harmonic mean of precision and recall for the retrieval system"
    ],
    "answer": "Combining ranked results from multiple different retrieval sources into a single ranked list",
    "explanation": "RRF is a robust method for data fusion that aggregates ranked lists from various retrievers (e.g., BM25, dense retrieval) by assigning a score of 1/(k+rank), avoiding the need to normalize disparate similarity scores.",
    "difficulty": "Advanced"
  },
  {
    "id": 87,
    "question": "What is the 'KV Cache' in Transformer architecture and how does it function?",
    "options": [
      "It stores the key and value vectors for all previous tokens in the sequence to avoid recomputation during autoregressive generation",
      "It caches the weight matrices of the attention layers to speed up the backward pass",
      "It stores the vocabulary mapping to accelerate tokenization",
      "It keeps track of the Key-Value pairs of the hyperparameters used for training"
    ],
    "answer": "It stores the key and value vectors for all previous tokens in the sequence to avoid recomputation during autoregressive generation",
    "explanation": "In autoregressive decoding, the KV for previous context does not change. By caching these vectors, the model only needs to compute the KV for the new token, significantly reducing latency.",
    "difficulty": "Advanced"
  },
  {
    "id": 88,
    "question": "Which technique is used to detect 'Hallucinations' in RAG pipelines by evaluating faithfulness?",
    "options": [
      "Increasing the temperature parameter to make the model more conservative",
      "Using a separate LLM to check if the generated response is supported by the retrieved context",
      "Calculating the Cosine Similarity between the prompt and the generated response",
      "Filtering out low-probability tokens during generation (Top-K sampling)"
    ],
    "answer": "Using a separate LLM to check if the generated response is supported by the retrieved context",
    "explanation": "Faithfulness evaluation often involves a 'critic' or 'judge' LLM that verifies if the assertions in the final answer can be attributed to the source documents provided in the context.",
    "difficulty": "Advanced"
  },
  {
    "id": 89,
    "question": "What is 'Gradient Checkpointing' in the context of training very deep neural networks?",
    "options": [
      "Saving the full activation history at every layer to speed up the backward pass",
      "Trading computation for memory by re-computing activations during the backward pass rather than storing them in VRAM",
      "Checkpointing the model weights to disk every N steps to prevent data loss",
      "Stopping the gradient flow at specific layers to prevent the vanishing gradient problem"
    ],
    "answer": "Trading computation for memory by re-computing activations during the backward pass rather than storing them in VRAM",
    "explanation": "Gradient checkpointing saves memory by not storing intermediate activations during the forward pass; instead, it discards them and re-computes them on the fly during backpropagation.",
    "difficulty": "Advanced"
  },
  {
    "id": 90,
    "question": "In the context of RLHF (Reinforcement Learning from Human Feedback), what is the purpose of the KL (Kullback-Leibler) divergence penalty in the PPO objective?",
    "options": [
      "To maximize the entropy of the generated tokens to ensure diversity",
      "To prevent the model from deviating too far from the initial policy (Language Model) to avoid reward hacking",
      "To minimize the difference between the human label distribution and the model distribution",
      "To ensure that the value function approximator converges to a global optimum"
    ],
    "answer": "To prevent the model from deviating too far from the initial policy (Language Model) to avoid reward hacking",
    "explanation": "The KL penalty constrains the updated policy to stay close to the reference policy, ensuring the model optimizes for reward without degenerating into gibberish or repetitive output that tricks the reward model.",
    "difficulty": "Advanced"
  },
  {
    "id": 91,
    "question": "What is the definition of 'Data Isotropy' in high-dimensional vector embeddings?",
    "options": [
      "All vectors in the embedding space are equally distant from each other",
      "Data vectors are uniformly distributed across the hypersphere, improving the retrieval effectiveness of approximate nearest neighbor (ANN) algorithms",
      "The embeddings are normalized to unit length to enable dot-product similarity",
      "The dimensionality of the vector space is reduced to isotropically compress the model size"
    ],
    "answer": "Data vectors are uniformly distributed across the hypersphere, improving the retrieval effectiveness of approximate nearest neighbor (ANN) algorithms",
    "explanation": "Isotropic embeddings prevent the 'hubness problem' (where some points are neighbors to almost all others) and allow vector databases like FAISS to perform more efficient and accurate spatial partitioning.",
    "difficulty": "Advanced"
  },
  {
    "id": 92,
    "question": "What is the 'Context Window' in an LLM, and what dictates its maximum size?",
    "options": [
      "The vocabulary size of the tokenizer",
      "The sequence length the model was trained on and the memory available for the KV Cache",
      "The number of layers in the Transformer architecture",
      "The bandwidth of the PCIe connection connecting the CPU and GPU"
    ],
    "answer": "The sequence length the model was trained on and the memory available for the KV Cache",
    "explanation": "The context window is limited by both the positional embeddings (trained length) and hardware constraints (KV cache size), though techniques like ALiBi or RoPE can allow extrapolation beyond trained lengths.",
    "difficulty": "Advanced"
  },
  {
    "id": 93,
    "question": "Which technique involves training a model on a sequence of examples and asking it to predict the last one, without updating the model weights?",
    "options": [
      "Zero-shot prompting",
      "In-Context Learning (Few-Shot Prompting)",
      "Fine-tuning",
      "Reinforcement Learning"
    ],
    "answer": "In-Context Learning (Few-Shot Prompting)",
    "explanation": "In-context learning refers to the model's ability to learn a task from demonstrations provided in the prompt context without any gradient updates.",
    "difficulty": "Advanced"
  },
  {
    "id": 94,
    "question": "What is the function of the 'SwiGLU' activation function in models like Llama-2 compared to ReLU?",
    "options": [
      "SwiGLU is computationally cheaper and requires no gates",
      "SwiGLU combines a Swish gating mechanism and a linear transformation to increase parameter count and improve expressiveness",
      "SwiGLU prevents the vanishing gradient problem by using a constant linear slope",
      "SwiGLU discretizes the output into binary states for faster inference"
    ],
    "answer": "SwiGLU combines a Swish gating mechanism and a linear transformation to increase parameter count and improve expressiveness",
    "explanation": "SwiGLU (Swish-Gated Linear Unit) replaces the standard ReLU non-linearity in the feed-forward network, offering better performance despite slightly higher computational cost.",
    "difficulty": "Advanced"
  },
  {
    "id": 95,
    "question": "Why are 'Positional Encodings' necessary in Transformer architectures?",
    "options": [
      "Transformers process tokens sequentially, and encodings mark the end of the sequence",
      "The self-attention mechanism is permutation invariant and has no inherent sense of order",
      "They allow the model to process variable batch sizes during training",
      "They encode the semantic meaning of the words into vector indices"
    ],
    "answer": "The self-attention mechanism is permutation invariant and has no inherent sense of order",
    "explanation": "Without positional encodings, the attention mechanism would treat the input as a 'bag of words,' ignoring the grammatical and semantic structure provided by word order.",
    "difficulty": "Advanced"
  },
  {
    "id": 96,
    "question": "What is 'Out-of-Distribution' (OOD) detection?",
    "options": [
      "Identifying data points that are outliers in the training set",
      "Detecting inputs that differ significantly from the training distribution, indicating the model may not be reliable",
      "Filtering out malicious inputs designed to poison the model",
      "Balancing the dataset distribution across different classes"
    ],
    "answer": "Detecting inputs that differ significantly from the training distribution, indicating the model may not be reliable",
    "explanation": "OOD detection is a safety mechanism identifying test data where the model's predictions are effectively extrapolation, rather than interpolation, to prevent overconfident errors.",
    "difficulty": "Advanced"
  },
  {
    "id": 97,
    "question": "Which regularization technique, commonly used in training modern LLMs, prevents gradient explosion by capping the maximum value of gradients?",
    "options": [
      "Dropout",
      "Weight Decay",
      "Gradient Clipping",
      "Layer Normalization"
    ],
    "answer": "Gradient Clipping",
    "explanation": "Gradient clipping rescales the gradient vector if its L2 norm exceeds a threshold, preventing the optimization from diverging due to exploding gradients in deep networks.",
    "difficulty": "Advanced"
  },
  {
    "id": 98,
    "question": "What distinguishes 'Semantic Search' from 'Lexical Search'?",
    "options": [
      "Semantic search requires exact keyword matches, while lexical search looks for meaning",
      "Semantic search uses vector embeddings to match intent and meaning, while lexical search relies on keyword matching (e.g., TF-IDF, BM25)",
      "Lexical search is strictly superior for long-tail queries, whereas semantic search is for head queries",
      "Semantic search only works on structured data tables"
    ],
    "answer": "Semantic search uses vector embeddings to match intent and meaning, while lexical search relies on keyword matching (e.g., TF-IDF, BM25)",
    "explanation": "While lexical search matches specific tokens, semantic search understands the contextual meaning of the query, allowing it to find relevant results even if exact keywords aren't present.",
    "difficulty": "Advanced"
  },
  {
    "id": 99,
    "question": "What is the 'Root Mean Square Logarithmic Error' (RMSLE) metric primarily used to evaluate?",
    "options": [
      "Classification accuracy for imbalanced datasets",
      "Regression problems where large errors in exponential growth predictions are undesirable to penalize",
      "Clustering quality in unsupervised learning",
      "Ranking quality in recommendation systems"
    ],
    "answer": "Regression problems where large errors in exponential growth predictions are undesirable to penalize",
    "explanation": "RMSLE penalizes underestimates more than overestimates and is robust to outliers, making it suitable for targets like revenue or population where errors scale with magnitude.",
    "difficulty": "Advanced"
  },
  {
    "id": 100,
    "question": "In the context of MLOps maturity, what characterizes a 'Level 2' (Automated) pipeline compared to a 'Level 1' (Manual) pipeline?",
    "options": [
      "Data scientists manually trigger training and deployment scripts",
      "Continuous training and deployment are triggered automatically based on data drift or performance triggers",
      "The model is deployed once and never updated",
      "There is no separation between training and inference environments"
    ],
    "answer": "Continuous training and deployment are triggered automatically based on data drift or performance triggers",
    "explanation": "Level 2 MLOps moves beyond simple CI/CD (Level 1) to continuous training (CT), where the system automatically retrains and revalidates models in response to data changes or performance degradation.",
    "difficulty": "Advanced"
  }
]