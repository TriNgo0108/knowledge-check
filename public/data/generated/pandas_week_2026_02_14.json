[
  {
    "id": 1,
    "question": "Which of the following best describes the primary data structure in pandas used for two-dimensional, size-mutable, potentially heterogeneous tabular data?",
    "options": [
      "List",
      "Dictionary",
      "DataFrame",
      "ndarray"
    ],
    "answer": "DataFrame",
    "explanation": "A DataFrame is a 2-dimensional labeled data structure with columns of potentially different types, similar to a spreadsheet or SQL table. Lists and Dictionaries are built-in Python types, while ndarray is the primary NumPy structure.",
    "difficulty": "Beginner"
  },
  {
    "id": 2,
    "question": "What is the standard method used to read a comma-separated values (CSV) file into a pandas DataFrame?",
    "options": [
      "pandas.load_csv()",
      "pandas.read_csv()",
      "pandas.import_csv()",
      "pandas.open_csv()"
    ],
    "answer": "pandas.read_csv()",
    "explanation": "read_csv() is the primary function for parsing CSV files and converting them into pandas DataFrame objects. Functions like load_csv or import_csv do not exist in the standard library.",
    "difficulty": "Beginner"
  },
  {
    "id": 3,
    "question": "Which attribute allows you to access the row index labels of a DataFrame?",
    "options": [
      "df.columns",
      "df.index",
      "df.axes",
      "df.values"
    ],
    "answer": "df.index",
    "explanation": "The index attribute stores the row labels (axis 0). df.columns stores column labels, df.axes returns a list of row and column labels, and df.values returns the numpy array representation.",
    "difficulty": "Beginner"
  },
  {
    "id": 4,
    "question": "Which indexer is strictly label-based, meaning you must use the actual names of the rows or columns?",
    "options": [
      "df.iloc[]",
      "df.loc[]",
      "df.ix[]",
      "df.where()"
    ],
    "answer": "df.loc[]",
    "explanation": ".loc is primarily label-based, whereas .iloc is strictly integer-position-based. .ix was deprecated and removed in pandas 1.0.0.",
    "difficulty": "Beginner"
  },
  {
    "id": 5,
    "question": "When performing a boolean indexing operation to filter rows, what type of data must be passed inside the square brackets?",
    "options": [
      "A list of column names",
      "A boolean Series of the same length as the DataFrame",
      "A tuple of row indices",
      "A dictionary of data values"
    ],
    "answer": "A boolean Series of the same length as the DataFrame",
    "explanation": "Boolean filtering requires a boolean array or Series matching the DataFrame's length; rows corresponding to True are retained. Lists of names select columns, while tuples are used for hierarchical index selections.",
    "difficulty": "Beginner"
  },
  {
    "id": 6,
    "question": "Which method is used to identify missing values (NaN, None) in a DataFrame?",
    "options": [
      "df.isnull()",
      "df.has_na()",
      "df.empty()",
      "df.isnan()"
    ],
    "answer": "df.isnull()",
    "explanation": "isnull() (synonym for isna()) returns a boolean mask indicating where values are missing. Methods like has_na or empty do not exist for this purpose.",
    "difficulty": "Beginner"
  },
  {
    "id": 7,
    "question": "What does the `inplace=True` parameter accomplish in pandas methods like `drop()` or `fillna()`?",
    "options": [
      "It creates a deep copy of the data before modifying it",
      "It returns a new DataFrame by default",
      "It modifies the existing DataFrame object directly and returns None",
      "It automatically saves the DataFrame to a CSV file"
    ],
    "answer": "It modifies the existing DataFrame object directly and returns None",
    "explanation": "Setting inplace=True alters the object directly without creating a new copy, saving memory. By default, pandas returns a new object with the applied changes, leaving the original unchanged.",
    "difficulty": "Beginner"
  },
  {
    "id": 8,
    "question": "Which statistical method returns the standard deviation of the values in a Series or DataFrame?",
    "options": [
      "std()",
      "var()",
      "dev()",
      "avg()"
    ],
    "answer": "std()",
    "explanation": "std() calculates the sample standard deviation. var() calculates variance, while dev() and avg() are not standard pandas methods.",
    "difficulty": "Beginner"
  },
  {
    "id": 9,
    "question": "What is the result of applying the `groupby()` method on a DataFrame?",
    "options": [
      "A list of DataFrames",
      "A new sorted DataFrame",
      "A DataFrameGroupBy object that requires an aggregation function to be useful",
      "A dictionary of grouped lists"
    ],
    "answer": "A DataFrameGroupBy object that requires an aggregation function to be useful",
    "explanation": "groupby() creates a lazy object (DataFrameGroupBy) that splits the data but does not compute results until an aggregation (like sum, mean, count) is applied.",
    "difficulty": "Beginner"
  },
  {
    "id": 10,
    "question": "Which function is used to concatenate two or more pandas objects along a particular axis?",
    "options": [
      "pd.join()",
      "pd.merge()",
      "pd.concat()",
      "pd.bind()"
    ],
    "answer": "pd.concat()",
    "explanation": "pd.concat() is the general function for concatenating Series or DataFrames along rows (axis 0) or columns (axis 1). merge() and join() are used for database-style joins based on keys.",
    "difficulty": "Beginner"
  },
  {
    "id": 11,
    "question": "How do you select a single column 'Age' from a DataFrame df?",
    "options": [
      "df('Age')",
      "df[['Age']]",
      "df.Age",
      "df.Age and df['Age']"
    ],
    "answer": "df.Age and df['Age']",
    "explanation": "A single column can be selected using attribute access (df.Age) or dictionary-like notation (df['Age']). df[['Age']] returns a DataFrame, not a Series.",
    "difficulty": "Beginner"
  },
  {
    "id": 12,
    "question": "What is the default separator character used by pandas `read_csv`?",
    "options": [
      "Comma (,)",
      "Tab (\\t)",
      "Semicolon (;)",
      "Pipe (|)"
    ],
    "answer": "Comma (,)",
    "explanation": "As implied by the name 'csv' (Comma-Separated Values), the default delimiter is a comma. Other delimiters must be specified using the `sep` or `delimiter` argument.",
    "difficulty": "Beginner"
  },
  {
    "id": 13,
    "question": "Which method provides a summary of the DataFrame's non-null counts and data types?",
    "options": [
      "df.head()",
      "df.describe()",
      "df.info()",
      "df.dtypes"
    ],
    "answer": "df.info()",
    "explanation": "df.info() prints a concise summary of the index, dtype, and non-null values. describe() shows statistical summaries, and dtypes shows only the data types.",
    "difficulty": "Beginner"
  },
  {
    "id": 14,
    "question": "What is the primary difference between `merge()` and `join()` in pandas?",
    "options": [
      "merge() is for indices, join() is for columns",
      "join() is an alias for merge() with no difference",
      "join() uses the index by default, while merge() uses specified columns",
      "merge() is slower than join()"
    ],
    "answer": "join() uses the index by default, while merge() uses specified columns",
    "explanation": "join() is a convenience method that merges on indexes by default. merge() is more general and merges on columns or indexes, requiring explicit key specification.",
    "difficulty": "Beginner"
  },
  {
    "id": 15,
    "question": "Which method is used to rename the labels of an axis (columns or index)?",
    "options": [
      "df.rename()",
      "df.set_index()",
      "df.reindex()",
      "df.alter()"
    ],
    "answer": "df.rename()",
    "explanation": "rename() allows altering axis labels based on a mapping (dict or function). set_index() converts a column into an index, and reindex() conforms data to a new set of labels.",
    "difficulty": "Beginner"
  },
  {
    "id": 16,
    "question": "In pandas, what does 'vectorization' refer to in the context of operations?",
    "options": [
      "Using C-extensions to compile code",
      "Applying operations to entire arrays at once rather than looping through elements",
      "Converting lists to numpy arrays",
      "Parallel processing across multiple CPU cores"
    ],
    "answer": "Applying operations to entire arrays at once rather than looping through elements",
    "explanation": "Vectorization utilizes low-level optimized array operations (often via NumPy) to process data in batches, avoiding slow Python loops.",
    "difficulty": "Beginner"
  },
  {
    "id": 17,
    "question": "How do you sort a DataFrame by the values in a specific column named 'Date'?",
    "options": [
      "df.order('Date')",
      "df.sort('Date')",
      "df.sort_values('Date')",
      "df.sort_index('Date')"
    ],
    "answer": "df.sort_values('Date')",
    "explanation": "sort_values() sorts the DataFrame by the values along either axis. sort_index() sorts by the index labels.",
    "difficulty": "Beginner"
  },
  {
    "id": 18,
    "question": "What does the `axis=1` parameter signify in methods like `df.drop()` or `df.sum()`?",
    "options": [
      "Operation on rows",
      "Operation on columns",
      "Operation on the index",
      "Operation on the DataFrame header"
    ],
    "answer": "Operation on columns",
    "explanation": "In pandas, axis=0 refers to rows (index), and axis=1 refers to columns. Using axis=1 in drop() removes a column, while in sum() it sums across the row.",
    "difficulty": "Beginner"
  },
  {
    "id": 19,
    "question": "Which method is used to apply a function to every element of a DataFrame?",
    "options": [
      "df.apply()",
      "df.applymap() (or df.map() in newer versions)",
      "df.transform()",
      "df.aggregate()"
    ],
    "answer": "df.applymap() (or df.map() in newer versions)",
    "explanation": "applymap() (deprecated for map() in Pandas 2.1.0) applies a function element-wise. apply() works on a row/column basis. transform() works on specific groups/series.",
    "difficulty": "Beginner"
  },
  {
    "id": 20,
    "question": "How can you check the number of dimensions (axes) of a pandas object?",
    "options": [
      "df.ndim",
      "df.shape",
      "df.size",
      "df.rank"
    ],
    "answer": "df.ndim",
    "explanation": "ndim returns the number of axes (2 for DataFrames). shape returns a tuple of dimensions, and size returns the total number of elements.",
    "difficulty": "Beginner"
  },
  {
    "id": 21,
    "question": "What is the purpose of the `pd.cut()` function?",
    "options": [
      "To remove rows from a DataFrame",
      "To bin continuous values into discrete intervals",
      "To slice a DataFrame into smaller chunks",
      "To cut strings based on a delimiter"
    ],
    "answer": "To bin continuous values into discrete intervals",
    "explanation": "pd.cut() is a discretization function used to segment and sort data values into bins (intervals).",
    "difficulty": "Beginner"
  },
  {
    "id": 22,
    "question": "Which method is used to fill missing values with a specific value or strategy?",
    "options": [
      "df.clean()",
      "df.fillna()",
      "df.replace()",
      "df.interpolate()"
    ],
    "answer": "df.fillna()",
    "explanation": "fillna() fills NA/NaN values with a specified value or method (like ffill or bfill). replace() substitutes specific values, and interpolate() fills using interpolation.",
    "difficulty": "Beginner"
  },
  {
    "id": 23,
    "question": "What does the `reset_index()` method do when called with `drop=True`?",
    "options": [
      "It drops the current index and replaces it with a RangeIndex",
      "It drops duplicate rows from the DataFrame",
      "It drops the column named 'index'",
      "It removes the index entirely without adding a column to the DataFrame"
    ],
    "answer": "It removes the index entirely without adding a column to the DataFrame",
    "explanation": "drop=True in reset_index() prevents the old index from being inserted as a column in the DataFrame.",
    "difficulty": "Beginner"
  },
  {
    "id": 24,
    "question": "Which accessor is used to apply string methods to elements in a Series of strings?",
    "options": [
      "df.str",
      "df.string",
      "df.str_methods",
      "df.cat"
    ],
    "answer": "df.str",
    "explanation": "The .str accessor provides vectorized string operations for Series. .cat is for categorical data.",
    "difficulty": "Beginner"
  },
  {
    "id": 25,
    "question": "What is the output of `df['A'].unique()` on a Series 'A'?",
    "options": [
      "Counts of unique values",
      "A NumPy array of unique values",
      "A boolean Series",
      "A DataFrame of unique rows"
    ],
    "answer": "A NumPy array of unique values",
    "explanation": "unique() returns the unique values as a NumPy array. value_counts() returns the counts.",
    "difficulty": "Beginner"
  },
  {
    "id": 26,
    "question": "Which function would you use to convert a list of dictionaries into a pandas DataFrame?",
    "options": [
      "pd.DataFrame.from_dict()",
      "pd.DataFrame(list_of_dicts)",
      "pd.from_records()",
      "pd.Series.to_frame()"
    ],
    "answer": "pd.DataFrame(list_of_dicts)",
    "explanation": "The DataFrame constructor can accept a list of dictionaries directly. from_dict() expects a single dictionary.",
    "difficulty": "Beginner"
  },
  {
    "id": 27,
    "question": "What is the difference between `rank()` and `sort_values()`?",
    "options": [
      "sort_values() reorders the data, rank() assigns a numerical rank to each element",
      "rank() sorts the data, sort_values() assigns ranks",
      "There is no difference",
      "rank() works only on strings"
    ],
    "answer": "sort_values() reorders the data, rank() assigns a numerical rank to each element",
    "explanation": "sort_values() physically rearranges the rows/columns. rank() returns a Series containing the rank of each element (1 to n) without changing the data order.",
    "difficulty": "Beginner"
  },
  {
    "id": 28,
    "question": "Which method returns the first n rows of a DataFrame?",
    "options": [
      "df.first(n)",
      "df.top(n)",
      "df.head(n)",
      "df.begin(n)"
    ],
    "answer": "df.head(n)",
    "explanation": "head(n) returns the first n rows. tail(n) returns the last n rows.",
    "difficulty": "Beginner"
  },
  {
    "id": 29,
    "question": "What is the default behavior of `pd.merge()`?",
    "options": [
      "Inner join on all common columns",
      "Left join on the index",
      "Outer join on specific columns",
      "Cross join"
    ],
    "answer": "Inner join on all common columns",
    "explanation": "If not specified, merge() performs an inner join based on the intersection of columns with the same names in both tables.",
    "difficulty": "Beginner"
  },
  {
    "id": 30,
    "question": "What does `memory_usage(deep=True)` calculate that `memory_usage()` does not?",
    "options": [
      "Memory usage of the index only",
      "Total memory usage including object types (strings) usage",
      "Memory usage of the disk file",
      "Virtual memory allocation"
    ],
    "answer": "Total memory usage including object types (strings) usage",
    "explanation": "deep=True introspects object dtypes (like strings) to calculate their actual memory consumption, which is otherwise estimated at a fixed size.",
    "difficulty": "Beginner"
  },
  {
    "id": 31,
    "question": "How do you drop rows containing any missing values from a DataFrame `df`?",
    "options": [
      "df.dropna(axis=0)",
      "df.dropna(axis=1)",
      "df.delna()",
      "df.clean()"
    ],
    "answer": "df.dropna(axis=0)",
    "explanation": "dropna() with axis=0 (default) drops rows containing NaNs. axis=1 would drop columns.",
    "difficulty": "Beginner"
  },
  {
    "id": 32,
    "question": "Which method is used to apply a function row-wise or column-wise?",
    "options": [
      "df.map()",
      "df.apply()",
      "df.transform()",
      "df.pipe()"
    ],
    "answer": "df.apply()",
    "explanation": "apply() applies a function along an axis (row or column). map() works element-wise on Series. pipe() chains functions.",
    "difficulty": "Beginner"
  },
  {
    "id": 33,
    "question": "What is the output of `df.shape`?",
    "options": [
      "A tuple representing dimensionality (rows, columns)",
      "An integer representing total elements",
      "A list of column names",
      "The number of dimensions"
    ],
    "answer": "A tuple representing dimensionality (rows, columns)",
    "explanation": "shape returns a tuple (n_rows, n_columns). size returns the total element count.",
    "difficulty": "Beginner"
  },
  {
    "id": 34,
    "question": "Which method creates a pivot table as a DataFrame?",
    "options": [
      "df.transform()",
      "pd.pivot_table()",
      "df.groupby()",
      "pd.unstack()"
    ],
    "answer": "pd.pivot_table()",
    "explanation": "pivot_table() creates a spreadsheet-style pivot table aggregating data. groupby() is the underlying mechanism for grouping, but pivot_table handles the reshaping and aggregating logic.",
    "difficulty": "Beginner"
  },
  {
    "id": 35,
    "question": "What is a 'SettingWithCopyWarning' intended to warn the user about?",
    "options": [
      "Attempting to modify a view of a DataFrame that might not affect the original DataFrame",
      "Trying to set a value with the wrong data type",
      "Modifying a DataFrame while iterating over it",
      "Setting a column that does not exist"
    ],
    "answer": "Attempting to modify a view of a DataFrame that might not affect the original DataFrame",
    "explanation": "This warning alerts you to potential chained assignment issues where the result of an operation (a view) is being modified, which may or may not update the original data as intended.",
    "difficulty": "Beginner"
  },
  {
    "id": 36,
    "question": "What is the primary consequence of using chained assignment when modifying a DataFrame in pandas?",
    "options": [
      "It automatically creates a deep copy of the DataFrame, consuming excessive memory",
      "It returns a SettingWithCopyWarning, indicating the operation may not work as intended due to view logic",
      "It forces the operation to run on the CPU, bypassing NumPy vectorization",
      "It disables the Index object, requiring the DataFrame to be reindexed immediately"
    ],
    "answer": "It returns a SettingWithCopyWarning, indicating the operation may not work as intended due to view logic",
    "explanation": "Chained assignment (e.g., df[df['A'] > 0]['B'] = 0) often operates on a temporary copy of the data slice. Pandas warns that the assignment might fail to update the original DataFrame because it cannot reliably determine if the slice is a view or a copy.",
    "difficulty": "Intermediate"
  },
  {
    "id": 37,
    "question": "When grouping data, what is the functional difference between the `transform` and `filter` methods?",
    "options": [
      "`transform` returns a boolean mask, while `filter` returns the grouped aggregates as a new DataFrame",
      "`transform` must return a scalar value, whereas `filter` can return Series or DataFrames",
      "`transform` returns a Series/DataFrame of the same shape as the input, while `filter` discards entire groups based on a condition",
      "`transform` aggregates data and reduces the index, while `filter` preserves the original index structure"
    ],
    "answer": "`transform` returns a Series/DataFrame of the same shape as the input, while `filter` discards entire groups based on a condition",
    "explanation": "`transform` applies a function to each group and broadcasts the result back to the original index shape, ideal for adding aggregated columns. `filter` evaluates a boolean condition per group and drops entire rows from groups that return False.",
    "difficulty": "Intermediate"
  },
  {
    "id": 38,
    "question": "How does `pd.cut` differ fundamentally from `pd.qcut`?",
    "options": [
      "`pd.cut` computes quantile-based bins, whereas `pd.qcut` computes equal-width bins",
      "`pd.cut` requires bins to be of equal width, whereas `pd.qcut` handles variable bin sizes based on distribution",
      "`pd.cut` divides data into equal-width bins, whereas `pd.qcut` divides data into bins with equal frequencies (quantiles)",
      "`pd.cut` is used for numerical data only, whereas `pd.qcut` is exclusively for datetime intervals"
    ],
    "answer": "`pd.cut` divides data into equal-width bins, whereas `pd.qcut` divides data into bins with equal frequencies (quantiles)",
    "explanation": "`pd.cut` creates bins based on value ranges (e.g., 0-10, 10-20), potentially resulting in uneven counts. `pd.qcut` creates bins such that each bin contains approximately the same number of records, resulting in variable value ranges.",
    "difficulty": "Intermediate"
  },
  {
    "id": 39,
    "question": "In the context of pandas performance, why is `itertuples()` generally preferred over `iterrows()`?",
    "options": [
      "`itertuples()` preserves the data types of the columns, while `iterrows()` converts all data to objects",
      "`itertuples()` returns a view of the DataFrame, while `iterrows()` returns a copy",
      "`itertuples()` can handle multi-indexing natively, whereas `iterrows()` cannot",
      "`itertuples()` does not require the NumPy library, making it faster for pure Python objects"
    ],
    "answer": "`itertuples()` preserves the data types of the columns, while `iterrows()` converts all data to objects",
    "explanation": "`iterrows()` converts each row into a pandas Series, which generally changes dtypes to object and incurs high overhead. `itertuples()` returns data as namedtuples, preserving the original C-level dtypes and being significantly faster and more memory-efficient.",
    "difficulty": "Intermediate"
  },
  {
    "id": 40,
    "question": "What is the specific behavior of the `inplace=True` parameter in pandas methods?",
    "options": [
      "It modifies the object in memory and returns the modified object to facilitate method chaining",
      "It avoids creating a new copy of the data, but prevents chaining as it returns None",
      "It uses the Python `id()` to ensure the operation is done on the CPU cache",
      "It overrides the Index lock to allow concurrent writes from multiple threads"
    ],
    "answer": "It avoids creating a new copy of the data, but prevents chaining as it returns None",
    "explanation": "While intended to save memory by operating on the existing object, `inplace=True` generally causes the method to return `None`. This prevents method chaining (e.g., `df.dropna().reset_index()` will fail if `dropna` is inplace).",
    "difficulty": "Intermediate"
  },
  {
    "id": 41,
    "question": "When using `pd.merge`, how does the `validate='one_to_one'` argument assist in data integrity?",
    "options": [
      "It ensures that the merged keys contain only unique values in both left and right DataFrames",
      "It automatically drops duplicate rows in the right DataFrame to ensure a 1:1 relationship",
      "It forces the merge to use the 'inner' method to ensure exact matches",
      "It checks that the merge keys are sorted in ascending order in both DataFrames"
    ],
    "answer": "It ensures that the merged keys contain only unique values in both left and right DataFrames",
    "explanation": "The `validate` argument checks if the merge keys are unique in the specified manner. `'one_to_one'` ensures a perfect unique mapping in both frames; if duplicates exist in either, a `MergeError` is raised, alerting the user to unexpected cardinality.",
    "difficulty": "Intermediate"
  },
  {
    "id": 42,
    "question": "What distinguishes `pd.concat` with `axis=0` from `pd.merge` when combining DataFrames?",
    "options": [
      "`pd.concat` stacks DataFrames vertically (adding rows) and performs alignment based on column names, while `merge` performs database-style joins based on shared columns or indices",
      "`pd.concat` can only combine DataFrames with identical indices, while `merge` handles arbitrary indices",
      "`pd.merge` is strictly an inner join, whereas `pd.concat` is strictly an outer join",
      "`pd.concat` requires a foreign key relationship, while `merge` concatenates based on position"
    ],
    "answer": "`pd.concat` stacks DataFrames vertically (adding rows) and performs alignment based on column names, while `merge` performs database-style joins based on shared columns or indices",
    "explanation": "`concat` is a general-purpose concatenation function that glues data along a specific axis (stacking rows or columns) with optional set logic (union/intersection). `merge` (and `join`) is specifically designed for relational database-style operations combining data based on key values.",
    "difficulty": "Intermediate"
  },
  {
    "id": 43,
    "question": "What is the primary use case for the `pipe` method in pandas?",
    "options": [
      "To aggregate data using custom statistical functions that are not built-in",
      "To apply a function to every element of a Series or DataFrame individually",
      "To inject a user-defined function or external library function into a method chain",
      "To pipeline I/O operations, allowing multiple file formats to be read simultaneously"
    ],
    "answer": "To inject a user-defined function or external library function into a method chain",
    "explanation": "While `apply` works on rows/columns, `pipe` allows the entire DataFrame to be passed as an argument to a custom function. This enables custom workflows or third-party functions to be used seamlessly within a method chain (e.g., `df.pipe(func, arg1).reset_index()`).",
    "difficulty": "Intermediate"
  },
  {
    "id": 44,
    "question": "Why is the `category` dtype recommended for columns with low cardinality?",
    "options": [
      "It automatically calculates one-hot encoded variables for machine learning models",
      "It uses memory-efficient internal representation (integers) instead of raw string values",
      "It forces the column to be sorted alphabetically to speed up binary search",
      "It converts the data to a Cyclic redundancy check (CRC) hash for faster merging"
    ],
    "answer": "It uses memory-efficient internal representation (integers) instead of raw string values",
    "explanation": "The `category` dtype stores data as integer codes mapping to a lookup table of unique values. For columns with repetitive strings (e.g., 'Male'/'Female'), this drastically reduces memory usage compared to storing the full string for every row.",
    "difficulty": "Intermediate"
  },
  {
    "id": 45,
    "question": "What is the result of setting `drop_first=True` when using `pd.get_dummies`?",
    "options": [
      "It drops the first column of the original DataFrame before creating dummy variables",
      "It removes the first level of a MultiIndex to flatten the result",
      "It creates k-1 dummy variables for a categorical variable with k categories to avoid perfect multicollinearity",
      "It drops the first row of the returned DataFrame to ensure no index overlap"
    ],
    "answer": "It creates k-1 dummy variables for a categorical variable with k categories to avoid perfect multicollinearity",
    "explanation": "In linear regression models, having k dummy variables for a k-category feature causes perfect multicollinearity (dummy variable trap). `drop_first=True` removes one level, making the data mathematically suitable for such models.",
    "difficulty": "Intermediate"
  },
  {
    "id": 46,
    "question": "When using `df.pivot`, under what condition will the function raise a `ValueError`?",
    "options": [
      "If the `columns` parameter contains only string values",
      "If there are duplicate entries in the index/columns combination",
      "If the resulting DataFrame contains only NaN values",
      "If the `index` is not set to the default RangeIndex"
    ],
    "answer": "If there are duplicate entries in the index/columns combination",
    "explanation": "`df.pivot` requires that the index/columns combinations be unique; it produces a reshaped table. If there are duplicates (e.g., two sales for the same month/product), the data cannot fit neatly into the grid, and pandas raises a ValueError. `pivot_table` should be used instead with an aggregation function.",
    "difficulty": "Intermediate"
  },
  {
    "id": 47,
    "question": "How does the `eval` method improve performance in pandas operations involving large DataFrames?",
    "options": [
      "It compiles the operation into C code using Cython automatically",
      "It uses string expression parsing to leverage NumPy's array operations and reduce intermediate memory allocation",
      "It converts the DataFrame into a SQLite database to execute SQL queries",
      "It parallelizes the operation across all available CPU cores using multiprocessing"
    ],
    "answer": "It uses string expression parsing to leverage NumPy's array operations and reduce intermediate memory allocation",
    "explanation": "`eval` parses a string expression (e.g., 'df.A > df.B + 5') and evaluates it using specialized 'numexpr' engines or optimized backends. This avoids creating temporary arrays for each intermediate step, speeding up complex algebraic expressions.",
    "difficulty": "Intermediate"
  },
  {
    "id": 48,
    "question": "What is the behavior of `stack()` on a DataFrame with a MultiIndex on the columns?",
    "options": [
      "It converts the innermost column index level into the innermost row index level",
      "It flattens the MultiIndex into a single index, separating levels with underscores",
      "It merges all columns into a single column containing tuples",
      "It sorts the DataFrame by the outermost column level"
    ],
    "answer": "It converts the innermost column index level into the innermost row index level",
    "explanation": "`stack()` 'rotates' the dataframe, pivoting the columns (usually the innermost level of a MultiIndex) into the index rows. This effectively transforms a wide table into a long (tall) format.",
    "difficulty": "Intermediate"
  },
  {
    "id": 49,
    "question": "In a `groupby` operation, what distinguishes the `agg` method from a direct aggregation method like `.sum()`?",
    "options": [
      "`agg` can process multiple different aggregation functions (or a list of functions) at once, while direct methods usually apply one",
      "`agg` performs the aggregation in parallel, whereas `.sum()` is single-threaded",
      "`agg` changes the data types of the result to float64 by default",
      "There is no difference; `agg` is simply an alias for the specific aggregation methods"
    ],
    "answer": "`agg` can process multiple different aggregation functions (or a list of functions) at once, while direct methods usually apply one",
    "explanation": "The `agg` (or `aggregate`) method allows for flexible aggregation, accepting lists (e.g., `['mean', 'sum']`), dicts (mapping cols to funcs), or custom functions. Direct methods like `.sum()` apply a single, pre-defined aggregation function.",
    "difficulty": "Intermediate"
  },
  {
    "id": 50,
    "question": "What is the default parameter value for the `method` argument in `df.fillna` when forward filling time-series data?",
    "options": [
      "'bfill' (backward fill)",
      "'ffill' (forward fill) only for scalar values",
      "'pad' (equivalent to forward fill)",
      "'nearest' based on index"
    ],
    "answer": "'pad' (equivalent to forward fill)",
    "explanation": "While users often pass `'ffill'` explicitly, the underlying behavior for forward propagation is often aliased as `'pad'` (propagate last valid observation forward). If `method` is None (default), `fillna` uses the `value` parameter instead of a propagation method.",
    "difficulty": "Intermediate"
  },
  {
    "id": 51,
    "question": "What does the `nunique()` method calculate when applied to a grouped DataFrame?",
    "options": [
      "The count of non-NaN values in each group",
      "The number of distinct (unique) values in each group",
      "The first unique value found in each group",
      "The number of unique index values in the DataFrame"
    ],
    "answer": "The number of distinct (unique) values in each group",
    "explanation": "`nunique` counts the number of distinct elements within each group. This is distinct from `count`, which simply counts the total number of non-null entries.",
    "difficulty": "Intermediate"
  },
  {
    "id": 52,
    "question": "How does `df.explode` transform a DataFrame column containing list-like objects?",
    "options": [
      "It converts the list-like objects into string representations separated by commas",
      "It flattens the list-like objects, duplicating the index elements for each item in the list",
      "It removes any rows where the list-like object contains NaN values",
      "It aggregates the list-like objects into a single list for the entire column"
    ],
    "answer": "It flattens the list-like objects, duplicating the index elements for each item in the list",
    "explanation": "`explode` transforms each element in a list-like object into a row. This creates a 'long' format DataFrame where rows for other columns are replicated to match the expanded list items.",
    "difficulty": "Intermediate"
  },
  {
    "id": 53,
    "question": "When resampling a time series, what is the difference between `resample().mean()` and `resample().asfreq()`?",
    "options": [
      "`asfreq` returns the first value of the interval, while `mean` calculates the average",
      "`asfreq` simply selects the data point at the new frequency (potentially filling NaNs), while `mean` aggregates the interval",
      "`asfreq` is strictly for upsampling, while `mean` is strictly for downsampling",
      "`asfreq` interpolates missing values, while `mean` drops them"
    ],
    "answer": "`asfreq` simply selects the data point at the new frequency (potentially filling NaNs), while `mean` aggregates the interval",
    "explanation": "`asfreq` is a low-level function that changes the frequency to the target date, resulting in NaNs if the specific timestamp doesn't exist (unless filled). `mean` is an aggregation function that calculates the average of values falling within the specific bin.",
    "difficulty": "Intermediate"
  },
  {
    "id": 54,
    "question": "What is the function of the `pd.Grouper` object in group operations?",
    "options": [
      "To calculate statistical gradients (slope) for grouped data",
      "To specify grouping keys with specific frequency or offset logic, particularly for time series",
      "To group data strictly by the integer values of the index",
      "To force the groupby operation to run in parallel using the CPU groups"
    ],
    "answer": "To specify grouping keys with specific frequency or offset logic, particularly for time series",
    "explanation": "`pd.Grouper` allows for advanced grouping, often used to group a DataFrame by a specific time frequency (e.g., `pd.Grouper(key='date', freq='M')`) alongside other column groupings.",
    "difficulty": "Intermediate"
  },
  {
    "id": 55,
    "question": "Why is `df.loc[rows, cols]` preferred over chained indexing like `df[cols][rows]`?",
    "options": [
      "Chained indexing triggers garbage collection on the intermediate step",
      "Chained indexing generally returns a copy versus a view, making assignment unpredictable",
      "`loc` automatically casts data types to float64 for precision",
      "Chained indexing is deprecated and will be removed in pandas 3.0"
    ],
    "answer": "Chained indexing generally returns a copy versus a view, making assignment unpredictable",
    "explanation": "Chained indexing `df[cols][rows]` creates two separate indexing operations. The first `df[cols]` may return a copy, so the second `[rows]` modifies the temporary copy, not the original DataFrame. `loc` ensures a single, direct modification.",
    "difficulty": "Intermediate"
  },
  {
    "id": 56,
    "question": "What is the purpose of the `convert_dtypes()` method?",
    "options": [
      "To convert all columns to float64 for numerical precision",
      "To infer and cast columns to the best possible dtypes using `dtypes` that support `pd.NA` (e.g., `StringDtype`, `Int64`)",
      "To convert object data into categorical data automatically",
      "To standardize datetime columns to the UTC timezone"
    ],
    "answer": "To infer and cast columns to the best possible dtypes using `dtypes` that support `pd.NA` (e.g., `StringDtype`, `Int64`)",
    "explanation": "Unlike `astype` or `infer_objects`, `convert_dtypes` converts columns to modern pandas nullable dtypes (like `string` instead of `object` and `Int64` instead of `int`), which support missing data (`pd.NA`) better than standard NumPy types.",
    "difficulty": "Intermediate"
  },
  {
    "id": 57,
    "question": "Which method is specifically designed to clean strings in a DataFrame column by removing leading and trailing whitespace?",
    "options": [
      "`df['col'].strip()`",
      "`df['col'].str.strip()`",
      "`df['col'].clean()`",
      "`df['col'].trim()`"
    ],
    "answer": "`df['col'].str.strip()`",
    "explanation": "Pandas provides the `.str` accessor for vectorized string operations. The `.str.strip()` method applies Python's `str.strip()` logic to every element in the Series, removing leading and trailing whitespace.",
    "difficulty": "Intermediate"
  },
  {
    "id": 58,
    "question": "In a pandas rolling window calculation, what does the `min_periods` argument control?",
    "options": [
      "The minimum time duration required for the window to be considered valid",
      "The minimum number of observations in the window required to have a value (otherwise result is NaN)",
      "The minimum frequency of the data points allowed in the calculation",
      "The minimum number of CPU cores required to run the calculation"
    ],
    "answer": "The minimum number of observations in the window required to have a value (otherwise result is NaN)",
    "explanation": "When calculating a rolling statistic, `min_periods` sets the threshold for valid data. If a window has fewer than this number of observations (e.g., at the start of the DataFrame), the result is NaN.",
    "difficulty": "Intermediate"
  },
  {
    "id": 59,
    "question": "What is the difference between `rank()` and `qcut()` regarding distribution?",
    "options": [
      "`rank` returns the index of the sorted element, while `qcut` assigns values to bins",
      "`rank` outputs a Series of ordinal ranks, while `qcut` bins data into quantiles and returns Categorical data",
      "`rank` is strictly for numeric data, while `qcut` is strictly for time-series data",
      "`rank` sorts the DataFrame in place, while `qcut` returns a GroupBy object"
    ],
    "answer": "`rank` outputs a Series of ordinal ranks, while `qcut` bins data into quantiles and returns Categorical data",
    "explanation": "`rank()` orders data and assigns a numerical rank to each entry (1, 2, 3...). `qcut()` (quantile cut) divides the data into bins so that each bin has roughly the same number of records, returning labels or bin identifiers.",
    "difficulty": "Intermediate"
  },
  {
    "id": 60,
    "question": "How does `df.reindex()` differ from `df.set_index()`?",
    "options": [
      "`set_index` moves a column into the index; `reindex` conforms the DataFrame to a new set of labels (filling or dropping data as needed)",
      "`reindex` sorts the DataFrame, while `set_index` creates a MultiIndex",
      "`set_index` modifies the data values, while `reindex` only modifies the index labels",
      "`reindex` is faster for large datasets, while `set_index` creates a copy"
    ],
    "answer": "`set_index` moves a column into the index; `reindex` conforms the DataFrame to a new set of labels (filling or dropping data as needed)",
    "explanation": "`set_index` is used to designate an existing column as the index. `reindex` is used to align the data to a completely new index (potentially adding rows of NaNs for new labels or dropping rows for missing ones).",
    "difficulty": "Intermediate"
  },
  {
    "id": 61,
    "question": "What is the result of using the `errors='ignore'` parameter in `pd.to_numeric`?",
    "options": [
      "Invalid parsing will be set as NaN",
      "Invalid parsing will raise a ValueError",
      "Invalid parsing will return the input column unchanged",
      "Invalid parsing will be converted to 0"
    ],
    "answer": "Invalid parsing will return the input column unchanged",
    "explanation": "In `pd.to_numeric`, `errors='coerce'` turns invalid parsing into NaN. `errors='raise'` throws an error. `errors='ignore'` suppresses the error and returns the original input column (object dtype) if it cannot be converted.",
    "difficulty": "Intermediate"
  },
  {
    "id": 62,
    "question": "Which `read_csv` parameter is best used to optimize memory when loading a large file that only needs specific columns?",
    "options": [
      "`usecols`",
      "`dtype`",
      "`nrows`",
      "`memory_map`"
    ],
    "answer": "`usecols`",
    "explanation": "While `dtype` optimizes the type of loaded data, `usecols` prevents pandas from loading unwanted columns entirely. This is the most effective first step for memory optimization when only a subset of a wide file is needed.",
    "difficulty": "Intermediate"
  },
  {
    "id": 63,
    "question": "What does the `argsort()` function return when applied to a pandas Series?",
    "options": [
      "The sorted values of the Series",
      "The indices that would sort the Series",
      "The cumulative sum of the sorted Series",
      "The unique values of the sorted Series"
    ],
    "answer": "The indices that would sort the Series",
    "explanation": "`argsort` returns the integer indices that would sort the array. This is useful for indirect sorting or ordering other arrays based on the sort order of the Series.",
    "difficulty": "Intermediate"
  },
  {
    "id": 64,
    "question": "What is the default `axis` argument value for the `apply` method on a DataFrame?",
    "options": [
      "1 (operate on columns)",
      "0 (operate on rows/index)",
      "'columns' (operate on columns)",
      "'index' (operate on index)"
    ],
    "answer": "0 (operate on rows/index)",
    "explanation": "By default, `axis=0` means the function is applied column-wise (down the rows). The function receives the column (a Series) as its argument. Setting `axis=1` applies the function row-wise (across columns).",
    "difficulty": "Intermediate"
  },
  {
    "id": 65,
    "question": "How does the `duplicated()` method determine the first instance of a duplicate?",
    "options": [
      "It selects the first occurrence based on the index order (keeping='first')",
      "It selects the first occurrence based on the column values (ignoring index)",
      "It always selects the occurrence with the lowest memory address",
      "It randomly selects one of the duplicates to mark as the original"
    ],
    "answer": "It selects the first occurrence based on the index order (keeping='first')",
    "explanation": "By default (`keep='first'`), `duplicated()` marks all occurrences *except* the first one as True. 'First' is defined by the order of rows in the DataFrame (i.e., the index order).",
    "difficulty": "Intermediate"
  },
  {
    "id": 66,
    "question": "What is the specific functionality of the `xs` method in pandas?",
    "options": [
      "To cross-section a MultiIndex DataFrame by selecting data at a specific level of the index",
      "To exclude specific columns from a DataFrame",
      "To execute an SQL-style cross-join with another DataFrame",
      "To calculate the cross-product of two Series"
    ],
    "answer": "To cross-section a MultiIndex DataFrame by selecting data at a specific level of the index",
    "explanation": "`xs` (cross-section) provides a shortcut to slice data at a specific level of a MultiIndex. It can select data at a specific key for a specific level without needing the full tuple syntax of `loc`.",
    "difficulty": "Intermediate"
  },
  {
    "id": 67,
    "question": "In the context of string manipulation, what does the `expand` parameter do in `df['col'].str.split(..., expand=True)`?",
    "options": [
      "It expands the string to uppercase",
      "It returns a DataFrame with new columns for each split element, instead of a Series of lists",
      "It splits the string into individual characters",
      "It duplicates the rows for each split value"
    ],
    "answer": "It returns a DataFrame with new columns for each split element, instead of a Series of lists",
    "explanation": "With `expand=False`, `str.split` returns a Series containing lists of strings. With `expand=True`, it returns a DataFrame where each split element occupies its own column.",
    "difficulty": "Intermediate"
  },
  {
    "id": 68,
    "question": "What is the difference between `sample(frac=0.5)` and `sample(n=5)`?",
    "options": [
      "`frac` samples a fixed number of rows, while `n` samples a fixed percentage",
      "`frac` samples a fixed percentage of rows, while `n` samples a fixed count of rows",
      "`frac` ensures the sample is sorted, while `n` is random",
      "`frac` performs sampling without replacement, while `n` performs with replacement"
    ],
    "answer": "`frac` samples a fixed percentage of rows, while `n` samples a fixed count of rows",
    "explanation": "The `frac` argument specifies the fraction of the axis (e.g., 0.5 for 50%) to return. The `n` argument specifies the absolute number of items to return.",
    "difficulty": "Intermediate"
  },
  {
    "id": 69,
    "question": "What happens when you apply `tz_localize(None)` to a timezone-aware DatetimeIndex?",
    "options": [
      "It converts the time to UTC",
      "It removes the timezone information, converting the data to naive (timezone-unaware) without changing the time values",
      "It raises an error because you cannot localize to None",
      "It converts the time to the system's local timezone"
    ],
    "answer": "It removes the timezone information, converting the data to naive (timezone-unaware) without changing the time values",
    "explanation": "`tz_localize(None)` is the standard method to strip timezone data from a datetime object, effectively making it 'naive'. This differs from `tz_convert`, which converts the time to a different zone.",
    "difficulty": "Intermediate"
  },
  {
    "id": 70,
    "question": "How does the `count` aggregation in `groupby` treat NaN values?",
    "options": [
      "It counts NaN values as 0",
      "It counts NaN values as 1",
      "It excludes NaN values from the count",
      "It raises an error if NaN values are present"
    ],
    "answer": "It excludes NaN values from the count",
    "explanation": "In pandas, the `count` method returns the number of non-NaN observations. This is distinct from `size`, which returns the total count of rows including NaNs.",
    "difficulty": "Intermediate"
  },
  {
    "id": 71,
    "question": "What is the primary computational advantage of using `pd.eval()` over standard Python arithmetic operations for large DataFrames?",
    "options": [
      "It automatically parallelizes operations across all available CPU cores",
      "It uses memory-mapped files to avoid loading data into RAM",
      "It reduces memory usage and speeds up computation by avoiding intermediate temporaries using the `numexpr` engine",
      "It compiles the operations into C extensions dynamically at runtime"
    ],
    "answer": "It reduces memory usage and speeds up computation by avoiding intermediate temporaries using the `numexpr` engine",
    "explanation": "`pd.eval()` leverages the `numexpr` library to evaluate expressions in a vectorized manner, bypassing the overhead of Python object alignment and temporary array allocation inherent in standard chaining.",
    "difficulty": "Advanced"
  },
  {
    "id": 72,
    "question": "In the context of the pandas GroupBy mechanism, what distinguishes the `transform` method from the `apply` method?",
    "options": [
      "`transform` returns a scalar value for each group, while `apply` returns a Series or DataFrame",
      "`transform` must return a result that is either the same size as the input group or broadcastable, while `apply` can return arbitrary shapes",
      "`apply` is optimized for numerical aggregations, while `transform` is strictly for string manipulation",
      "`transform` supports parallel execution via `n_jobs`, whereas `apply` is strictly single-threaded"
    ],
    "answer": "`transform` must return a result that is either the same size as the input group or broadcastable, while `apply` can return arbitrary shapes",
    "explanation": "The constraint on `transform` is that it returns a DataFrame having the same index as the original object, filled with the transformed values, making it ideal for adding aggregated columns back to the original dataframe.",
    "difficulty": "Advanced"
  },
  {
    "id": 73,
    "question": "When using `pd.merge(how='outer')`, what is the behavior regarding keys that exist in only one of the two DataFrames?",
    "options": [
      "Those rows are dropped entirely to ensure data purity",
      "The missing keys are filled with the string 'missing' for categorical columns",
      "The join is performed, and columns from the DataFrame lacking the key are filled with NaN",
      "An exception is raised requiring the user to specify a `fill_value` parameter"
    ],
    "answer": "The join is performed, and columns from the DataFrame lacking the key are filled with NaN",
    "explanation": "An outer merge preserves all keys from both the left and right DataFrames, aligning data where matches exist and inserting NaN (or the specified fill value) where matches do not.",
    "difficulty": "Advanced"
  },
  {
    "id": 74,
    "question": "What specific optimization strategy does pandas utilize when sorting a DataFrame containing a MultiIndex compared to a flat index?",
    "options": [
      "It flattens the index into a tuple and sorts using a radix sort",
      "It sorts by the first level only, ignoring subsequent levels by default",
      "It uses the `lexsort` algorithm (lexicographical sort), sorting by the lower levels first and then moving up to higher levels",
      "It converts the MultiIndex to strings and performs a standard quicksort"
    ],
    "answer": "It uses the `lexsort` algorithm (lexicographical sort), sorting by the lower levels first and then moving up to higher levels",
    "explanation": "Lexicographical sorting is efficient for multi-level indices because it sorts the data sequentially, starting with the last level and moving to the first (or vice versa depending on implementation specifics), effectively nesting the sorts.",
    "difficulty": "Advanced"
  },
  {
    "id": 75,
    "question": "How does the `memory_usage(deep=True)` method differ from the default `memory_usage()` for a DataFrame containing `object` (string) dtypes?",
    "options": [
      "`deep=True` includes the memory usage of the Index, while the default excludes it",
      "`deep=True` calculates the total memory consumption of the actual string data, whereas the default only accounts for the pointer size of the Python object wrappers",
      "`deep=True` returns the memory in bits instead of bytes",
      "`deep=True` aggregates the memory of the parent process, while the default is limited to the DataFrame object"
    ],
    "answer": "`deep=True` calculates the total memory consumption of the actual string data, whereas the default only accounts for the pointer size of the Python object wrappers",
    "explanation": "Object dtypes in pandas are pointers; the default memory usage reflects the size of the pointer array. The `deep=True` parameter introspects the actual objects (like strings) to calculate their true memory footprint.",
    "difficulty": "Advanced"
  },
  {
    "id": 76,
    "question": "What is the technical result of passing `as_index=False` to a `groupby` aggregation operation?",
    "options": [
      "The grouped keys are moved into the columns of the resulting DataFrame rather than forming the Index",
      "The aggregation ignores the group keys and returns a flat Series",
      "The index of the result is reset to a default RangeIndex, but the group keys are discarded",
      "It creates a MultiIndex on the columns for hierarchical aggregation"
    ],
    "answer": "The grouped keys are moved into the columns of the resulting DataFrame rather than forming the Index",
    "explanation": "By default, `groupby` places the unique group keys in the Index. Setting `as_index=False` overrides this, treating the group keys as standard columns in the output DataFrame, often resulting in a 'tidy' format.",
    "difficulty": "Advanced"
  },
  {
    "id": 77,
    "question": "Which parameter in `pd.read_csv` controls the parsing of dates in the index column specifically for optimizing time-series operations?",
    "options": [
      "`parse_dates`",
      "`index_col` combined with `date_parser`",
      "`parse_dates` with a list containing the index position",
      "`infer_datetime_format`"
    ],
    "answer": "`parse_dates` with a list containing the index position",
    "explanation": "To convert an index column to DateTime objects during import, you pass the column position to `index_col` *and* include that position in the list passed to `parse_dates`, instructing pandas to parse that specific column as dates.",
    "difficulty": "Advanced"
  },
  {
    "id": 78,
    "question": "In the context of method chaining, what is the function of the `.pipe()` method?",
    "options": [
      "It applies a function to every element (row or column) of the DataFrame",
      "It injects a user-defined function or callable into the method chain, passing the entire DataFrame as an argument",
      "It connects two DataFrames end-to-end similar to SQL's `UNION`",
      "It creates a lazy evaluation pipeline that is not executed until `compute()` is called"
    ],
    "answer": "It injects a user-defined function or callable into the method chain, passing the entire DataFrame as an argument",
    "explanation": "While `.apply()` works row/column-wise, `.pipe()` allows for custom functions that take the DataFrame (or Series) as the first argument, enabling cleaner integration of custom logic within a fluent programming style.",
    "difficulty": "Advanced"
  },
  {
    "id": 79,
    "question": "What is the effect of setting `dtype='category'` for a column containing low-cardinality string data?",
    "options": [
      "It enables native regex searching on the column",
      "It converts the strings to a numerical representation with a lookup table, reducing memory usage and potentially speeding up groupby operations",
      "It automatically sorts the column alphabetically",
      "It forces the column to be immutable, preventing any modifications to existing values"
    ],
    "answer": "It converts the strings to a numerical representation with a lookup table, reducing memory usage and potentially speeding up groupby operations",
    "explanation": "Categorical data stores data as integer codes (categories) rather than raw strings. This significantly reduces memory footprint and can accelerate operations like grouping because the underlying comparisons are performed on integers.",
    "difficulty": "Advanced"
  },
  {
    "id": 80,
    "question": "When dealing with missing data in time series, what is the specific difference between `df.interpolate(method='time')` and `df.interpolate(method='linear')`?",
    "options": [
      "`method='time'` interpolates based on the index values (time delta), while `method='linear'` ignores the index and treats data as equidistant",
      "`method='time'` uses forward filling, while `method='linear'` uses average filling",
      "`method='linear'` can only be used on numeric columns, while `method='time'` works on strings",
      "There is no difference; they are aliases for the same algorithm"
    ],
    "answer": "`method='time'` interpolates based on the index values (time delta), while `method='linear'` ignores the index and treats data as equidistant",
    "explanation": "For time-series data with irregular intervals, `method='time'` calculates the interpolation based on the actual time distance between points, whereas standard `linear` interpolation assumes a uniform distance between data points (based on their position).",
    "difficulty": "Advanced"
  },
  {
    "id": 81,
    "question": "What does the `SettingWithCopyWarning` specifically indicate about an operation on a DataFrame?",
    "options": [
      "The DataFrame is locked and cannot be edited",
      "The operation was attempted on a view of a DataFrame returned by a chained indexing assignment, and the outcome is ambiguous",
      "The user is trying to set a value with the wrong data type",
      "The DataFrame is running out of memory and cannot allocate the copy"
    ],
    "answer": "The operation was attempted on a view of a DataFrame returned by a chained indexing assignment, and the outcome is ambiguous",
    "explanation": "This warning alerts the user that they might be modifying a view (a reference to a slice of another DataFrame) unintentionally, or that pandas failed to set the value back to the original data due to chained assignment like `df[df['a'] > 0]['b'] = 0`.",
    "difficulty": "Advanced"
  },
  {
    "id": 82,
    "question": "Which `pandas` data structure is optimized for storing one-dimensional data with labeled axes and can handle heterogeneous types?",
    "options": [
      "DataFrame",
      "Panel",
      "Series",
      "ndarray"
    ],
    "answer": "Series",
    "explanation": "A Series is a 1-D labeled array capable of holding any data type. While DataFrames are 2D, the Series is the fundamental building block optimized for single-column operations with index alignment.",
    "difficulty": "Advanced"
  },
  {
    "id": 83,
    "question": "What is the result of using the `pd.cut()` function with the argument `duplicates='drop'` when bin edges have duplicate values?",
    "options": [
      "It raises a `ValueError` because bins must be unique",
      "It returns only the unique bins, reducing the number of output categories",
      "It merges the duplicate edges into a single bin edge",
      "It treats duplicate bins as continuous intervals and expands them"
    ],
    "answer": "It returns only the unique bins, reducing the number of output categories",
    "explanation": "When bin edges contain duplicates, `duplicates='drop'` ensures the function proceeds by non-uniquing the bin edges, which results in fewer categories than initially specified based on the number of edges.",
    "difficulty": "Advanced"
  },
  {
    "id": 84,
    "question": "How does the `stack()` method transform a DataFrame?",
    "options": [
      "It converts the columns into a new index level, creating a Series with a MultiIndex",
      "It applies a vertical concatenation of the rows",
      "It rotates the data from columns to rows while converting the index to columns",
      "It compresses the index levels into a single column"
    ],
    "answer": "It converts the columns into a new index level, creating a Series with a MultiIndex",
    "explanation": "The `stack()` method 'pivots' the columns of a DataFrame (potentially with a MultiIndex on columns) down into the Index, effectively converting a wide table into a long (tall) Series or DataFrame.",
    "difficulty": "Advanced"
  },
  {
    "id": 85,
    "question": "When performing a `merge` operation on two DataFrames with non-unique keys in both, what is the default behavior (`how='inner'`)?",
    "options": [
      "It performs a many-to-many join, creating the Cartesian product of rows with matching keys",
      "It performs a many-to-one join, keeping only the first match from the right DataFrame",
      "It raises a `MergeError` requiring the user to specify a suffix for overlapping columns",
      "It performs a one-to-many join based on the order of the DataFrames passed"
    ],
    "answer": "It performs a many-to-many join, creating the Cartesian product of rows with matching keys",
    "explanation": "If keys are not unique in both DataFrames, an inner join (or any join) defaults to a many-to-many relationship. For every match on the key, pandas combines every row from the left with every row from the right.",
    "difficulty": "Advanced"
  },
  {
    "id": 86,
    "question": "What is the primary purpose of the `pd.get_dummies()` function in the context of machine learning preprocessing?",
    "options": [
      "To automatically fill missing values with the mean of the column",
      "To convert categorical variables into a series of binary (0/1) columns",
      "To detect and remove outliers using Z-scores",
      "To normalize numerical data to a range between 0 and 1"
    ],
    "answer": "To convert categorical variables into a series of binary (0/1) columns",
    "explanation": "This function performs one-hot encoding, transforming a single column of categorical labels into k new columns of binary flags (where k is the number of unique categories), making the data compatible with algorithms requiring numeric input.",
    "difficulty": "Advanced"
  },
  {
    "id": 87,
    "question": "In pandas Extension Arrays, what is the characteristic of the `string[pyarrow]` dtype compared to the standard `object` dtype?",
    "options": [
      "It is significantly faster for vectorized string operations and uses less memory due to the Apache Arrow backend",
      "It allows for mutable strings in place",
      "It automatically strips whitespace from all entries upon creation",
      "It prevents the use of NA values, forcing the use of empty strings"
    ],
    "answer": "It is significantly faster for vectorized string operations and uses less memory due to the Apache Arrow backend",
    "explanation": "Leveraging PyArrow for string types bypasses Python's overhead for object access, provides a contiguous memory buffer, and enables zero-copy operations, leading to better performance and lower memory usage.",
    "difficulty": "Advanced"
  },
  {
    "id": 88,
    "question": "What does the `values` attribute return for a pandas Series or DataFrame?",
    "options": [
      "A list of Python objects",
      "A NumPy ndarray representing the data (without the index/columns)",
      "A dictionary of the data",
      "A deep copy of the data"
    ],
    "answer": "A NumPy ndarray representing the data (without the index/columns)",
    "explanation": "The `.values` attribute provides the underlying homogeneous data as a NumPy array. Note that in modern pandas, it is recommended to use `.to_numpy()` or `.array` to be explicit about the returned type, but `.values` remains the historical legacy accessor.",
    "difficulty": "Advanced"
  },
  {
    "id": 89,
    "question": "When using `pd.concat([df1, df2])` with DataFrames that have non-overlapping columns, what is the default value of the `join` parameter?",
    "options": [
      "`inner` (intersection of columns)",
      "`outer` (union of columns)",
      "`cross` (Cartesian product)",
      "`left`"
    ],
    "answer": "`outer` (union of columns)",
    "explanation": "By default, `pd.concat` performs a set union (outer join) on the axes (columns for axis=0, rows for axis=1), introducing NaN for missing values in the columns that do not exist in one of the inputs.",
    "difficulty": "Advanced"
  },
  {
    "id": 90,
    "question": "What is the difference between the `rank()` method with `method='first'` versus `method='average'`?",
    "options": [
      "`method='first'` assigns random ranks to ties, while `method='average'` calculates the mean",
      "`method='first'` ranks items in the order they appear in the data (no averaging), while `method='average'` assigns the mean rank to tied values",
      "`method='first'` is used for dates only, while `method='average'` is for numbers",
      "`method='average'` is the default and faster, while `method='first'` requires sorting"
    ],
    "answer": "`method='first'` ranks items in the order they appear in the data (no averaging), while `method='average'` assigns the mean rank to tied values",
    "explanation": "When values are tied, `method='first'` breaks the tie based on the observation order (e.g., ranks 1, 2, 3), whereas `method='average'` assigns the average of the ranks to all tied entries (e.g., 1, 2, 3 becomes 2, 2, 2).",
    "difficulty": "Advanced"
  },
  {
    "id": 91,
    "question": "What is the function of the `isin()` method in pandas?",
    "options": [
      "It checks if the Index of the DataFrame is unique",
      "It filters the DataFrame by checking if elements are contained in a provided sequence (like a list or set)",
      "It tests if a substring exists within a string column",
      "It verifies if the DataFrame is empty"
    ],
    "answer": "It filters the DataFrame by checking if elements are contained in a provided sequence (like a list or set)",
    "explanation": "`isin()` returns a boolean Series indicating whether each element in the Series/column is exactly present in the passed sequence, allowing efficient filtering similar to SQL's `IN` clause.",
    "difficulty": "Advanced"
  },
  {
    "id": 92,
    "question": "Which method is preferred to find the percentage change between the current and a prior element in a Series?",
    "options": [
      "`pct_change()`",
      "`diff()` divided by `shift()`",
      "`rank(pct=True)`",
      "`apply(lambda x: (x - x.shift(1)) / x.shift(1))`"
    ],
    "answer": "`pct_change()`",
    "explanation": "`pct_change()` is a built-in optimized method that calculates the fractional change (current - prior) / prior. While it can be replicated with `shift` and arithmetic, the dedicated method is cleaner and handles periods and fill data natively.",
    "difficulty": "Advanced"
  },
  {
    "id": 93,
    "question": "What is the default behavior of `df.resample('ME')` when applied to a time-series DataFrame?",
    "options": [
      "It upsamples the data to the minute frequency",
      "It downsamples the data to Month-End frequency",
      "It interpolates missing values for every month",
      "It shifts the index to the start of the month"
    ],
    "answer": "It downsamples the data to Month-End frequency",
    "explanation": "Resampling is a convenience method for frequency conversion. In this case, 'ME' groups the data by the end of the month (aggregating the time series into buckets representing the month's end), requiring an aggregation function like `.sum()` or `.mean()` to finalize the result.",
    "difficulty": "Advanced"
  },
  {
    "id": 94,
    "question": "How does the `query()` method handle variables defined in the local environment?",
    "options": [
      "It cannot access them; all references must be columns in the DataFrame",
      "It can reference local variables by prefixing them with the `@` character",
      "It automatically imports all globals",
      "Variables must be passed as a dictionary via the `env` argument"
    ],
    "answer": "It can reference local variables by prefixing them with the `@` character",
    "explanation": "The `query()` method uses the `numexpr` parser but allows referencing external Python variables using the `@` symbol (e.g., `df.query('A > @threshold')`), distinguishing them from DataFrame column names.",
    "difficulty": "Advanced"
  },
  {
    "id": 95,
    "question": "When using `pd.to_datetime`, what is the function of the `format` argument?",
    "options": [
      "It specifies the output style (e.g., ISO 8601 vs. US)",
      "It defines the specific strptime format string to speed up parsing if the format is known",
      "It converts the timezone to UTC",
      "It validates the data after parsing"
    ],
    "answer": "It defines the specific strptime format string to speed up parsing if the format is known",
    "explanation": "If the format string is provided, pandas does not need to infer the date format, which significantly speeds up the parsing of large datasets compared to the default `infer_datetime_format=True` (which is deprecated/implicit) behavior.",
    "difficulty": "Advanced"
  },
  {
    "id": 96,
    "question": "What is the effect of using `df.pivot(index='date', columns='item', values='price')` compared to `df.pivot_table(...)`?",
    "options": [
      "`pivot` is just an alias for `pivot_table`",
      "`pivot` assumes there are no duplicate combinations of index/columns and throws an error if there are, whereas `pivot_table` aggregates duplicates",
      "`pivot_table` is deprecated in favor of `pivot`",
      "`pivot` automatically sorts the index, while `pivot_table` does not"
    ],
    "answer": "`pivot` assumes there are no duplicate combinations of index/columns and throws an error if there are, whereas `pivot_table` aggregates duplicates",
    "explanation": "The simple `pivot` method is purely a reshape operation and fails if the data is not 'unique' on the index/columns intersection. `pivot_table` solves this by accepting an `aggfunc` (like 'mean' or 'sum') to handle duplicates.",
    "difficulty": "Advanced"
  },
  {
    "id": 97,
    "question": "In the context of the `.where()` method in pandas, how are values modified?",
    "options": [
      "It filters the DataFrame to only show rows where the condition is True",
      "It keeps values where the condition is True and replaces values where the condition is False with NaN (or a specified other value)",
      "It replaces values where the condition is True with NaN",
      "It deletes rows where the condition is False"
    ],
    "answer": "It keeps values where the condition is True and replaces values where the condition is False with NaN (or a specified other value)",
    "explanation": "Unlike boolean indexing which selects/drops data, `.where()` preserves the shape of the DataFrame. It acts as an 'if-then' construct: if True, keep; if False, replace with `other` (default is NaN).",
    "difficulty": "Advanced"
  },
  {
    "id": 98,
    "question": "Which pandas function is used to apply a rolling window calculation weighted by a Gaussian distribution?",
    "options": [
      "`df.rolling().apply()` with a custom function",
      "`df.expanding()`",
      "`df.ewm()`",
      "`df.rolling(window='gaussian')`"
    ],
    "answer": "`df.ewm()`",
    "explanation": "`ewm()` (Exponential Weighted Functions) is the standard interface for applying weighted windows. While standard rolling windows allow custom weights, `ewm` is specifically optimized for exponential and Gaussian-like decay weights.",
    "difficulty": "Advanced"
  },
  {
    "id": 99,
    "question": "What is the behavior of `df.drop_duplicates()` with the argument `keep='last'`?",
    "options": [
      "It keeps the first occurrence and deletes the rest",
      "It keeps the last occurrence and deletes the rest",
      "It keeps all duplicates but removes the unique values",
      "It raises an error because `keep` only accepts 'first' or False"
    ],
    "answer": "It keeps the last occurrence and deletes the rest",
    "explanation": "The `keep` parameter determines which duplicates to retain. `keep='last'` retains the final instance of a duplicated set and discards earlier ones, whereas the default (`keep='first'`) retains the first.",
    "difficulty": "Advanced"
  },
  {
    "id": 100,
    "question": "What is the `NA` scalar introduced in pandas 1.0 (utilized by `pd.NA`) designed to address?",
    "options": [
      "To provide a faster integer type for indexing",
      "To offer a missing value indicator that is consistent across different data types (integer, boolean, string) without forcing float conversion",
      "To replace the `None` keyword in Python syntax",
      "To act as a placeholder for categorical data only"
    ],
    "answer": "To offer a missing value indicator that is consistent across different data types (integer, boolean, string) without forcing float conversion",
    "explanation": "Historically, pandas used `NaN` (float) for missing data, forcing integers to float. `pd.NA` is a 'missing' singleton that allows Int64, boolean, and string dtypes to hold missing values without changing the fundamental data type of the array.",
    "difficulty": "Advanced"
  }
]