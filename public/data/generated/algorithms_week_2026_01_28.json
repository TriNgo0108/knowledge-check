[
  {
    "id": 1,
    "question": "According to the text, what is a primary benefit of studying Data Structures and Algorithms (DSA)?",
    "options": [
      "Learning to memorize code snippets",
      "Improving logical thinking and optimization skills",
      "Learning how to design user interfaces",
      "Understanding hardware circuit design"
    ],
    "answer": "Improving logical thinking and optimization skills",
    "explanation": "The text states that DSA trains you to think logically, optimize solutions, and solve real-world problems efficiently. It emphasizes that DSA is about building logical thinking and confidence, not just memorizing solutions.",
    "difficulty": "Beginner"
  },
  {
    "id": 2,
    "question": "What is synthetic data?",
    "options": [
      "Data collected directly from users via surveys",
      "Data that is encrypted for security purposes",
      "Highly realistic but entirely fake data generated by AI",
      "Historical data stored in legacy systems"
    ],
    "answer": "Highly realistic but entirely fake data generated by AI",
    "explanation": "Synthetic data is defined in the text as highly realistic but entirely fake data generated by AI algorithms. It is used to simulate the world closely without the privacy risks or costs associated with real-world data.",
    "difficulty": "Beginner"
  },
  {
    "id": 3,
    "question": "What is the main purpose of AI data observability?",
    "options": [
      "To manually review all data entries for errors",
      "To use machine learning to detect and resolve anomalies automatically",
      "To replace all human data engineers with bots",
      "To store data in a single monolithic location"
    ],
    "answer": "To use machine learning to detect and resolve anomalies automatically",
    "explanation": "AI data observability is described as a preventive method that uses machine learning algorithms to detect, diagnose, and resolve anomalies as they happen, shifting from reactive problem-solving to predictive prevention.",
    "difficulty": "Beginner"
  },
  {
    "id": 4,
    "question": "In a data mesh architecture, how is data ownership handled?",
    "options": [
      "By a single central IT department only",
      "By individual business domains like finance or marketing",
      "By external cloud providers exclusively",
      "By storing all data in one large legacy lake"
    ],
    "answer": "By individual business domains like finance or marketing",
    "explanation": "The text explains that data mesh distributes ownership and responsibility across business domains, meaning individual departments like finance or marketing take ownership of their data as products.",
    "difficulty": "Beginner"
  },
  {
    "id": 5,
    "question": "The text mentions that advanced data structures extend beyond basic structures. Which of the following is listed as a basic structure?",
    "options": [
      "Segment Tree",
      "Trie",
      "Array",
      "Fenwick Tree"
    ],
    "answer": "Array",
    "explanation": "The text states that the domain of advanced data structures extends beyond basic arrays or linked lists, focusing on specialized structures. Therefore, an array is considered a basic structure in this context.",
    "difficulty": "Beginner"
  },
  {
    "id": 6,
    "question": "In the context of bitmasking and Dynamic Programming (DP), what allows for solving complex problems like the Traveling Salesperson Problem?",
    "options": [
      "Using only recursive functions without storage",
      "Representing 'state' using bitmasks combined with DP",
      "Sorting the data alphabetically",
      "Converting all data to string format"
    ],
    "answer": "Representing 'state' using bitmasks combined with DP",
    "explanation": "The text notes that pairing bitmasks (to represent small groups of data) with Dynamic Programming allows you to solve massive puzzles by forcing you to think differently about how you store a 'state.'",
    "difficulty": "Beginner"
  },
  {
    "id": 7,
    "question": "What is the primary goal of 'Binary Search' compared to simple searching?",
    "options": [
      "To search for data in an unsorted list sequentially",
      "To find a specific target efficiently in a sorted range",
      "To sort data into a binary tree format",
      "To convert decimal numbers to binary code"
    ],
    "answer": "To find a specific target efficiently in a sorted range",
    "explanation": "The text discusses 'Binary Search on Answer' and Ternary Search as staples for finding the 'sweet spot' or a target efficiently within a range, which is the core concept of binary search algorithms.",
    "difficulty": "Beginner"
  },
  {
    "id": 8,
    "question": "How will GenAI assist data engineers in 2026 according to the text?",
    "options": [
      "By writing all code from scratch without any human input",
      "By handling repetitive tasks like cleaning data and managing ETL workflows",
      "By replacing the need for databases entirely",
      "By eliminating the need for data security measures"
    ],
    "answer": "By handling repetitive tasks like cleaning data and managing ETL workflows",
    "explanation": "The text predicts that GenAI will handle time-consuming and repetitive steps like cleaning, formatting data, and managing ETL workflows autonomously, allowing engineers to describe pipelines in natural language.",
    "difficulty": "Beginner"
  },
  {
    "id": 9,
    "question": "What is a requirement for data to be 'agent-ready'?",
    "options": [
      "It must be locked behind legacy architecture",
      "It must be inaccessible to third-party services",
      "It must be accessible and in a format that AI agents can handle",
      "It must be stored only on local physical hard drives"
    ],
    "answer": "It must be accessible and in a format that AI agents can handle",
    "explanation": "The text states that for agents to be effective in 2026, data will have to be accessible and in a format they can handle, rather than siloed away or locked behind legacy architecture.",
    "difficulty": "Beginner"
  },
  {
    "id": 10,
    "question": "The text implies that data should not be 'siloed.' What does siloed data refer to?",
    "options": [
      "Data stored in secure cloud systems",
      "Data that is isolated or locked away from other parts of the organization",
      "Data that has been cleaned and verified by AI",
      "Data that is open and available to the public"
    ],
    "answer": "Data that is isolated or locked away from other parts of the organization",
    "explanation": "In the context of 'agent-ready' data, the text mentions that data should not be 'siloed away,' meaning it should not be isolated or inaccessible within separate systems.",
    "difficulty": "Beginner"
  },
  {
    "id": 11,
    "question": "Why do elite tech firms care deeply about the tools and algorithms developers choose?",
    "options": [
      "To ensure the code looks long and complex",
      "Because tool choice impacts speed and efficiency in solving problems",
      "To ensure the code uses the maximum amount of memory possible",
      "Because they only want to use old, legacy tools"
    ],
    "answer": "Because tool choice impacts speed and efficiency in solving problems",
    "explanation": "The text explains that working with advanced algorithms is about speed and efficiency, and elite firms care about tool choice because it determines how effectively impossible-seeming hurdles can be solved.",
    "difficulty": "Beginner"
  },
  {
    "id": 12,
    "question": "Mastering advanced algorithms helps reduce what aspect of software execution?",
    "options": [
      "Time complexity",
      "Number of variable names",
      "User interface colors",
      "Network cable length"
    ],
    "answer": "Time complexity",
    "explanation": "The text explicitly states that mastering advanced concepts allows you to 'significantly reduce the time complexity of software,' making programs run faster.",
    "difficulty": "Beginner"
  },
  {
    "id": 13,
    "question": "Besides reducing time complexity, what else does mastering advanced data structures help optimize?",
    "options": [
      "Memory usage",
      "Screen resolution",
      "Keyboard typing speed",
      "Internet bandwidth usage"
    ],
    "answer": "Memory usage",
    "explanation": "The text mentions that mastering these concepts allows you to 'optimize memory usage' as well as reduce time complexity, ensuring resources are used efficiently.",
    "difficulty": "Beginner"
  },
  {
    "id": 14,
    "question": "'Ternary Search' is mentioned as a staple in competitive programming. What type of range is it typically used to find?",
    "options": [
      "A range where values go up and then down (a peak or valley)",
      "A range where values are always constant",
      "A range where values decrease infinitely",
      "A range containing only alphabetical characters"
    ],
    "answer": "A range where values go up and then down (a peak or valley)",
    "explanation": "The text describes Ternary Search as a method for finding the 'sweet spot' in a range where values go up and then down, distinct from binary search which finds a specific target.",
    "difficulty": "Beginner"
  },
  {
    "id": 15,
    "question": "According to the text, what is a common myth about Data Structures and Algorithms (DSA)?",
    "options": [
      "That it is useful for real-world problem solving",
      "That it is only useful for job interviews",
      "That it helps build patience",
      "That it can be difficult to learn"
    ],
    "answer": "That it is only useful for job interviews",
    "explanation": "The text explicitly states: 'Many students think DSA is only for interviews — that’s a myth.' The reality is that it trains logical thinking and problem-solving skills applicable to real-world scenarios.",
    "difficulty": "Beginner"
  },
  {
    "id": 16,
    "question": "Why is synthetic data considered a solution for privacy issues in industries like healthcare or finance?",
    "options": [
      "Because it deletes real user data permanently",
      "Because it is realistic but fake, so it doesn't expose real personal information",
      "Because it is stored in a secure physical vault",
      "Because it is encrypted twice for safety"
    ],
    "answer": "Because it is realistic but fake, so it doesn't expose real personal information",
    "explanation": "The text highlights that real-world data can be a privacy nightmare, but synthetic data is 'entirely fake,' allowing companies to work with realistic data without compromising actual personal information.",
    "difficulty": "Beginner"
  },
  {
    "id": 17,
    "question": "AI data observability is described as a shift from reactive problem-solving to what?",
    "options": [
      "Predictive prevention",
      "Ignoring the problems entirely",
      "Manual troubleshooting",
      "Random guessing"
    ],
    "answer": "Predictive prevention",
    "explanation": "The text states that AI observability represents a major shift 'from reactive problem-solving to predictive prevention,' using ML to stop issues before they become critical.",
    "difficulty": "Beginner"
  },
  {
    "id": 18,
    "question": "The text contrasts monolithic data lakes with a new approach. What is a defining characteristic of a monolithic data lake?",
    "options": [
      "Distributed ownership across business domains",
      "Single, large, centralized storage",
      "Data generated exclusively by AI agents",
      "Data that is highly accessible to all users"
    ],
    "answer": "Single, large, centralized storage",
    "explanation": "The text describes a shift away from 'single, monolithic data lakes' toward decentralized architectures, implying that monolithic lakes are centralized and singular in structure.",
    "difficulty": "Beginner"
  },
  {
    "id": 19,
    "question": "According to the 2026 trends, how will data engineers implement pipelines?",
    "options": [
      "By writing complex Assembly code",
      "By describing what they want in natural language",
      "By purchasing expensive hardware",
      "By hiring more manual data entry clerks"
    ],
    "answer": "By describing what they want in natural language",
    "explanation": "The text predicts that data engineers will implement pipelines 'simply by describing what they want to happen in natural language,' thanks to GenAI handling the technical execution.",
    "difficulty": "Beginner"
  },
  {
    "id": 20,
    "question": "Which data structure is mentioned in the text as being used for 'intense string searches'?",
    "options": [
      "Suffix Tree",
      "Fenwick Tree",
      "Binary Search Tree",
      "Hash Table"
    ],
    "answer": "Suffix Tree",
    "explanation": "The text specifically mentions the Suffix Tree as a tool that elite tech firms care about when performing intense string searches.",
    "difficulty": "Beginner"
  },
  {
    "id": 21,
    "question": "Which data structure is recommended for 'tricky range queries'?",
    "options": [
      "Suffix Tree",
      "Fenwick Tree",
      "Stack",
      "Queue"
    ],
    "answer": "Fenwick Tree",
    "explanation": "The text lists the Fenwick Tree (also known as a Binary Indexed Tree) as a structure used for handling tricky range queries.",
    "difficulty": "Beginner"
  },
  {
    "id": 22,
    "question": "Besides logic, what character trait does the text say is built by solving difficult DSA problems?",
    "options": [
      "Aggressiveness",
      "Haste",
      "Patience",
      "Indifference"
    ],
    "answer": "Patience",
    "explanation": "The text states that if DSA feels difficult, 'you're doing it right,' and that every problem solved builds patience, logical thinking, and confidence.",
    "difficulty": "Beginner"
  },
  {
    "id": 23,
    "question": "In the data mesh model, departments treat their data as what?",
    "options": [
      "A liability",
      "A product",
      "A secret",
      "An expense"
    ],
    "answer": "A product",
    "explanation": "The text explains that in a data mesh, individual departments 'take ownership of their data as products,' managing it as a valuable asset.",
    "difficulty": "Beginner"
  },
  {
    "id": 24,
    "question": "What allows users to subscribe to live reports and share data notebooks without overwhelming complexity?",
    "options": [
      "Strict access control lists",
      "Paper-based reports",
      "Collaborative platforms and dashboards",
      "Manual email requests"
    ],
    "answer": "Collaborative platforms and dashboards",
    "explanation": "The text mentions that collaborative platforms and dashboards allow users to explore information intuitively, subscribe to reports, and share notebooks without complexity.",
    "difficulty": "Beginner"
  },
  {
    "id": 25,
    "question": "Synthetic data is generated by AI algorithms trained to do what?",
    "options": [
      "To delete existing data",
      "To simulate and model the real world",
      "To hack into secure databases",
      "To write poetry and fiction"
    ],
    "answer": "To simulate and model the real world",
    "explanation": "The text defines synthetic data as being generated by AI algorithms 'trained to simulate and model the world as closely as possible.'",
    "difficulty": "Beginner"
  },
  {
    "id": 26,
    "question": "What is the reality regarding DSA and interviews as opposed to the myth?",
    "options": [
      "It is useless for interviews",
      "It trains you to think logically and solve problems efficiently",
      "It is only about memorizing syntax",
      "It is becoming obsolete in 2026"
    ],
    "answer": "It trains you to think logically and solve problems efficiently",
    "explanation": "Contrary to the myth that it is only for interviews, the reality presented is that DSA trains you to think logically, optimize solutions, and solve real-world problems efficiently.",
    "difficulty": "Beginner"
  },
  {
    "id": 27,
    "question": "Disjoint Set Unions are listed as an example of what type of structure?",
    "options": [
      "Basic data structures",
      "Specialized advanced data structures",
      "User interface components",
      "Network hardware"
    ],
    "answer": "Specialized advanced data structures",
    "explanation": "The text introduces Disjoint Set Unions as one of the 'specialized structures' that make up the framework of Advance Data Structure and Algorithms, distinct from basic arrays or linked lists.",
    "difficulty": "Beginner"
  },
  {
    "id": 28,
    "question": "What is a primary benefit of AI handling ETL workflows autonomously?",
    "options": [
      "It increases the cost of operations significantly",
      "It allows for faster insights and less friction",
      "It makes the data less secure",
      "It requires more manual effort from engineers"
    ],
    "answer": "It allows for faster insights and less friction",
    "explanation": "The text states that the result of GenAI handling ETL workflows autonomously will be 'faster insights and less friction between ideas and execution.'",
    "difficulty": "Beginner"
  },
  {
    "id": 29,
    "question": "In data quality control, what is an anomaly?",
    "options": [
      "A standard expected result",
      "A deviation or error in the data that needs attention",
      "A new software update",
      "A type of variable used in loops"
    ],
    "answer": "A deviation or error in the data that needs attention",
    "explanation": "In the context of the text, anomalies refer to data quality issues or deviations that AI data observability tools aim to detect, diagnose, and resolve.",
    "difficulty": "Beginner"
  },
  {
    "id": 30,
    "question": "Once a programmer pushes past the beginner phase, what are the only metrics that truly matter according to the text?",
    "options": [
      "Speed and efficiency",
      "Code length and variable names",
      "Color scheme and design",
      "Popularity and fame"
    ],
    "answer": "Speed and efficiency",
    "explanation": "The text states, 'When you push past the beginner phase, you enter a space where speed and efficiency are the only metrics that truly matter.'",
    "difficulty": "Beginner"
  },
  {
    "id": 31,
    "question": "What is predicted to be a major use of GenAI in businesses by 2026 regarding customer data?",
    "options": [
      "Stealing customer data",
      "Creating synthetic customer data",
      "Deleting customer data",
      "Printing customer data on paper"
    ],
    "answer": "Creating synthetic customer data",
    "explanation": "Gartner predicts that by 2026, businesses will use GenAI to create synthetic customer data, making it a top trend.",
    "difficulty": "Beginner"
  },
  {
    "id": 32,
    "question": "The title '2026 DSA: Clarity Trumps Complexity' suggests that modern hiring values what over complexity?",
    "options": [
      "Understanding and clear reasoning",
      "Writing the longest code possible",
      "Using obscure features",
      "Memorizing textbooks"
    ],
    "answer": "Understanding and clear reasoning",
    "explanation": "The title implies that in 2026, the focus is on clarity—understanding and reasoning—rather than unnecessarily complex solutions.",
    "difficulty": "Beginner"
  },
  {
    "id": 33,
    "question": "Bitmasking is a technique often used to represent what?",
    "options": [
      "Images on a screen",
      "Large video files",
      "Small groups of data or 'state'",
      "Network cables"
    ],
    "answer": "Small groups of data or 'state'",
    "explanation": "The text explains that bitmasking is used to 'represent small groups of data' and is paired with Dynamic Programming to manage state.",
    "difficulty": "Beginner"
  },
  {
    "id": 34,
    "question": "What is the ultimate goal of using advanced algorithms to solve problems?",
    "options": [
      "To increase the difficulty of the task",
      "To make solutions impossible to find",
      "To optimize performance and solve problems efficiently",
      "To create more data than necessary"
    ],
    "answer": "To optimize performance and solve problems efficiently",
    "explanation": "The overarching theme of the text is that advanced data structures and algorithms are used to handle complex tasks, optimize performance, and solve problems efficiently.",
    "difficulty": "Beginner"
  },
  {
    "id": 35,
    "question": "Which of the following best describes the purpose of 'Data Sovereignty' mentioned in the text headers?",
    "options": [
      "Allowing any country to access all data",
      "Implementing strategies around accessibility and privacy for data control",
      "Making all data public domain",
      "Deleting data after 24 hours"
    ],
    "answer": "Implementing strategies around accessibility and privacy for data control",
    "explanation": "While the body text for Data Sovereignty is brief, the context of 'Agent-Ready Data' mentions implementing strategies around accessibility and privacy to ensure security, which aligns with the concept of Data Sovereignty.",
    "difficulty": "Beginner"
  },
  {
    "id": 36,
    "question": "You are solving a problem where you need to find the maximum value in a range and update values at specific indices frequently. While a Segment Tree is a valid choice, which data structure is often preferred for its lower memory footprint and constant-factor speed advantages when only associative operations (like sum or xor) are needed?",
    "options": [
      "Disjoint Set Union",
      "Fenwick Tree (Binary Indexed Tree)",
      "Trie",
      "Adjacency List"
    ],
    "answer": "Fenwick Tree (Binary Indexed Tree)",
    "explanation": "Fenwick Trees (or Binary Indexed Trees) are often preferred over Segment Trees for range sum queries and point updates because they use less memory and are generally faster due to simpler bitwise operations. DSU is for connectivity, Tries are for strings, and Adjacency Lists are for graph representation.",
    "difficulty": "Intermediate"
  },
  {
    "id": 37,
    "question": "In the context of 2026 data trends, why is Synthetic Data becoming critical for training machine learning models in sectors like healthcare and finance?",
    "options": [
      "It is always more accurate than real-world data",
      "It allows training without compromising user privacy or relying on expensive real data collection",
      "It eliminates the need for data validation",
      "It automatically optimizes the algorithm's time complexity"
    ],
    "answer": "It allows training without compromising user privacy or relying on expensive real data collection",
    "explanation": "Synthetic data is artificially generated but realistic data that mimics real-world patterns. It is crucial in sensitive fields because it avoids privacy issues (no real personal data is used) and solves problems related to the scarcity or high cost of real data. It does not inherently improve accuracy or optimization.",
    "difficulty": "Intermediate"
  },
  {
    "id": 38,
    "question": "You are using Dynamic Programming to solve the Traveling Salesperson Problem (TSP) for a small set of cities (n <= 20). Why is Bitmasking essential in this approach?",
    "options": [
      "To reduce the space complexity by compressing the visited city set into an integer",
      "To perform binary search on the answer",
      "To handle floating-point calculations efficiently",
      "To sort the cities by distance"
    ],
    "answer": "To reduce the space complexity by compressing the visited city set into an integer",
    "explanation": "In DP for TSP, the 'state' consists of the current city and the set of visited cities. Bitmasking allows us to represent a set of up to 20 cities as a single integer, making the state representation feasible for DP tables. Without it, tracking subsets explicitly would be too memory-intensive.",
    "difficulty": "Intermediate"
  },
  {
    "id": 39,
    "question": "What is the primary advantage of using a Trie (Prefix Tree) over a Hash Map for storing a dictionary of strings?",
    "options": [
      "Faster lookup time for exact matches",
      "Lower memory usage for short strings",
      "Efficient prefix-based searches and alphabetical ordering",
      "Ability to store floating-point numbers"
    ],
    "answer": "Efficient prefix-based searches and alphabetical ordering",
    "explanation": "While Hash Maps offer O(1) average lookups, Tries excel at prefix-based operations (like autocomplete or spell checking) and naturally maintain lexicographical order. Tries generally use more memory than Hash Maps because of pointer overhead, and exact match lookups are comparable or slightly slower due to pointer traversal.",
    "difficulty": "Intermediate"
  },
  {
    "id": 40,
    "question": "When applying Binary Search on an unimodal function (a function that increases then decreases), why is Ternary Search used instead of standard Binary Search?",
    "options": [
      "To find the target value in logarithmic time",
      "To find the maximum or minimum point (the peak/valley) of the function",
      "To handle multiple distinct targets",
      "To reduce space complexity to O(1)"
    ],
    "answer": "To find the maximum or minimum point (the peak/valley) of the function",
    "explanation": "Ternary Search is specifically designed to find the maximum or minimum of a unimodal function by dividing the search range into three parts and discarding the section where the extremum cannot exist. Standard Binary Search is used to find a specific value in a sorted sequence, not an extremum.",
    "difficulty": "Intermediate"
  },
  {
    "id": 41,
    "question": "According to 2026 trends on Data Observability, how does the approach to data quality management shift compared to traditional methods?",
    "options": [
      "From manual checks to AI-driven predictive anomaly detection",
      "From cloud storage to on-premise servers",
      "From SQL databases to NoSQL only",
      "From batch processing to real-time streaming only"
    ],
    "answer": "From manual checks to AI-driven predictive anomaly detection",
    "explanation": "AI data observability represents a shift from reactive, manual quality monitoring to proactive, predictive prevention. It uses machine learning to detect anomalies as they happen, which is necessary because modern data infrastructures are too complex for effective manual oversight.",
    "difficulty": "Intermediate"
  },
  {
    "id": 42,
    "question": "You are implementing a system to manage dynamic connectivity in a graph (e.g., adding edges and checking if two nodes are connected). Which data structure provides near-constant time complexity for these operations?",
    "options": [
      "Segment Tree",
      "Disjoint Set Union (Union-Find)",
      "Binary Search Tree",
      "Linked List"
    ],
    "answer": "Disjoint Set Union (Union-Find)",
    "explanation": "Disjoint Set Union (DSU), optimized with path compression and union by rank, provides amortized near-constant time (inverse Ackermann function) for union and find operations. Segment Trees are for range queries, and BSTs/Linked Lists are too slow for dynamic connectivity.",
    "difficulty": "Intermediate"
  },
  {
    "id": 43,
    "question": "In a 'Data Mesh' architecture, how is the responsibility for data managed differently than in a monolithic data lake?",
    "options": [
      "A single centralized team owns all data",
      "Data ownership is distributed to domain-specific business units (e.g., Finance, Marketing)",
      "Data is managed entirely by external vendors",
      "Ownership is assigned based on the file size"
    ],
    "answer": "Data ownership is distributed to domain-specific business units (e.g., Finance, Marketing)",
    "explanation": "Data Mesh decentralizes data architecture, treating data as a product. Individual business domains take ownership of their data, rather than a single centralized team managing a monolithic lake. This improves scalability and accountability.",
    "difficulty": "Intermediate"
  },
  {
    "id": 44,
    "question": "You need to find the shortest path in an unweighted graph. Why is Breadth-First Search (BFS) preferred over Dijkstra's algorithm for this specific scenario?",
    "options": [
      "BFS uses less memory",
      "BFS is inherently faster because it doesn't require a priority queue",
      "BFS can handle negative weights",
      "Dijkstra cannot be used on unweighted graphs"
    ],
    "answer": "BFS is inherently faster because it doesn't require a priority queue",
    "explanation": "In an unweighted graph, edge weights are effectively uniform. BFS explores neighbors in layers, guaranteeing the shortest path is found without the overhead of a priority queue (min-heap) required by Dijkstra's algorithm. While Dijkstra works, it is less efficient constant-factor wise here.",
    "difficulty": "Intermediate"
  },
  {
    "id": 45,
    "question": "When implementing an LRU (Least Recently Used) Cache, which combination of data structures is typically used to achieve O(1) time complexity for both get and put operations?",
    "options": [
      "Array and Stack",
      "Hash Map and Doubly Linked List",
      "Hash Map and Single Linked List",
      "Queue and Priority Queue"
    ],
    "answer": "Hash Map and Doubly Linked List",
    "explanation": "A Hash Map provides O(1) access to the cache entries, while a Doubly Linked List maintains the usage order, allowing O(1) removal and insertion of nodes (updating the 'recently used' status). Arrays or Stacks do not support O(1) arbitrary insertion/deletion.",
    "difficulty": "Intermediate"
  },
  {
    "id": 46,
    "question": "What is the main benefit of the 'Binary Search on Answer' technique in competitive programming?",
    "options": [
      "It allows sorting an array in O(n) time",
      "It enables finding a specific value in a linked list",
      "It solves optimization problems by searching for a threshold in a monotonic solution space",
      "It reduces the memory usage of recursive algorithms"
    ],
    "answer": "It solves optimization problems by searching for a threshold in a monotonic solution space",
    "explanation": "Binary Search on Answer is used when the problem asks to find a maximum/minimum value that satisfies a condition. If checking the condition is monotonic (if X works, all values below/above X also work), we can binary search the answer space rather than iterating linearly.",
    "difficulty": "Intermediate"
  },
  {
    "id": 47,
    "question": "How is GenAI expected to change the workflow of Data Engineers by 2026?",
    "options": [
      "By writing all code in binary machine language",
      "By replacing Data Engineers entirely",
      "By allowing engineers to describe pipelines in natural language for autonomous generation",
      "By eliminating the need for databases"
    ],
    "answer": "By allowing engineers to describe pipelines in natural language for autonomous generation",
    "explanation": "The trend indicates that GenAI will handle repetitive ETL, cleaning, and formatting tasks. Engineers will be able to specify what they want in natural language, and the AI will implement the complex pipeline logic, reducing friction and speeding up insight generation.",
    "difficulty": "Intermediate"
  },
  {
    "id": 48,
    "question": "You are given a sorted array and a target sum. You need to find two distinct numbers in the array that add up to the target. Which algorithmic approach is most space-efficient compared to using a Hash Set?",
    "options": [
      "Brute Force Nested Loops",
      "Two Pointers",
      "Recursion",
      "Depth-First Search"
    ],
    "answer": "Two Pointers",
    "explanation": "The Two Pointers technique (one at start, one at end, moving inward) finds the pair in O(n) time with O(1) space, as it operates directly on the sorted array. A Hash Set approach also takes O(n) time but requires O(n) extra space to store elements.",
    "difficulty": "Intermediate"
  },
  {
    "id": 49,
    "question": "When dealing with 'Agent-Ready Data' in 2026, why is it critical to move away from siloed or legacy architectures?",
    "options": [
      "To increase the cost of storage",
      "To ensure AI agents can effectively access and process data to complete tasks",
      "To reduce the speed of data processing",
      "To make data harder to find for humans"
    ],
    "answer": "To ensure AI agents can effectively access and process data to complete tasks",
    "explanation": "For autonomous AI agents to function effectively in 2026, data must be accessible and in a format they can consume. Siloed legacy architectures block this access, preventing agents from performing the complex, cross-functional tasks they are designed to do.",
    "difficulty": "Intermediate"
  },
  {
    "id": 50,
    "question": "What is the time complexity of the standard Bitmasking DP solution for the Traveling Salesperson Problem (TSP) with N cities?",
    "options": [
      "O(N^2)",
      "O(N^3)",
      "O(2^N * N^2)",
      "O(N log N)"
    ],
    "answer": "O(2^N * N^2)",
    "explanation": "The TSP DP solution involves states defined by (mask, city), resulting in 2^N * N states. For each state, we iterate over N cities to transition. Thus, the total time complexity is O(2^N * N^2). This is exponential, making it feasible only for small N (usually N <= 20).",
    "difficulty": "Intermediate"
  },
  {
    "id": 51,
    "question": "You need to perform a range query (e.g., sum or min) on a static array that never changes. Between a Sparse Table and a Segment Tree, which is generally preferred for faster query times?",
    "options": [
      "Segment Tree",
      "Sparse Table",
      "Fenwick Tree",
      "Trie"
    ],
    "answer": "Sparse Table",
    "explanation": "Sparse Tables allow O(1) range queries for idempotent functions (like min, gcd, or max) on static arrays, whereas Segment Trees offer O(log n) queries. Since the array is static, the Sparse Table's O(N log N) preprocessing cost is a worthwhile trade-off for instant queries.",
    "difficulty": "Intermediate"
  },
  {
    "id": 52,
    "question": "In the context of algorithm design, what distinguishes a 'Greedy' approach from 'Dynamic Programming'?",
    "options": [
      "Greedy uses recursion; DP uses iteration",
      "Greedy makes the locally optimal choice at each step hoping for a global optimum; DP breaks the problem into overlapping subproblems",
      "Greedy is always faster than DP",
      "DP uses more memory than Greedy"
    ],
    "answer": "Greedy makes the locally optimal choice at each step hoping for a global optimum; DP breaks the problem into overlapping subproblems",
    "explanation": "The core difference is the strategy: Greedy algorithms make the best immediate choice without revisiting decisions, which works if the problem has the 'greedy choice property'. Dynamic programming considers all possibilities by solving subproblems and combining them, ensuring a global optimum even when greedy fails.",
    "difficulty": "Intermediate"
  },
  {
    "id": 53,
    "question": "Why is a Segment Tree considered a more flexible data structure than a Binary Indexed Tree (Fenwick Tree)?",
    "options": [
      "Segment trees always use less memory",
      "Segment trees are easier to implement",
      "Segment trees can handle a wider range of associative operations (like sum, min, max, gcd) and range updates more naturally",
      "Segment trees are faster for point queries"
    ],
    "answer": "Segment trees can handle a wider range of associative operations (like sum, min, max, gcd) and range updates more naturally",
    "explanation": "While Fenwick Trees are efficient for specific operations (sum, xor, point updates), Segment Trees are more versatile. They can handle any associative operation and complex range updates (like adding a value to a range) with relative ease, using Lazy Propagation.",
    "difficulty": "Intermediate"
  },
  {
    "id": 54,
    "question": "What does 'Data Provenance' refer to in the context of modern data engineering?",
    "options": [
      "The security protocol used to encrypt data",
      "The formal record of the origin, movement, and transformations of data throughout its lifecycle",
      "The physical location of the data center",
      "The frequency at which data is deleted"
    ],
    "answer": "The formal record of the origin, movement, and transformations of data throughout its lifecycle",
    "explanation": "Data Provenance tracks the 'lineage' of data—where it came from, who touched it, and how it was processed. This is crucial for debugging, auditing, and trusting AI outputs, ensuring data scientists can trace anomalies back to their source.",
    "difficulty": "Intermediate"
  },
  {
    "id": 55,
    "question": "You are designing a autocomplete feature for a mobile keyboard. You notice that using a standard Hash Map results in high memory consumption due to storing many long strings. Which data structure reduces memory usage by sharing common prefixes?",
    "options": [
      "Hash Map",
      "Trie (Prefix Tree)",
      "Stack",
      "Queue"
    ],
    "answer": "Trie (Prefix Tree)",
    "explanation": "Tries store strings character by character. Common prefixes (like 'app' in 'apple' and 'application') are stored only once and shared among different branches. A Hash Map stores each full string as a separate entry, leading to redundant storage of prefix characters.",
    "difficulty": "Intermediate"
  },
  {
    "id": 56,
    "question": "When implementing a graph using an Adjacency Matrix versus an Adjacency List, what is the primary trade-off regarding space complexity for a sparse graph?",
    "options": [
      "Adjacency Matrix uses less space because it stores only edges",
      "Adjacency List uses more space because it stores indices",
      "Adjacency Matrix uses O(V^2) space regardless of edges, making it inefficient for sparse graphs",
      "Adjacency List uses O(V^2) space"
    ],
    "answer": "Adjacency Matrix uses O(V^2) space regardless of edges, making it inefficient for sparse graphs",
    "explanation": "An Adjacency Matrix reserves a slot for every possible pair of vertices (V^2), even if no edge exists. In a sparse graph (few edges), this wastes significant space. An Adjacency List stores only the existing edges, scaling with O(V + E).",
    "difficulty": "Intermediate"
  },
  {
    "id": 57,
    "question": "You need to find the shortest path in a graph with negative edge weights (but no negative cycles). Why can't Dijkstra's algorithm be used?",
    "options": [
      "Dijkstra's algorithm is too slow",
      "Dijkstra's algorithm assumes that once a node is visited, the shortest distance to it is finalized, which fails with negative weights",
      "Dijkstra's algorithm cannot handle graphs",
      "Dijkstra's algorithm requires a priority queue, which is incompatible with negatives"
    ],
    "answer": "Dijkstra's algorithm assumes that once a node is visited, the shortest distance to it is finalized, which fails with negative weights",
    "explanation": "Dijkstra's greedy approach assumes that adding a non-negative weight to a path never makes it shorter. With negative weights, a path previously thought to be shortest can be shortened later by traversing a negative edge, violating the algorithm's core assumption.",
    "difficulty": "Intermediate"
  },
  {
    "id": 58,
    "question": "What is the purpose of the 'Union by Rank' (or Union by Size) heuristic in the Disjoint Set Union data structure?",
    "options": [
      "To increase the speed of the 'Find' operation",
      "To keep the tree flat, preventing the 'Find' operation from degrading to linear time",
      "To sort the elements in the set",
      "To reduce the number of sets"
    ],
    "answer": "To keep the tree flat, preventing the 'Find' operation from degrading to linear time",
    "explanation": "Union by Rank ensures that when merging two trees, the root of the smaller tree is attached to the root of the larger tree. This prevents the tree from becoming too tall (skewed), ensuring that 'Find' operations remain fast (near constant time) when combined with path compression.",
    "difficulty": "Intermediate"
  },
  {
    "id": 59,
    "question": "In a system that processes real-time data streams, why is 'Self-Service Data' considered a valuable trend for 2026?",
    "options": [
      "It forces users to write their own SQL queries",
      "It allows business users to access and combine data sources directly without waiting for IT, speeding up decision making",
      "It reduces the security of the data",
      "It increases the latency of data processing"
    ],
    "answer": "It allows business users to access and combine data sources directly without waiting for IT, speeding up decision making",
    "explanation": "Self-Service data empowers non-technical users (like marketing or HR) to access dashboards and notebooks themselves. This removes the bottleneck of waiting for a centralized data team to generate reports, leading to faster innovation and insights.",
    "difficulty": "Intermediate"
  },
  {
    "id": 60,
    "question": "When analyzing algorithms, why is Big O notation generally preferred over Big Theta (Θ) or Big Omega (Ω) for worst-case scenarios in interviews?",
    "options": [
      "Big O provides the exact running time",
      "Big O provides an upper bound (worst-case limit), which is crucial for guaranteeing performance limits",
      "Big Omega is only for sorting algorithms",
      "Big Theta is too complex to calculate"
    ],
    "answer": "Big O provides an upper bound (worst-case limit), which is crucial for guaranteeing performance limits",
    "explanation": "Big O describes the upper limit of growth rate. In engineering and interviews, we usually care about the worst-case scenario to ensure our system won't crash or hang. Big Theta is a tight bound (average/best/worst same), and Big Omega is the lower bound (best case).",
    "difficulty": "Intermediate"
  },
  {
    "id": 61,
    "question": "You are sorting a small array of integers where N is always less than 50. Although Merge Sort is O(N log N), why might Insertion Sort be practically faster?",
    "options": [
      "Insertion Sort has better space complexity",
      "Insertion Sort has lower constant factors and overhead for small N",
      "Merge Sort is unstable",
      "Insertion Sort uses divide and conquer"
    ],
    "answer": "Insertion Sort has lower constant factors and overhead for small N",
    "explanation": "For small input sizes, the theoretical complexity (Big O) matters less than the constant factors and implementation overhead. Insertion Sort has very low overhead and is extremely fast for small or nearly sorted arrays, often outperforming the more complex recursive Merge Sort.",
    "difficulty": "Intermediate"
  },
  {
    "id": 62,
    "question": "What is the 'Sliding Window' technique primarily used for?",
    "options": [
      "Sorting linked lists",
      "Finding substrings or subarrays that satisfy a specific condition by maintaining a window of elements",
      "Implementing a priority queue",
      "Traversing trees"
    ],
    "answer": "Finding substrings or subarrays that satisfy a specific condition by maintaining a window of elements",
    "explanation": "The Sliding Window technique involves maintaining a range (window) of elements in an array/string that expands or contracts. It is highly efficient (often O(N)) for problems like finding the longest substring with unique characters or the maximum sum of a subarray of size K.",
    "difficulty": "Intermediate"
  },
  {
    "id": 63,
    "question": "You encounter a 'Hash Collision' in a hash map implementation. How does a good collision resolution strategy affect performance?",
    "options": [
      "It converts the time complexity from O(1) to O(N) in the worst case",
      "It maintains O(1) average time by distributing keys uniformly",
      "It increases the memory usage to O(N^2)",
      "It eliminates the need for a hash function"
    ],
    "answer": "It maintains O(1) average time by distributing keys uniformly",
    "explanation": "Collisions are inevitable. A good resolution strategy (like separate chaining with linked lists or open addressing) ensures that collisions remain rare or are handled efficiently, preserving the average O(1) access time. Poor strategies can degrade performance to O(N).",
    "difficulty": "Intermediate"
  },
  {
    "id": 64,
    "question": "Why is Topological Sort only applicable to Directed Acyclic Graphs (DAGs)?",
    "options": [
      "Because undirected graphs cannot be sorted",
      "Because a cycle implies a dependency loop, making it impossible to find a valid linear ordering",
      "Because DAGs are the only graphs that have vertices",
      "Because Topological Sort uses DFS which fails on cycles"
    ],
    "answer": "Because a cycle implies a dependency loop, making it impossible to find a valid linear ordering",
    "explanation": "Topological Sort orders vertices such that for every directed edge (u, v), u comes before v. If there is a cycle (A depends on B, B depends on A), no linear order satisfies the condition. Therefore, the graph must be acyclic.",
    "difficulty": "Intermediate"
  },
  {
    "id": 65,
    "question": "In the Knuth-Morris-Pratt (KMP) algorithm, what is the function of the 'LPS' (Longest Prefix Suffix) array?",
    "options": [
      "To store the length of the longest substring",
      "To skip unnecessary comparisons by utilizing previously matched characters when a mismatch occurs",
      "To sort the pattern string",
      "To convert the pattern to lowercase"
    ],
    "answer": "To skip unnecessary comparisons by utilizing previously matched characters when a mismatch occurs",
    "explanation": "The LPS array stores the length of the longest proper prefix which is also a suffix for every prefix of the pattern. When a mismatch happens, the LPS value tells the algorithm how many characters it can safely skip re-matching, preventing the pointer from backtracking to the beginning.",
    "difficulty": "Intermediate"
  },
  {
    "id": 66,
    "question": "When comparing Merge Sort and Quick Sort, why is Quick Sort generally faster in practice despite having the same average time complexity?",
    "options": [
      "Quick Sort is always O(N log N)",
      "Quick Sort has better cache performance due to in-place partitioning and locality of reference",
      "Merge Sort uses too much memory",
      "Quick Sort uses a linked list internally"
    ],
    "answer": "Quick Sort has better cache performance due to in-place partitioning and locality of reference",
    "explanation": "While both are O(N log N) on average, Quick Sort operates in-place and accesses memory sequentially during partitioning, which aligns well with CPU cache lines (spatial locality). Merge Sort often requires auxiliary arrays and more data movement, which can incur higher overhead.",
    "difficulty": "Intermediate"
  },
  {
    "id": 67,
    "question": "You are tasked with designing a system for 'Data Sovereignty'. What constraint must your algorithm and storage architecture primarily address?",
    "options": [
      "Data must be replicated across all global regions",
      "Data must be stored and processed within the specific legal jurisdiction of its origin",
      "Data must be encrypted using 256-bit keys only",
      "Data must be anonymized before storage"
    ],
    "answer": "Data must be stored and processed within the specific legal jurisdiction of its origin",
    "explanation": "Data Sovereignty refers to laws and regulations requiring data about a nation's citizens (e.g., EU's GDPR, China's CSL) to be stored and processed inside that country's borders. Algorithms must ensure data routing and storage comply with these geographical constraints.",
    "difficulty": "Intermediate"
  },
  {
    "id": 68,
    "question": "What is the 'Heavy-Light Decomposition' technique primarily used for in advanced graph algorithms?",
    "options": [
      "Sorting edges by weight",
      "Breaking a tree into paths to answer path queries efficiently",
      "Finding the shortest path in a weighted graph",
      "Detecting cycles in a directed graph"
    ],
    "answer": "Breaking a tree into paths to answer path queries efficiently",
    "explanation": "Heavy-Light Decomposition (HLD) is a technique on trees that splits the tree into a set of disjoint paths. This allows complex queries on paths (like sum, max, or xor on the path between two nodes) to be answered using standard data structures like Segment Trees on the decomposed paths.",
    "difficulty": "Intermediate"
  },
  {
    "id": 69,
    "question": "Why might you choose a Suffix Tree over a Suffix Array for string processing?",
    "options": [
      "Suffix Trees are faster to construct",
      "Suffix Trees use significantly less memory",
      "Suffix Trees allow faster pattern matching and more complex queries (like longest common substring) at the cost of higher memory",
      "Suffix Arrays cannot be used for pattern matching"
    ],
    "answer": "Suffix Trees allow faster pattern matching and more complex queries (like longest common substring) at the cost of higher memory",
    "explanation": "Suffix Trees provide O(m) search time (where m is pattern length) and are intuitive for complex substring problems. However, they are extremely memory-intensive. Suffix Arrays use much less memory but are slightly slower (O(m log n)) and harder to implement for complex queries.",
    "difficulty": "Intermediate"
  },
  {
    "id": 70,
    "question": "When using recursion to solve a problem like Depth-First Search (DFS) on a deep graph, what is the specific risk of 'Stack Overflow'?",
    "options": [
      "The heap memory runs out",
      "The call stack runs out of memory due to deep recursive calls",
      "The algorithm enters an infinite loop only",
      "The CPU overheats"
    ],
    "answer": "The call stack runs out of memory due to deep recursive calls",
    "explanation": "Recursion uses the system call stack to keep track of function calls. If the recursion depth is too high (e.g., traversing a long linked list or a deep tree), the stack memory limit is exceeded, causing a Stack Overflow error. This is often why an iterative approach is preferred for very deep structures.",
    "difficulty": "Intermediate"
  },
  {
    "id": 71,
    "question": "In the context of solving the Traveling Salesperson Problem (TSP) using Dynamic Programming with Bitmasking, what is the time complexity of the standard Held-Karp algorithm for N cities?",
    "options": [
      "O(N!)",
      "O(N² * 2^N)",
      "O(N * 2^N)",
      "O(N^3)"
    ],
    "answer": "O(N² * 2^N)",
    "explanation": "The Held-Karp algorithm uses a DP state dp[mask][i] representing the minimum cost to visit the set of cities defined by 'mask' ending at city 'i'. There are 2^N subsets (masks) and for each mask, we iterate through N cities. The transition involves iterating through previous cities, leading to a total complexity of O(N² * 2^N). O(N!) is the complexity of the naive brute-force recursive solution.",
    "difficulty": "Advanced"
  },
  {
    "id": 72,
    "question": "Why is Ternary Search generally preferred over Binary Search when finding the maximum or minimum of a unimodal function, but strictly avoided for discrete monotonic functions?",
    "options": [
      "Ternary search has a better time complexity of O(log N) compared to Binary Search's O(N)",
      "Ternary search divides the search space into three parts, reducing the number of iterations needed by half",
      "Ternary search evaluates two midpoints per iteration, making it slower (more comparisons) than Binary Search for simple searching",
      "Binary search cannot handle floating-point numbers, whereas Ternary search is designed specifically for them"
    ],
    "answer": "Ternary search evaluates two midpoints per iteration, making it slower (more comparisons) than Binary Search for simple searching",
    "explanation": "While both are logarithmic, Ternary Search typically requires more comparisons per iteration because it must evaluate two points (m1 and m2) to determine which section to discard. Binary Search only evaluates one midpoint. For monotonic searches, Binary Search is strictly more efficient. Ternary Search is used for unimodal functions (finding a peak/valley) where the derivative changes sign, a scenario Binary Search cannot handle directly.",
    "difficulty": "Advanced"
  },
  {
    "id": 73,
    "question": "When implementing Heavy-Light Decomposition (HLD) on a tree to answer path queries, what is the worst-case time complexity for a single query operation (e.g., sum or max on a path)?",
    "options": [
      "O(N)",
      "O(log² N)",
      "O(log N)",
      "O(√N)"
    ],
    "answer": "O(log² N)",
    "explanation": "HLD breaks the tree into chains. A query between two nodes may require jumping between O(log N) different chains. Within each chain, we typically use a Segment Tree or Fenwick Tree to answer the sub-query in O(log N) time. Therefore, the total complexity is O(log N) chains * O(log N) query time = O(log² N). O(N) would imply a linear scan, which HLD is designed to avoid.",
    "difficulty": "Advanced"
  },
  {
    "id": 74,
    "question": "A Fenwick Tree (Binary Indexed Tree) is often preferred over a Segment Tree for specific operations. Which of the following statements accurately describes a scenario where a Fenwick Tree is the superior choice?",
    "options": [
      "When the operation is not invertible (e.g., matrix multiplication)",
      "When range updates and range queries are required simultaneously without complex modification",
      "When the code needs to be extremely concise and memory-efficient for prefix sums of invertible operations",
      "When the data points are sparse and non-integer"
    ],
    "answer": "When the code needs to be extremely concise and memory-efficient for prefix sums of invertible operations",
    "explanation": "Fenwick Trees are generally more memory-efficient (lower constant factors) and easier to implement than Segment Trees. However, they strictly require the operation to be invertible (like sum or XOR) to compute range queries [L, R] as Prefix(R) - Prefix(L). Segment Trees are more versatile, handling non-invertible operations (like max, min, GCD) and lazy propagation for range updates more naturally.",
    "difficulty": "Advanced"
  },
  {
    "id": 75,
    "question": "In Disjoint Set Union (DSU), the 'Path Compression' heuristic is used during the 'Find' operation. What is the theoretical amortized time complexity of a DSU operation when both Path Compression and Union by Rank are applied?",
    "options": [
      "O(1)",
      "O(log N)",
      "O(α(N))",
      "O(N log N)"
    ],
    "answer": "O(α(N))",
    "explanation": "α(N) is the Inverse Ackermann function. For all practical values of N (even up to the size of the universe), α(N) is less than 5. Therefore, the operations are effectively constant time, O(1), but strictly speaking, the mathematical bound is O(α(N)). O(log N) is the complexity if only Union by Rank is used without Path Compression.",
    "difficulty": "Advanced"
  },
  {
    "id": 76,
    "question": "What is the primary structural advantage of a Suffix Tree over a Suffix Trie when processing large strings for substring searches?",
    "options": [
      "A Suffix Tree allows for pattern matching in O(1) regardless of pattern length",
      "A Suffix Tree compresses non-branching paths into single edges, reducing space complexity from O(N²) to O(N)",
      "A Suffix Tree stores strings in a binary search format, allowing standard BST traversal",
      "A Suffix Tree does not require pre-processing the source text"
    ],
    "answer": "A Suffix Tree compresses non-branching paths into single edges, reducing space complexity from O(N²) to O(N)",
    "explanation": "A Suffix Trie explicitly stores every suffix, leading to O(N²) nodes and space. A Suffix Tree is a compressed trie ( Patricia tree ) where chains of nodes with only one child are merged. This reduces the space to linear O(N) while retaining the ability to search for substrings in O(M) time, where M is pattern length.",
    "difficulty": "Advanced"
  },
  {
    "id": 77,
    "question": "When using 'Binary Search on Answer' to solve an optimization problem, what is the strict requirement regarding the predicate function P(x)?",
    "options": [
      "P(x) must be a continuous function",
      "P(x) must be monotonic (e.g., True, True ... False, False)",
      "P(x) must have a single global maximum or minimum",
      "P(x) must be differentiable to calculate the slope"
    ],
    "answer": "P(x) must be monotonic (e.g., True, True ... False, False)",
    "explanation": "Binary search relies on the ability to discard half of the search space based on the result of the mid-point check. This is only possible if the predicate function is monotonic—meaning if P(x) is True, then P(x+1) is either always True (or always False, depending on definition). If the function fluctuates (True, False, True), binary search fails.",
    "difficulty": "Advanced"
  },
  {
    "id": 78,
    "question": "In the context of 2026 data trends, why is 'Synthetic Data' generation considered a crucial algorithmic advancement for training Machine Learning models in highly regulated industries like finance?",
    "options": [
      "It increases the volume of data by simply duplicating existing records",
      "It bypasses the need for data cleaning and feature engineering",
      "It generates realistic data that mimics statistical properties of real data without exposing sensitive Personally Identifiable Information (PII)",
      "It guarantees that the resulting model will have 100% accuracy in production"
    ],
    "answer": "It generates realistic data that mimics statistical properties of real data without exposing sensitive Personally Identifiable Information (PII)",
    "explanation": "The primary algorithmic benefit of synthetic data (often generated via GANs or diffusion models) is the ability to train on datasets that reflect the variance and distribution of real-world data without privacy risks. It solves the 'data scarcity' and 'privacy' problems simultaneously. It does not guarantee accuracy or remove the need for cleaning the underlying generation logic.",
    "difficulty": "Advanced"
  },
  {
    "id": 79,
    "question": "When implementing a persistent data structure (e.g., a Persistent Segment Tree), what is the primary trade-off accepted to achieve versioning capabilities?",
    "options": [
      "Write operations become significantly slower to maintain history",
      "The time complexity for queries increases from O(log N) to O(N)",
      "Space complexity increases significantly because nodes are often shared or copied instead of modified in-place",
      "The structure cannot support range updates"
    ],
    "answer": "Space complexity increases significantly because nodes are often shared or copied instead of modified in-place",
    "explanation": "Persistence requires keeping access to previous states of the data structure. Instead of modifying a node directly (which would alter history), a new node is created. Path copying ensures that unmodified parts of the tree are shared between versions, but the total number of nodes grows linearly with the number of updates, trading space for the ability to query past versions.",
    "difficulty": "Advanced"
  },
  {
    "id": 80,
    "question": "How does 'Data + AI Observability' fundamentally differ from traditional data monitoring dashboards in terms of algorithmic approach?",
    "options": [
      "It relies solely on human analysts to review logs manually",
      "It uses machine learning algorithms to detect anomalies and predict data drifts proactively, rather than just checking static thresholds",
      "It focuses only on infrastructure uptime rather than data quality metrics",
      "It eliminates the need for ETL pipelines by processing data at the edge"
    ],
    "answer": "It uses machine learning algorithms to detect anomalies and predict data drifts proactively, rather than just checking static thresholds",
    "explanation": "Traditional monitoring is reactive and rule-based (e.g., 'alert if nulls > 5%'). AI Observability uses algorithms to learn the 'normal' shape and distribution of data over time, identifying subtle drifts, outliers, or anomalies that static rules would miss, shifting the paradigm from reactive fixing to predictive prevention.",
    "difficulty": "Advanced"
  },
  {
    "id": 81,
    "question": "When solving range queries on a static array, what is the worst-case time complexity for answering a query using a Sparse Table, and why is it not suitable for dynamic data?",
    "options": [
      "O(N) because it iterates through the range",
      "O(log N) because it uses binary lifting",
      "O(1) because it precomputes answers for intervals of length 2^k, but updates require O(N log N) reconstruction",
      "O(1) for both query and update"
    ],
    "answer": "O(1) because it precomputes answers for intervals of length 2^k, but updates require O(N log N) reconstruction",
    "explanation": "Sparse Tables offer O(1) query time by precomputing range minimums (or other idempotent operations) for powers of two. However, because the precomputation is heavily based on the static indices of the original array, updating a single element would invalidate O(N) precomputed intervals, requiring O(N log N) time to rebuild. Thus, it is strictly for static arrays.",
    "difficulty": "Advanced"
  },
  {
    "id": 82,
    "question": "In the context of a Trie (Prefix Tree), what is the primary advantage of using a 'Radix Tree' (or Patricia Trie) over a standard Trie?",
    "options": [
      "It reduces the number of nodes by compressing chains of single-child nodes into edges labeled with strings",
      "It allows for faster insertion of integers",
      "It stores data in a sorted order without extra space",
      "It converts the trie into a binary search tree"
    ],
    "answer": "It reduces the number of nodes by compressing chains of single-child nodes into edges labeled with strings",
    "explanation": "Standard Tries can have many nodes with only one child, wasting memory. Radix Trees compress these 'single-child' paths. Instead of having edges for 'c', 'a', 'r', a Radix Tree will have a single edge labeled 'car'. This significantly reduces memory overhead and improves lookup speed in some cases by reducing pointer traversals.",
    "difficulty": "Advanced"
  },
  {
    "id": 83,
    "question": "When designing an algorithm for 'Agent-Ready Data' in 2026, what is a critical architectural consideration regarding data accessibility?",
    "options": [
      "Data must be stored in legacy monolithic silos to ensure security",
      "Data must be highly structured, well-documented via metadata, and accessible via APIs, not just readable by humans",
      "Data should be encrypted in such a way that even agents cannot decrypt it",
      "Data must be converted exclusively into unstructured text"
    ],
    "answer": "Data must be highly structured, well-documented via metadata, and accessible via APIs, not just readable by humans",
    "explanation": "For autonomous agents to interact with data effectively, the data cannot be locked in unstructured PDFs or legacy mainframes. It needs to be machine-readable (structured), accompanied by rich semantic metadata (context), and accessible via standardized interfaces (APIs) so agents can discover, interpret, and utilize it without manual intervention.",
    "difficulty": "Advanced"
  },
  {
    "id": 84,
    "question": "What is the time complexity of the Z-algorithm (Z-function) for pattern matching, and why is it efficient?",
    "options": [
      "O(N log N) due to sorting operations",
      "O(N²) in the worst case",
      "O(N) because it calculates the Z-array in a single pass maintaining an interval [L, R]",
      "O(M * N) where M is pattern length and N is text length"
    ],
    "answer": "O(N) because it calculates the Z-array in a single pass maintaining an interval [L, R]",
    "explanation": "The Z-algorithm computes an array Z[i] which is the length of the longest substring starting from S[i] that is also a prefix of S. It achieves linear time O(N) by maintaining a window [L, R] that represents the segment with maximum R. If the current index i lies within [L, R], it uses previously computed values to initialize Z[i], avoiding re-comparison of characters.",
    "difficulty": "Advanced"
  },
  {
    "id": 85,
    "question": "Why might a randomized algorithm like Monte Carlo be preferred over a deterministic algorithm like Las Vegas in a distributed system for consensus or decision making?",
    "options": [
      "Monte Carlo algorithms always produce the correct result, whereas Las Vegas algorithms might fail",
      "Monte Carlo algorithms have a bounded execution time, whereas Las Vegas algorithms have unbounded execution time in the worst case",
      "Monte Carlo algorithms require less memory than Las Vegas algorithms",
      "Monte Carlo algorithms are easier to parallelize than sorting algorithms"
    ],
    "answer": "Monte Carlo algorithms have a bounded execution time, whereas Las Vegas algorithms have unbounded execution time in the worst case",
    "explanation": "The key distinction is the guarantee. A Las Vegas algorithm always gives the correct result but its runtime varies (might get stuck or take long). A Monte Carlo algorithm has a fixed, bounded runtime but has a probability of producing an incorrect result. In real-time distributed systems, meeting the deadline (bounded time) is often more critical than absolute probabilistic correctness.",
    "difficulty": "Advanced"
  },
  {
    "id": 86,
    "question": "When using a Treap (Cartesian Tree), what property guarantees that the tree remains balanced with high probability?",
    "options": [
      "Strict height balancing rules like in AVL trees",
      "Color flipping rules like in Red-Black trees",
      "Randomized priority assignment to nodes following heap property alongside binary search tree keys",
      "Splitting and merging based solely on hash values of keys"
    ],
    "answer": "Randomized priority assignment to nodes following heap property alongside binary search tree keys",
    "explanation": "A Treap combines the properties of a Binary Search Tree (by key) and a Heap (by priority). By assigning priorities randomly (or pseudo-randomly), the expected shape of the tree mimics that of a randomly generated BST, which has logarithmic height with high probability. It avoids the complex rotation logic of AVL or Red-Black trees by using simple split/merge operations based on priority.",
    "difficulty": "Advanced"
  },
  {
    "id": 87,
    "question": "In the context of graph algorithms, what is the 'Min-Cut' problem in a flow network, and how does the Max-Flow Min-Cut theorem relate to it?",
    "options": [
      "Min-Cut is the path with the minimum weight; Max-Flow is the sum of all edge weights",
      "Min-Cut is the minimum total weight of edges that, if removed, disconnect the source from the sink; its value equals the maximum flow possible",
      "Min-Cut is the shortest path in an unweighted graph; Max-Flow is the number of distinct paths",
      "Min-Cut is the node with the lowest degree; Max-Flow is the maximum number of nodes reachable"
    ],
    "answer": "Min-Cut is the minimum total weight of edges that, if removed, disconnect the source from the sink; its value equals the maximum flow possible",
    "explanation": "The Max-Flow Min-Cut theorem states that in a flow network, the maximum amount of flow passing from source to sink is equal to the total weight of the edges in the minimum cut. The 'cut' is a partition of vertices separating source and sink, and its capacity is the sum of capacities of edges crossing the partition.",
    "difficulty": "Advanced"
  },
  {
    "id": 88,
    "question": "When implementing an LRU (Least Recently Used) Cache, which data structure combination provides O(1) time complexity for both 'get' and 'put' operations?",
    "options": [
      "A doubly linked list and a hash map",
      "A singly linked list and an array",
      "A min-heap and a hash map",
      "A balanced binary search tree and a queue"
    ],
    "answer": "A doubly linked list and a hash map",
    "explanation": "The Hash Map provides O(1) access to cache items. The Doubly Linked List maintains the usage order: the head is the most recently used, and the tail is the least recently used. A doubly linked list is required to easily remove a node from the middle of the list (when accessed) and move it to the head in O(1) time, which a singly linked list cannot do without a pointer to the previous node.",
    "difficulty": "Advanced"
  },
  {
    "id": 89,
    "question": "What is the purpose of 'Lazy Propagation' in a Segment Tree, and how does it affect complexity?",
    "options": [
      "It delays the building of the tree until the first query, saving memory",
      "It updates nodes only when necessary during a query, deferring updates to children to ensure Range Updates are O(log N) instead of O(N)",
      "It converts the tree into a binary heap to sort the data",
      "It stores updates in a separate log file to process them later in batch"
    ],
    "answer": "It updates nodes only when necessary during a query, deferring updates to children to ensure Range Updates are O(log N) instead of O(N)",
    "explanation": "Without lazy propagation, updating a range [L, R] requires visiting and updating every leaf node in that range, which is O(N). Lazy propagation marks a node with a 'pending update' and only updates the node's current value. The update is pushed to children only when we actually need to query them. This keeps range updates at O(log N).",
    "difficulty": "Advanced"
  },
  {
    "id": 90,
    "question": "Why is the A* (A-Star) search algorithm preferred over Dijkstra's algorithm for pathfinding in game AI or robotics?",
    "options": [
      "A* has a better worst-case time complexity than Dijkstra",
      "A* uses a heuristic to guide the search toward the goal, potentially exploring far fewer nodes than Dijkstra's blind search",
      "A* can handle negative edge weights, whereas Dijkstra cannot",
      "A* guarantees the shortest path in all graph types including those with cycles"
    ],
    "answer": "A* uses a heuristic to guide the search toward the goal, potentially exploring far fewer nodes than Dijkstra's blind search",
    "explanation": "Dijkstra explores in all directions uniformly like a ripple in a pond until the target is found. A* uses a heuristic function (estimated distance to goal) to prioritize nodes that seem closer to the target. If the heuristic is admissible (never overestimates), A* finds the optimal path but usually does so much faster by pruning the search space.",
    "difficulty": "Advanced"
  },
  {
    "id": 91,
    "question": "What is the 'Convex Hull Trick' (CHT) used to optimize, and in what scenario does the standard CDT (Li Chao Tree) variant become necessary?",
    "options": [
      "Sorting arrays; when data is dynamic",
      "Finding the shortest path in a graph; when edges have negative weights",
      "Optimizing DP problems involving maximum/minimum of linear functions; when lines are added dynamically or queries are offline vs online",
      "Compressing strings; when the alphabet size is large"
    ],
    "answer": "Optimizing DP problems involving maximum/minimum of linear functions; when lines are added dynamically or queries are offline vs online",
    "explanation": "The Convex Hull Trick is used to optimize DP recurrences where the transition involves taking the max/min of lines (y = mx + c) at a specific x. Standard CHT works if lines are added in a specific slope order (offline). The Li Chao Tree variant is used when lines are added in arbitrary order (dynamic insertions) or when we need to query at arbitrary x-coordinates efficiently.",
    "difficulty": "Advanced"
  },
  {
    "id": 92,
    "question": "In the context of Big O notation, what is the time complexity of the 'Strassen's Algorithm' for matrix multiplication, and how does it compare to the standard naive approach?",
    "options": [
      "O(N³), same as naive",
      "O(N^2.81), which is asymptotically faster than naive O(N³)",
      "O(N log N), similar to Fast Fourier Transform",
      "O(N^2), linear in the number of elements"
    ],
    "answer": "O(N^2.81), which is asymptotically faster than naive O(N³)",
    "explanation": "Standard matrix multiplication takes O(N³) because each element of the resulting matrix requires a dot product of N elements. Strassen's Algorithm uses a divide-and-conquer strategy with clever arithmetic to reduce the number of recursive multiplications, achieving a complexity of approximately O(N^2.81). While faster for large N theoretically, it has high constant factors and numerical stability issues compared to the naive O(N³) approach.",
    "difficulty": "Advanced"
  },
  {
    "id": 93,
    "question": "What specific algorithmic challenge is addressed by 'Data Mesh' architecture in 2026, moving away from monolithic data lakes?",
    "options": [
      "It centralizes all data into a single database to simplify queries",
      "It applies domain-oriented ownership to solve bottlenecks in data ingestion, quality, and access caused by a single centralized team",
      "It replaces SQL with NoSQL for all storage layers",
      "It eliminates the need for metadata by using AI to guess data structures"
    ],
    "answer": "It applies domain-oriented ownership to solve bottlenecks in data ingestion, quality, and access caused by a single centralized team",
    "explanation": "In monolithic architectures, a single team is the bottleneck for all data engineering tasks. Data Mesh decentralizes this, treating data as a product. While algorithmically this implies federated query processing and governance, the core challenge being solved is the scalability of data management and bottlenecks inherent in a centrally managed pipeline.",
    "difficulty": "Advanced"
  },
  {
    "id": 94,
    "question": "When implementing a Bloom Filter, how does increasing the size of the bit array and the number of hash functions affect the False Positive rate?",
    "options": [
      "Increasing both increases the False Positive rate",
      "Increasing both decreases the False Positive rate up to a point, but too many hash functions increases latency",
      "Increasing the bit array increases False Positives, but increasing hash functions decreases them",
      "Bloom Filters do not have False Positives, only False Negatives"
    ],
    "answer": "Increasing both decreases the False Positive rate up to a point, but too many hash functions increases latency",
    "explanation": "A Bloom Filter is a probabilistic data structure. A larger bit array spreads bits out more (fewer collisions), and more hash functions provide more distinct checks per element. Both contribute to a lower probability of False Positives. However, more hash functions mean more computation per query, increasing latency. There is an optimal number of hash functions for a given bit array size and expected number of elements.",
    "difficulty": "Advanced"
  },
  {
    "id": 95,
    "question": "What distinguishes a 'Splay Tree' from other balanced binary search trees like AVL or Red-Black trees?",
    "options": [
      "It stores additional balance information in every node",
      "It is a deterministic tree where the shape is always perfectly balanced",
      "It does not require explicit balance fields; it moves recently accessed elements to the root via 'splaying' for amortized O(log n) performance",
      "It allows duplicate keys but no cycles"
    ],
    "answer": "It does not require explicit balance fields; it moves recently accessed elements to the root via 'splaying' for amortized O(log n) performance",
    "explanation": "Splay trees are self-adjusting. They do not store height or color metadata. Instead, every time a node is accessed (found, inserted, or deleted), it is moved to the root via rotations (splaying). This makes frequently accessed elements faster to reach (locality of reference) and provides amortized logarithmic performance, though individual operations can be O(N).",
    "difficulty": "Advanced"
  },
  {
    "id": 96,
    "question": "In the context of computational geometry, what is the primary limitation of the 'Graham Scan' algorithm for finding the Convex Hull?",
    "options": [
      "It has a time complexity of O(N²)",
      "It requires the points to be sorted by one coordinate (e.g., polar angle) first, which takes O(N log N) time",
      "It only works in 3D space, not 2D",
      "It cannot handle integer coordinates"
    ],
    "answer": "It requires the points to be sorted by one coordinate (e.g., polar angle) first, which takes O(N log N) time",
    "explanation": "The Graham Scan algorithm itself processes points in O(N) time using a stack. However, to work correctly, it needs the points to be sorted radially (by angle) around a pivot point. This sorting step dictates the overall time complexity of the algorithm to O(N log N). Monotone Chain is an alternative that sorts by x/y coordinates instead of angle.",
    "difficulty": "Advanced"
  },
  {
    "id": 97,
    "question": "What is the 'Mo's Algorithm' used for, and what is the characteristic impact on query time complexity?",
    "options": [
      "Shortest path in weighted graphs; O(V log V)",
      "Processing offline range queries on arrays by sorting queries in a specific order to achieve O((N+Q)√N) complexity",
      "String matching; O(N+M)",
      "Finding the Minimum Spanning Tree; O(E log V)"
    ],
    "answer": "Processing offline range queries on arrays by sorting queries in a specific order to achieve O((N+Q)√N) complexity",
    "explanation": "Mo's Algorithm is a method to efficiently answer range queries (e.g., sum, distinct count, mode) offline. It sorts the queries in a special order (Hilbert order or block sorting) so that moving the current Left and Right pointers from one query to the next minimizes total movement. This achieves a complexity of roughly O((N+Q) * √N), often better than O(N*Q) naive.",
    "difficulty": "Advanced"
  },
  {
    "id": 98,
    "question": "When analyzing algorithms for 'Data Provenance', what is the primary technical challenge that makes tracking lineage difficult in modern ETL pipelines?",
    "options": [
      "Data is always stored in a single file, making it easy to track",
      "Transformations are often lossy or multi-source, meaning the output is a complex function of many inputs, not a 1:1 mapping",
      "Data provenance is not supported by SQL databases",
      "Tracking lineage increases the time complexity of the algorithm to O(N!)"
    ],
    "answer": "Transformations are often lossy or multi-source, meaning the output is a complex function of many inputs, not a 1:1 mapping",
    "explanation": "In simple pipelines, moving a file A to B is trivial to track. In modern pipelines, data might be aggregated, filtered, joined, or anonymized (lossy). Determining exactly which input rows influenced a specific output row (fine-grained provenance) requires complex instrumentation and often keeping intermediate data snapshots, which is computationally and storage-intensive.",
    "difficulty": "Advanced"
  },
  {
    "id": 99,
    "question": "Why is 'Consistent Hashing' a critical algorithm component in distributed systems implementing a Data Mesh or Data Fabric?",
    "options": [
      "It ensures that every node stores a copy of all data for safety",
      "It minimizes data movement when nodes are added or removed by mapping both data and nodes to a hash ring",
      "It sorts the data alphabetically to improve search speed",
      "It compresses data before sending it over the network"
    ],
    "answer": "It minimizes data movement when nodes are added or removed by mapping both data and nodes to a hash ring",
    "explanation": "In a distributed system, nodes fail or are added frequently. Standard hashing (mod N) would require re-mapping almost all keys to new nodes, causing massive data transfer. Consistent Hashing places both nodes and keys on a continuous ring. A key belongs to the nearest clockwise node. When a node is added/removed, only the keys near that node on the ring are affected, minimizing disruption.",
    "difficulty": "Advanced"
  },
  {
    "id": 100,
    "question": "In the context of advanced string algorithms, what is the computational complexity of finding the Longest Common Subsequence (LCS) between two strings of length M and N using standard Dynamic Programming?",
    "options": [
      "O(M + N)",
      "O(M * N)",
      "O(M * N * log(M * N))",
      "O(min(M, N))"
    ],
    "answer": "O(M * N)",
    "explanation": "The standard DP solution for LCS uses a 2D table dp[i][j] representing the LCS length of the first i characters of string A and first j characters of string B. Filling this table requires computing M * N states, and each state takes O(1) time to compute (comparing chars). Therefore, the time and space complexity is O(M * N). While space-optimized versions exist, the time complexity remains quadratic.",
    "difficulty": "Advanced"
  }
]