[
  {
    "id": 1,
    "question": "What is the primary goal of MLOps?",
    "options": [
      "To replace data scientists with automated algorithms",
      "To deploy and maintain machine learning models in production reliably and efficiently",
      "To reduce the cost of cloud computing services",
      "To eliminate the need for training data"
    ],
    "answer": "To deploy and maintain machine learning models in production reliably and efficiently",
    "explanation": "MLOps aims to automate and streamline the deployment, monitoring, and maintenance of ML models, bridging the gap between development and operations.",
    "difficulty": "Beginner"
  },
  {
    "id": 2,
    "question": "In a production ML system, what component is responsible for transforming raw data into a format suitable for training?",
    "options": [
      "The inference API",
      "The data pipeline",
      "The model registry",
      "The user interface"
    ],
    "answer": "The data pipeline",
    "explanation": "The data pipeline handles data ingestion, cleaning, feature engineering, and transformation, preparing raw data for the model training process.",
    "difficulty": "Beginner"
  },
  {
    "id": 3,
    "question": "What is the 'Global Interpreter Lock' (GIL) in CPython?",
    "options": [
      "A mechanism to prevent multiple threads from executing Python bytecodes at once",
      "A tool for locking Python files to prevent editing",
      "A security feature for restricting network access",
      "A method for optimizing memory usage in lists"
    ],
    "answer": "A mechanism to prevent multiple threads from executing Python bytecodes at once",
    "explanation": "The GIL is a mutex that ensures only one thread executes Python bytecode at a time, simplifying memory management but limiting CPU-bound parallelism.",
    "difficulty": "Beginner"
  },
  {
    "id": 4,
    "question": "Which technology is most commonly used to ensure that an ML model runs in the same environment regardless of where it is deployed?",
    "options": [
      "Virtual Machines (VMs)",
      "Containers (e.g., Docker)",
      "Serverless Functions",
      "Bare Metal Servers"
    ],
    "answer": "Containers (e.g., Docker)",
    "explanation": "Containers package the model, dependencies, and runtime environment into a single portable unit, ensuring consistency across development and production.",
    "difficulty": "Beginner"
  },
  {
    "id": 5,
    "question": "What is the primary function of a Model Registry in MLOps?",
    "options": [
      "To store raw training datasets",
      "To version and manage trained model artifacts",
      "To monitor server CPU usage",
      "To write code for data visualization"
    ],
    "answer": "To version and manage trained model artifacts",
    "explanation": "A Model Registry acts as a centralized repository for storing trained models, tracking their versions, and managing metadata associated with each model.",
    "difficulty": "Beginner"
  },
  {
    "id": 6,
    "question": "What is 'Concept Drift' in the context of machine learning?",
    "options": [
      "A change in the programming language used for the model",
      "A change in the relationship between input data and the target variable over time",
      "An error in the database connection string",
      "A technique for speeding up model training"
    ],
    "answer": "A change in the relationship between input data and the target variable over time",
    "explanation": "Concept drift occurs when the statistical properties of the target variable change, rendering the model's learned patterns less accurate over time.",
    "difficulty": "Beginner"
  },
  {
    "id": 7,
    "question": "Which deployment strategy routes a small percentage of user traffic to a new model version to test its performance?",
    "options": [
      "Big Bang Deployment",
      "Blue/Green Deployment",
      "Canary Deployment",
      "Recreate Deployment"
    ],
    "answer": "Canary Deployment",
    "explanation": "Canary deployment incrementally rolls out the new version to a small subset of users before a full rollout, allowing for risk mitigation.",
    "difficulty": "Beginner"
  },
  {
    "id": 8,
    "question": "Why is Experiment Tracking important for an AI Engineer?",
    "options": [
      "It automatically writes unit tests",
      "It records parameters, metrics, and artifacts to reproduce model runs",
      "It reduces the size of the model file",
      "It encrypts the training data"
    ],
    "answer": "It records parameters, metrics, and artifacts to reproduce model runs",
    "explanation": "Experiment tracking logs hyperparameters, code versions, and results, enabling reproducibility and comparison between different training runs.",
    "difficulty": "Beginner"
  },
  {
    "id": 9,
    "question": "What distinguishes MLOps CI/CD from traditional software CI/CD?",
    "options": [
      "MLOps does not use testing",
      "MLOps includes stages for model training, validation, and data testing",
      "MLOps only works with Java code",
      "MLOps pipelines are always manual"
    ],
    "answer": "MLOps includes stages for model training, validation, and data testing",
    "explanation": "Unlike standard CI/CD, MLOps incorporates model-re specific steps such as data validation, model training, and performance evaluation.",
    "difficulty": "Beginner"
  },
  {
    "id": 10,
    "question": "What is 'Data Versioning' and why is it necessary?",
    "options": [
      "Keeping track of changes to the database schema",
      "Tracking specific datasets used for training to ensure reproducibility",
      "Updating the software version of the Python interpreter",
      "Compressing files to save disk space"
    ],
    "answer": "Tracking specific datasets used for training to ensure reproducibility",
    "explanation": "Data versioning allows teams to revert to the exact dataset state used for a specific model version, which is crucial for debugging and auditing.",
    "difficulty": "Beginner"
  },
  {
    "id": 11,
    "question": "In the context of LLMs, what does 'RAG' stand for?",
    "options": [
      "Retrieval-Augmented Generation",
      "Real-time Application Gateway",
      "Recursive Argument Generator",
      "Random Access Graph"
    ],
    "answer": "Retrieval-Augmented Generation",
    "explanation": "RAG enhances an LLM by retrieving relevant information from an external knowledge base before generating a response, improving accuracy and reducing hallucinations.",
    "difficulty": "Beginner"
  },
  {
    "id": 12,
    "question": "What is the primary function of a 'Vector Database'?",
    "options": [
      "To store user passwords securely",
      "To store and query high-dimensional embeddings efficiently",
      "To manage SQL transactions",
      "To serve static website files"
    ],
    "answer": "To store and query high-dimensional embeddings efficiently",
    "explanation": "Vector databases are optimized to index and search vector embeddings, enabling semantic search and similarity matching in AI applications.",
    "difficulty": "Beginner"
  },
  {
    "id": 13,
    "question": "What is 'Grounding' in the context of Generative AI?",
    "options": [
      "Deleting the model after use",
      "Restricting the model to specific, trusted data sources to ensure factuality",
      "Training the model on images only",
      "Running the model on a physical server"
    ],
    "answer": "Restricting the model to specific, trusted data sources to ensure factuality",
    "explanation": "Grounding involves connecting an LLM to verified data sources (like documents or APIs) to anchor its responses in facts and reduce hallucinations.",
    "difficulty": "Beginner"
  },
  {
    "id": 14,
    "question": "What is the difference between 'Batch Inference' and 'Real-time Inference'?",
    "options": [
      "Batch is faster than real-time",
      "Real-time processes data points immediately as requests arrive; Batch processes accumulations of data at once",
      "Batch requires GPUs; Real-time requires CPUs",
      "Real-time uses unstructured data; Batch uses structured data"
    ],
    "answer": "Real-time processes data points immediately as requests arrive; Batch processes accumulations of data at once",
    "explanation": "Real-time inference serves predictions instantly upon request (low latency), while batch inference processes large volumes of data offline (high throughput).",
    "difficulty": "Beginner"
  },
  {
    "id": 15,
    "question": "What is a 'Golden Set' or 'Holdout Set' used for in model evaluation?",
    "options": [
      "Training the model parameters",
      "Providing an unbiased evaluation of a model's final performance",
      "Storing backup copies of the code",
      "Reducing the memory footprint"
    ],
    "answer": "Providing an unbiased evaluation of a model's final performance",
    "explanation": "A holdout set is data kept separate from the training and validation process, used strictly for the final assessment of how the model will perform in the real world.",
    "difficulty": "Beginner"
  },
  {
    "id": 16,
    "question": "Which metric is commonly used to evaluate the performance of a regression model?",
    "options": [
      "Accuracy",
      "F1 Score",
      "Root Mean Squared Error (RMSE)",
      "Area Under the Curve (AUC)"
    ],
    "answer": "Root Mean Squared Error (RMSE)",
    "explanation": "RMSE measures the average magnitude of the error between predicted and actual continuous values, making it standard for regression tasks.",
    "difficulty": "Beginner"
  },
  {
    "id": 17,
    "question": "What is 'Prompt Injection' in the context of AI Security?",
    "options": [
      "A method to speed up database queries",
      "A security attack where malicious inputs manipulate an LLM's output",
      "A technique for compressing prompts",
      "A debugging tool for developers"
    ],
    "answer": "A security attack where malicious inputs manipulate an LLM's output",
    "explanation": "Prompt injection involves crafting inputs that bypass the model's intended instructions or safety filters to force it to perform unintended actions.",
    "difficulty": "Beginner"
  },
  {
    "id": 18,
    "question": "What does 'Quantization' refer to in the context of model optimization?",
    "options": [
      "Splitting a model into smaller files",
      "Reducing the precision of model weights (e.g., 32-bit to 8-bit) to decrease size and increase speed",
      "Increasing the training data volume",
      "Encrypting the model for security"
    ],
    "answer": "Reducing the precision of model weights (e.g., 32-bit to 8-bit) to decrease size and increase speed",
    "explanation": "Quantization lowers the computational cost and memory footprint of a model by using lower-precision numbers for calculations and storage.",
    "difficulty": "Beginner"
  },
  {
    "id": 19,
    "question": "Which tool is primarily used for orchestrating complex ML workflows and pipelines?",
    "options": [
      "Apache Airflow",
      "Tableau",
      "Microsoft Excel",
      "Visual Studio Code"
    ],
    "answer": "Apache Airflow",
    "explanation": "Airflow is an open-source tool for programmatically authoring, scheduling, and monitoring workflows, commonly used to manage data and ML pipelines.",
    "difficulty": "Beginner"
  },
  {
    "id": 20,
    "question": "What is the primary function of 'Feature Engineering'?",
    "options": [
      "Writing code for the user interface",
      "Using domain knowledge to extract features from raw data to improve model performance",
      "Configuring the network firewall",
      "Purchasing hardware for the model"
    ],
    "answer": "Using domain knowledge to extract features from raw data to improve model performance",
    "explanation": "Feature engineering involves creating new input variables from existing data to help algorithms learn patterns more effectively.",
    "difficulty": "Beginner"
  },
  {
    "id": 21,
    "question": "What is a 'Transformer' in the context of Deep Learning?",
    "options": [
      "A hardware device that converts AC to DC power",
      "A neural network architecture that uses attention mechanisms to process sequential data",
      "A script for changing data formats",
      "A type of database join operation"
    ],
    "answer": "A neural network architecture that uses attention mechanisms to process sequential data",
    "explanation": "Transformers utilize self-attention mechanisms to process sequences in parallel, forming the backbone of modern Large Language Models (LLMs).",
    "difficulty": "Beginner"
  },
  {
    "id": 22,
    "question": "What does the term 'Hallucination' mean when discussing Large Language Models?",
    "options": [
      "The model running out of memory",
      "The model generating confident but factually incorrect or nonsensical information",
      "The model visualizing images from text",
      "The model optimizing its weights"
    ],
    "answer": "The model generating confident but factually incorrect or nonsensical information",
    "explanation": "Hallucination occurs when an LLM generates plausible-sounding text that is not grounded in its training data or reality.",
    "difficulty": "Beginner"
  },
  {
    "id": 23,
    "question": "Why might an organization choose On-Premise deployment over Public Cloud APIs for LLMs?",
    "options": [
      "Public APIs are always more expensive",
      "To ensure data privacy and keep sensitive information within their own infrastructure",
      "On-premise servers are always faster",
      "Public APIs do not support Python"
    ],
    "answer": "To ensure data privacy and keep sensitive information within their own infrastructure",
    "explanation": "Organizations use on-premise deployment to maintain strict control over proprietary data and comply with data sovereignty regulations.",
    "difficulty": "Beginner"
  },
  {
    "id": 24,
    "question": "What is the main difference between 'Fine-tuning' and 'Prompt Engineering'?",
    "options": [
      "Prompt engineering updates the model weights; Fine-tuning updates the input text",
      "Fine-tuning updates the model weights on a specific dataset; Prompt engineering optimizes the input text",
      "Fine-tuning is done before training; Prompt engineering is done during inference",
      "There is no difference; they are synonyms"
    ],
    "answer": "Fine-tuning updates the model weights on a specific dataset; Prompt engineering optimizes the input text",
    "explanation": "Fine-tuning involves retraining the model to adjust its weights, whereas prompt engineering involves crafting the input to guide the pre-trained model's behavior without changing weights.",
    "difficulty": "Beginner"
  },
  {
    "id": 25,
    "question": "What is a 'Token' in Natural Language Processing?",
    "options": [
      "A physical coin used for payment",
      "The basic unit of text, such as a word, part of a word, or character",
      "A security password",
      "A database row"
    ],
    "answer": "The basic unit of text, such as a word, part of a word, or character",
    "explanation": "Tokenization breaks text down into smaller units (tokens) that the model can process and convert into numerical vectors.",
    "difficulty": "Beginner"
  },
  {
    "id": 26,
    "question": "What is 'SFT' an acronym for in LLM training?",
    "options": [
      "Sparse Feature Training",
      "Supervised Fine-Tuning",
      "Scalable File Transfer",
      "System Feedback Testing"
    ],
    "answer": "Supervised Fine-Tuning",
    "explanation": "SFT involves training a pre-trained model on a labeled dataset to instruct it to follow specific directions and behaviors.",
    "difficulty": "Beginner"
  },
  {
    "id": 27,
    "question": "Which of the following is a popular open-source library for tracking ML experiments?",
    "options": [
      "Pandas",
      "MLflow",
      "NumPy",
      "Flask"
    ],
    "answer": "MLflow",
    "explanation": "MLflow is an open-source platform designed specifically to manage the ML lifecycle, including experimentation, reproducibility, and deployment.",
    "difficulty": "Beginner"
  },
  {
    "id": 28,
    "question": "What does 'A/B Testing' refer to in production ML systems?",
    "options": [
      "Testing two different programming languages",
      "Comparing two model versions in production to see which performs better on live traffic",
      "Testing the model with two different databases",
      "Alternating between training and testing phases"
    ],
    "answer": "Comparing two model versions in production to see which performs better on live traffic",
    "explanation": "A/B testing splits user traffic between a control (current model) and a variant (new model) to statistically validate performance improvements.",
    "difficulty": "Beginner"
  },
  {
    "id": 29,
    "question": "What is the primary benefit of using a 'Feature Store'?",
    "options": [
      "It stores raw video files",
      "It provides a centralized repository for storing, retrieving, and managing features for training and serving",
      "It visualizes neural network layers",
      "It automatically writes unit tests"
    ],
    "answer": "It provides a centralized repository for storing, retrieving, and managing features for training and serving",
    "explanation": "Feature stores ensure consistency between training and serving by providing a single source of truth for feature logic and data.",
    "difficulty": "Beginner"
  },
  {
    "id": 30,
    "question": "Which metric is best suited for evaluating a classification model's ability to distinguish between classes?",
    "options": [
      "Mean Absolute Error (MAE)",
      "Area Under the ROC Curve (AUC-ROC)",
      "R-squared",
      "Sum of Squared Errors"
    ],
    "answer": "Area Under the ROC Curve (AUC-ROC)",
    "explanation": "AUC-ROC measures the ability of a classifier to distinguish between positive and negative classes across various threshold settings.",
    "difficulty": "Beginner"
  },
  {
    "id": 31,
    "question": "What is 'Hyperparameter Tuning'?",
    "options": [
      "Adjusting the model weights during training",
      "Optimizing the configuration settings of the learning algorithm before training",
      "Cleaning the data of null values",
      "Compressing the model for mobile devices"
    ],
    "answer": "Optimizing the configuration settings of the learning algorithm before training",
    "explanation": "Hyperparameters are settings (like learning rate or tree depth) that control the learning process and must be set prior to training.",
    "difficulty": "Beginner"
  },
  {
    "id": 32,
    "question": "What is 'Shadow Mode' deployment?",
    "options": [
      "Deploying the model only to the darkest markets",
      "Running the new model alongside the old one in production without exposing it to users to compare performance",
      "Deploying the model without any server",
      "Deleting the old model immediately"
    ],
    "answer": "Running the new model alongside the old one in production without exposing it to users to compare performance",
    "explanation": "Shadow deployment allows engineers to validate the new model's behavior and latency metrics on live traffic without affecting user experience.",
    "difficulty": "Beginner"
  },
  {
    "id": 33,
    "question": "In Kubernetes, what is a 'Pod'?",
    "options": [
      "A group of containers sharing storage and network resources",
      "A monitoring dashboard",
      "A type of database",
      "A Python package manager"
    ],
    "answer": "A group of containers sharing storage and network resources",
    "explanation": "A Pod is the smallest deployable unit in Kubernetes, representing a single instance of a running process in a cluster.",
    "difficulty": "Beginner"
  },
  {
    "id": 34,
    "question": "What is 'Model Drift'?",
    "options": [
      "The model accidentally deleting data",
      "The degradation of model performance due to changes in the environment or data over time",
      "The model downloading files slowly",
      "The compilation time of the model"
    ],
    "answer": "The degradation of model performance due to changes in the environment or data over time",
    "explanation": "Model drift encompasses both data drift (changes in input data distribution) and concept drift (changes in the target variable), leading to reduced accuracy.",
    "difficulty": "Beginner"
  },
  {
    "id": 35,
    "question": "What is the purpose of 'RLHF' in training LLMs?",
    "options": [
      "To compress the model size",
      "To align the model's outputs with human preferences using Reinforcement Learning",
      "To reduce the training cost",
      "To filter the training data"
    ],
    "answer": "To align the model's outputs with human preferences using Reinforcement Learning",
    "explanation": "RLHF (Reinforcement Learning from Human Feedback) uses human ratings on model outputs to fine-tune the model, making it more helpful and safe.",
    "difficulty": "Beginner"
  },
  {
    "id": 36,
    "question": "In the context of MLOps, what is the primary purpose of a 'Shadow Deployment' strategy?",
    "options": [
      "To split incoming traffic between two model versions to compare performance",
      "To deploy the new model to production while routing all inference traffic to the previous version",
      "To run the new model alongside the old one on duplicated traffic without affecting user responses",
      "To gradually increase the percentage of traffic sent to the new model based on confidence metrics"
    ],
    "answer": "To run the new model alongside the old one on duplicated traffic without affecting user responses",
    "explanation": "Shadow deployment routes a copy of live requests to the new model to validate its behavior and latency metrics before it serves actual user traffic. Unlike A/B testing or canary deployments, the shadow model's predictions are discarded and not returned to the client.",
    "difficulty": "Intermediate"
  },
  {
    "id": 37,
    "question": "Which specific type of data drift occurs when the distribution of the input features (X) changes between the training and production environments, but the relationship between X and Y remains intact?",
    "options": [
      "Covariate Shift",
      "Prior Probability Shift",
      "Concept Drift",
      "Label Shift"
    ],
    "answer": "Covariate Shift",
    "explanation": "Covariate shift specifically refers to a change in the distribution of the input data (features) $P(X)$, while the conditional distribution of the target given the features $P(Y|X)$ stays the same. This requires techniques like importance weighting to correct model performance.",
    "difficulty": "Intermediate"
  },
  {
    "id": 38,
    "question": "What is the primary function of a 'Feature Store' in a production Machine Learning architecture?",
    "options": [
      "To store raw data lakes for exploratory data analysis",
      "To provide a centralized repository for storing, retrieving, and managing versioned features for training and serving",
      "To automate the hyperparameter tuning process for neural networks",
      "To serve as a database for storing model checkpoints and weights"
    ],
    "answer": "To provide a centralized repository for storing, retrieving, and managing versioned features for training and serving",
    "explanation": "A feature store ensures feature consistency by decoupling feature engineering from model training and serving. It prevents training-serving skew by allowing both training pipelines and online inference applications to read the exact same feature definitions and values.",
    "difficulty": "Intermediate"
  },
  {
    "id": 39,
    "question": "When orchestrating an ML pipeline using a Directed Acyclic Graph (DAG), what is the consequence of a node failing?",
    "options": [
      "The downstream tasks will proceed with cached data from the previous successful run",
      "The entire branch of the DAG downstream from the failed node will be skipped or marked as failed",
      "The orchestrator will automatically roll back the production model to the previous version",
      "The failed node will retry indefinitely until it succeeds, blocking all other pipelines"
    ],
    "answer": "The entire branch of the DAG downstream from the failed node will be skipped or marked as failed",
    "explanation": "In a DAG, tasks depend on the completion of their parents. If a node fails, its children cannot execute as their required inputs are missing, causing the pipeline execution to halt or skip that specific dependency chain.",
    "difficulty": "Intermediate"
  },
  {
    "id": 40,
    "question": "In an MLOps CI/CD pipeline, what distinguishes a 'Continuous Training' (CT) trigger from a standard software deployment trigger?",
    "options": [
      "CT triggers are initiated manually by a data scientist, whereas software deployments are automated",
      "CT triggers are based on code commits to the repository, not data changes",
      "CT triggers are initiated automatically by changes in data performance metrics or data drift, not just code commits",
      "CT triggers run only when the underlying infrastructure (e.g., Kubernetes) scales up"
    ],
    "answer": "CT triggers are initiated automatically by changes in data performance metrics or data drift, not just code commits",
    "explanation": "While standard CI/CD relies on code commits, Continuous Training (CT) is often triggered by data-related events, such as a drop in model accuracy, concept drift detection, or the arrival of a significant volume of new training data.",
    "difficulty": "Intermediate"
  },
  {
    "id": 41,
    "question": "What is the definition of 'Training-Serving Skew'?",
    "options": [
      "The difference in latency between offline batch scoring and real-time API inference",
      "The discrepancy between the model's performance during offline evaluation and its performance in a live production environment",
      "The variance in predictions made by two different model versions running simultaneously",
      "The time delay between when a model is trained and when it is deployed to production"
    ],
    "answer": "The discrepancy between the model's performance during offline evaluation and its performance in a live production environment",
    "explanation": "Training-serving skew occurs when the data distribution or feature engineering logic in production differs from what was used during training. It results in degradation of model performance in live environments despite good offline validation metrics.",
    "difficulty": "Intermediate"
  },
  {
    "id": 42,
    "question": "Which technique is used to reduce the size of a Large Language Model (LLM) with minimal loss in accuracy by identifying and removing less significant weights?",
    "options": [
      "Quantization",
      "Distillation",
      "Pruning",
      "Retrieval-Augmented Generation (RAG)"
    ],
    "answer": "Pruning",
    "explanation": "Pruning involves removing weights within a neural network that contribute little to the model's output, effectively sparsifying the model. This reduces memory footprint and computational needs, whereas distillation trains a smaller student model and quantization reduces precision.",
    "difficulty": "Intermediate"
  },
  {
    "id": 43,
    "question": "When implementing Retrieval-Augmented Generation (RAG), what is the role of the 'Embedding Model'?",
    "options": [
      "To generate the final natural language response to the user",
      "To compress the external knowledge base into a smaller binary file",
      "To convert text documents and user queries into numerical vectors for semantic similarity comparison",
      "To filter out malicious prompts from the user input"
    ],
    "answer": "To convert text documents and user queries into numerical vectors for semantic similarity comparison",
    "explanation": "The embedding model translates unstructured text into high-dimensional vectors (embeddings). This allows the system to perform a semantic search in the vector database to find contextually relevant documents based on vector proximity.",
    "difficulty": "Intermediate"
  },
  {
    "id": 44,
    "question": "Why is 'Non-Determinism' a specific challenge in monitoring GPU-based model inference in production?",
    "options": [
      "GPUs generate random errors during floating-point arithmetic that require manual correction",
      "Parallel processing and nondeterministic low-level optimizations can cause slight output variations for the same input",
      "GPU memory is volatile and resets every time the inference container restarts",
      "GPUs cannot handle asynchronous requests, leading to race conditions in the logs"
    ],
    "answer": "Parallel processing and nondeterministic low-level optimizations can cause slight output variations for the same input",
    "explanation": "Deep learning frameworks often utilize GPU parallelism (e.g., atomic operations) where the order of operations is not guaranteed. This can lead to minute numerical differences in outputs on subsequent runs, complicating exact log comparisons and debugging.",
    "difficulty": "Intermediate"
  },
  {
    "id": 45,
    "question": "In the context of LLMOps, what is 'Grounding' (or RAG) primarily designed to mitigate?",
    "options": [
      "High latency in model inference",
      "Hallucinations and outdated knowledge in the language model",
      "High cost of GPU compute hours",
      "Syntax errors in the generated code"
    ],
    "answer": "Hallucinations and outdated knowledge in the language model",
    "explanation": "Grounding connects the model to external, verifiable data sources. By constraining the generation process to retrieved context, it significantly reduces the likelihood of the model fabricating information or relying on pre-training cutoff dates.",
    "difficulty": "Intermediate"
  },
  {
    "id": 46,
    "question": "Which strategy involves replacing a large, complex model with a smaller, simpler model that mimics its behavior?",
    "options": [
      "Model Quantization",
      "Knowledge Distillation",
      "Model Pruning",
      "Feature Selection"
    ],
    "answer": "Knowledge Distillation",
    "explanation": "Knowledge distillation involves training a 'student' model to reproduce the outputs (probabilities/logits) of a larger 'teacher' model. This compresses the knowledge into a lighter model suitable for edge deployment or low-latency requirements.",
    "difficulty": "Intermediate"
  },
  {
    "id": 47,
    "question": "What is the primary technical advantage of using 'Model Parallelism' over 'Data Parallelism' during distributed training?",
    "options": [
      "It allows the training of models that are too large to fit on a single device",
      "It is faster because it does not require synchronization of gradients between devices",
      "It eliminates the need for a validation dataset",
      "It allows for the use of different batch sizes on different devices"
    ],
    "answer": "It allows the training of models that are too large to fit on a single device",
    "explanation": "Model parallelism splits the model itself across multiple devices (e.g., layers on different GPUs). This is essential for training massive models (like LLMs) that exceed the VRAM of a single GPU, whereas data parallelism splits the dataset across GPUs.",
    "difficulty": "Intermediate"
  },
  {
    "id": 48,
    "question": "When using a 'Blue-Green Deployment' for a machine learning model, what is the critical infrastructure requirement?",
    "options": [
      "The ability to run two identical production environments simultaneously",
      "A feature store that supports real-time streaming",
      "A load balancer that supports only weighted routing",
      "The database schema must be mutable during the deployment"
    ],
    "answer": "The ability to run two identical production environments simultaneously",
    "explanation": "Blue-green deployment requires a full duplicate environment (Green) running alongside the current live one (Blue). This allows for an instant switch (cutover) by changing the router configuration, minimizing downtime and providing an instant rollback path.",
    "difficulty": "Intermediate"
  },
  {
    "id": 49,
    "question": "What is 'Concept Drift' in the context of production machine learning?",
    "options": [
      "The change in the statistical distribution of the input variables over time",
      "The change in the relationship between input variables and the target variable over time",
      "The degradation of model accuracy due to bugs in the deployment pipeline",
      "The natural decay of the learning rate during the training phase"
    ],
    "answer": "The change in the relationship between input variables and the target variable over time",
    "explanation": "Unlike covariate shift (where input data changes), concept drift means the fundamental mapping $f(x) \to y$ has evolvedâ€”meaning the model learned past patterns that are no longer valid, requiring retraining.",
    "difficulty": "Intermediate"
  },
  {
    "id": 50,
    "question": "Which metric is best suited for evaluating a classification model when the cost of False Positives is significantly higher than the cost of False Negatives?",
    "options": [
      "Recall",
      "Accuracy",
      "Precision",
      "F1 Score"
    ],
    "answer": "Precision",
    "explanation": "Precision measures the accuracy of positive predictions ($TP / (TP + FP)$). Maximizing precision minimizes false positives, which is the priority when a false alarm is more expensive or damaging than missing a true positive (e.g., spam detection or fraud alerts).",
    "difficulty": "Intermediate"
  },
  {
    "id": 51,
    "question": "What is the function of the 'Collator' in a modern data loading pipeline (such as Hugging Face Datasets)?",
    "options": [
      "To download the dataset from the cloud storage provider",
      "To apply data augmentation techniques such as rotation or flipping",
      "To batch and pad lists of samples into tensor formats for the model",
      "To split the dataset into training, validation, and test sets"
    ],
    "answer": "To batch and pad lists of samples into tensor formats for the model",
    "explanation": "A collator (or Data Collator) takes a list of dataset elements and applies dynamic padding or batching logic to create a single batch of tensors suitable for input into a model, handling variable sequence lengths efficiently.",
    "difficulty": "Intermediate"
  },
  {
    "id": 52,
    "question": "In the context of explainable AI (XAI), what does the SHAP (SHapley Additive exPlanations) value represent?",
    "options": [
      "The absolute importance of a feature regardless of the specific prediction",
      "The contribution of a specific feature to the difference between the actual prediction and the average prediction",
      "The probability that a feature is causally linked to the target variable",
      "The reduction in entropy achieved by splitting on a specific feature"
    ],
    "answer": "The contribution of a specific feature to the difference between the actual prediction and the average prediction",
    "explanation": "SHAP values are based on game theory; they allocate the 'payout' (the prediction) among the features (players). Each value represents how much a specific feature contributed to shifting the model output from the baseline (average) to the observed value.",
    "difficulty": "Intermediate"
  },
  {
    "id": 53,
    "question": "What is the primary benefit of using 'Semantic Caching' in LLM applications?",
    "options": [
      "To store user PII for regulatory compliance",
      "To reduce inference cost and latency by serving responses from previously generated similar queries",
      "To encrypt the prompt before it is sent to the API",
      "To automatically fine-tune the model based on user feedback"
    ],
    "answer": "To reduce inference cost and latency by serving responses from previously generated similar queries",
    "explanation": "Semantic caching checks the vector embedding of a user query against a cache of past queries. If a semantically similar question exists, the cached response is returned immediately, avoiding expensive API calls to the LLM.",
    "difficulty": "Intermediate"
  },
  {
    "id": 54,
    "question": "When managing microservices for ML models, which pattern allows you to route specialized requests to specific models optimized for those tasks?",
    "options": [
      "The Sidecar Pattern",
      "The Ambassador Pattern",
      "The Model Ensemble Pattern",
      "The Router/Ensembler Pattern"
    ],
    "answer": "The Router/Ensembler Pattern",
    "explanation": "A router (or meta-learner) receives the initial request and directs it to the most appropriate downstream microservice model based on the request content, geography, or business logic, enabling a heterogeneous mix of specialized models.",
    "difficulty": "Intermediate"
  },
  {
    "id": 55,
    "question": "In the context of Differential Privacy, what is the 'Privacy Budget' ($\\epsilon$)?",
    "options": [
      "The monetary cost allocated for cloud compute resources",
      "The maximum number of queries allowed on a dataset",
      "The parameter that controls the trade-off between data utility and privacy loss",
      "The number of features that can be exposed in the model API"
    ],
    "answer": "The parameter that controls the trade-off between data utility and privacy loss",
    "explanation": "Epsilon ($\\epsilon$) represents the privacy loss. A lower $\\epsilon$ ensures higher privacy (less risk of identifying an individual) but adds more noise to the data, reducing the model's utility or accuracy.",
    "difficulty": "Intermediate"
  },
  {
    "id": 56,
    "question": "Which of the following best describes the 'Curse of Dimensionality' in the context of feature engineering for high-dimensional data?",
    "options": [
      "Models take longer to train because the data is too large to fit in memory",
      "The distance between data points becomes less meaningful as the number of dimensions increases",
      "The model overfits because the number of features exceeds the number of rows",
      "Visualizing data becomes impossible without using dimensionality reduction"
    ],
    "answer": "The distance between data points becomes less meaningful as the number of dimensions increases",
    "explanation": "In high-dimensional spaces, data points tend to become equidistant from each other. This makes distance-based algorithms (like k-NN) inefficient because the concept of 'nearest neighbor' loses its discriminative power.",
    "difficulty": "Intermediate"
  },
  {
    "id": 57,
    "question": "What is the primary difference between 'LoRA' (Low-Rank Adaptation) and full fine-tuning of Large Language Models?",
    "options": [
      "LoRA requires a larger dataset to achieve comparable performance",
      "LoRA freezes the pre-trained weights and injects trainable rank decomposition matrices",
      "LoRA modifies the architecture of the base model by adding new layers",
      "LoRA is a technique for training models from scratch to reduce time"
    ],
    "answer": "LoRA freezes the pre-trained weights and injects trainable rank decomposition matrices",
    "explanation": "LoRA is a parameter-efficient fine-tuning (PEFT) method. Instead of updating all weights, it freezes the main model and trains smaller adapter matrices, drastically reducing the number of trainable parameters and memory usage.",
    "difficulty": "Intermediate"
  },
  {
    "id": 58,
    "question": "When designing an ML system, what distinguishes 'Online Inference' from 'Batch Inference'?",
    "options": [
      "Online inference is used for training models, while batch inference is used for evaluation",
      "Online inference processes individual or small groups of requests in real-time with low latency; batch inference processes large volumes of data asynchronously",
      "Online inference requires GPUs, while batch inference only uses CPUs",
      "Online inference is more accurate than batch inference"
    ],
    "answer": "Online inference processes individual or small groups of requests in real-time with low latency; batch inference processes large volumes of data asynchronously",
    "explanation": "Online inference (or serving) prioritizes low latency for interactive applications (e.g., chatbots), while batch inference prioritizes high throughput for non-urgent, bulk processing tasks (e.g., overnight score generation).",
    "difficulty": "Intermediate"
  },
  {
    "id": 59,
    "question": "In a CI/CD pipeline for Machine Learning, what is 'Model Card' documentation intended to provide?",
    "options": [
      "A list of all software dependencies required to train the model",
      "A standardized record of the model's intended use, limitations, performance metrics, and ethical considerations",
      "The source code used to preprocess the training data",
      "The login credentials for the model registry"
    ],
    "answer": "A standardized record of the model's intended use, limitations, performance metrics, and ethical considerations",
    "explanation": "Model Cards (and Datasheets for Datasets) are documentation frameworks used to promote transparency. They inform stakeholders about what the model is designed for, what data it was trained on, and where it should not be used.",
    "difficulty": "Intermediate"
  },
  {
    "id": 60,
    "question": "Why is 'Stationarity' an important concept in time-series forecasting models?",
    "options": [
      "Stationary data ensures that the model runs faster on a CPU",
      "Stationary data has statistical properties (mean, variance) that do not change over time, making patterns easier to learn",
      "Stationary data is required for all neural network architectures",
      "Stationary data refers to data that is stored in a single static location"
    ],
    "answer": "Stationary data has statistical properties (mean, variance) that do not change over time, making patterns easier to learn",
    "explanation": "Many forecasting models assume stationarity. If the mean or variance trends or seasonality change over time (non-stationarity), the model may fail to generalize; differencing or detrending is often applied to achieve stationarity.",
    "difficulty": "Intermediate"
  },
  {
    "id": 61,
    "question": "What is the specific role of 'Reward Modeling' in Reinforcement Learning from Human Feedback (RLHF) for LLMs?",
    "options": [
      "To calculate the loss function for the pre-training phase",
      "To train a separate model to predict human preference scores, which guides the policy optimization",
      "To generate synthetic data for augmenting the training set",
      "To quantize the model weights for faster inference"
    ],
    "answer": "To train a separate model to predict human preference scores, which guides the policy optimization",
    "explanation": "In RLHF, a reward model is first trained on rankings of outputs generated by the supervised model. This reward model then acts as a critic, scoring the LLM's outputs during the reinforcement learning phase to align it with human instructions.",
    "difficulty": "Intermediate"
  },
  {
    "id": 62,
    "question": "What problem does 'Token Waiting Time' (Prefill) cause in LLM serving systems?",
    "options": [
      "It causes the model to generate text slower than the user can type",
      "It refers to the latency incurred before generation begins, caused by processing the entire input prompt context",
      "It is the time required to load the model from disk to GPU memory",
      "It describes the queue length when multiple users request generation simultaneously"
    ],
    "answer": "It refers to the latency incurred before generation begins, caused by processing the entire input prompt context",
    "explanation": "Before an LLM can generate the first token (Time to First Token), it must process the entire input prompt through the transformer layers (the prefill phase). Long prompts significantly increase this initial latency.",
    "difficulty": "Intermediate"
  },
  {
    "id": 63,
    "question": "Which component of an MLOps system is primarily responsible for tracking the lineage of data artifacts?",
    "options": [
      "The Model Registry",
      "The Data Catalog",
      "The Feature Store",
      "The Experiment Tracker"
    ],
    "answer": "The Data Catalog",
    "explanation": "While feature stores store current data, a Data Catalog is specifically designed to provide metadata management, including lineage (where data comes from, how it was transformed), discovery, and governance of data assets.",
    "difficulty": "Intermediate"
  },
  {
    "id": 64,
    "question": "What is the primary technical reason to use 'gRPC' instead of REST for internal model-to-model communication in microservices?",
    "options": [
      "gRPC uses human-readable JSON, which is easier to debug",
      "gRPC utilizes Protocol Buffers and HTTP/2 for lower latency and higher efficiency than JSON/HTTP",
      "gRPC is natively supported by all web browsers",
      "gRPC does not require an interface definition file (IDL)"
    ],
    "answer": "gRPC utilizes Protocol Buffers and HTTP/2 for lower latency and higher efficiency than JSON/HTTP",
    "explanation": "gRPC uses binary Protocol Buffers for serialization, which is smaller and faster to parse than JSON. Combined with HTTP/2 multiplexing, it offers significantly higher performance for high-volume internal service communication.",
    "difficulty": "Intermediate"
  },
  {
    "id": 65,
    "question": "What is 'Prompt Injection' in the context of securing LLM applications?",
    "options": [
      "A Denial of Service attack on the model server",
      "An attack where a user manipulates the input to cause the model to ignore its instructions and perform unintended actions",
      "A method to fine-tune the model using adversarial examples",
      "A technique to inject code into the model's weights"
    ],
    "answer": "An attack where a user manipulates the input to cause the model to ignore its instructions and perform unintended actions",
    "explanation": "Prompt injection (e.g., jailbreaking) involves crafting input that overrides the system prompt. This can lead to data exfiltration, offensive content generation, or the execution of arbitrary system commands connected to the LLM.",
    "difficulty": "Intermediate"
  },
  {
    "id": 66,
    "question": "In the context of Model Risk Management (MRM), what is 'Challenger Models'?",
    "options": [
      "Models used specifically to test the security of the API endpoints",
      "Alternative models or versions run in production shadow mode to compare performance against the 'Champion' model",
      "Models that have been decommissioned but kept in archives",
      "Statistical models used to generate synthetic data for testing"
    ],
    "answer": "Alternative models or versions run in production shadow mode to compare performance against the 'Champion' model",
    "explanation": "Challenger models are new architectures or retrained versions deployed alongside the current production Champion (often in shadow mode). If the Challenger demonstrates statistically significant improvement, it may replace the Champion.",
    "difficulty": "Intermediate"
  },
  {
    "id": 67,
    "question": "Which technique helps mitigate the 'Cold Start' problem in recommender systems?",
    "options": [
      "Using a content-based filtering approach for new users or items",
      "Decreasing the regularization parameter in the collaborative filtering matrix",
      "Increasing the batch size during the training of the recommendation engine",
      "Deleting old user history to make room for new data"
    ],
    "answer": "Using a content-based filtering approach for new users or items",
    "explanation": "Collaborative filtering fails when there is no interaction history (Cold Start). Content-based filtering relies on item or user attributes (metadata) rather than interaction history, allowing recommendations to be made for new items or users immediately.",
    "difficulty": "Intermediate"
  },
  {
    "id": 68,
    "question": "What is the purpose of a 'Control Plane' in a distributed machine learning training cluster (e.g., on Kubernetes)?",
    "options": [
      "To store the actual training dataset shards",
      "To manage the state of the worker nodes, handle scheduling, and orchestrate the training job",
      "To perform the forward and backward passes of the neural network",
      "To aggregate the gradients from the workers (the AllReduce operation)"
    ],
    "answer": "To manage the state of the worker nodes, handle scheduling, and orchestrate the training job",
    "explanation": "The Control Plane (often the master node or orchestrator) is responsible for scheduling pods, monitoring health, and managing the overall workflow. It separates management logic from the actual computation (Worker Nodes) and data.",
    "difficulty": "Intermediate"
  },
  {
    "id": 69,
    "question": "When implementing 'A/B Testing' for a machine learning model, why is 'Interleaving' sometimes considered superior to random assignment?",
    "options": [
      "Interleaving is easier to implement than random assignment",
      "Interleaving exposes the user to both models simultaneously, reducing the impact of noise and user variability on the metrics",
      "Interleaving allows for testing more than two models at the same time",
      "Interleaving does not require statistical significance testing"
    ],
    "answer": "Interleaving exposes the user to both models simultaneously, reducing the impact of noise and user variability on the metrics",
    "explanation": "In interleaving, results from both models are mixed into a single list (e.g., ranking). This directly compares model performance on the exact same context and user session, yielding statistically significant results much faster than traditional A/B testing.",
    "difficulty": "Intermediate"
  },
  {
    "id": 70,
    "question": "In a Retrieval-Augmented Generation (RAG) system, what is the primary technical advantage of performing a 're-ranking' step after the initial vector retrieval?",
    "options": [
      "It reduces the dimensionality of the vector embeddings to save memory",
      "It allows the system to correct hallucinations in the Large Language Model (LLM)",
      "It uses a more computationally expensive model (e.g., Cross-Encoder) to improve precision on a smaller candidate subset",
      "It converts the semantic search into a keyword-based BM25 search"
    ],
    "answer": "It uses a more computationally expensive model (e.g., Cross-Encoder) to improve precision on a smaller candidate subset",
    "explanation": "Initial retrieval uses fast Bi-Encoders to return a broad set of candidates (high recall). Re-ranking applies a complex Cross-Encoder model on this reduced subset to maximize relevance (precision) without the latency cost of running the heavy model on the entire database.",
    "difficulty": "Advanced"
  },
  {
    "id": 71,
    "question": "When deploying a Large Language Model (LLM) using KV-cache optimization, what specific hardware bottleneck dictates the maximum context window size during inference?",
    "options": [
      "The compute capability of the CUDA cores for matrix multiplication",
      "The bandwidth of the HBM (High Bandwidth Memory) for loading weights",
      "The available VRAM capacity to store the Key and Value tensors for every generated token",
      "The clock speed of the Tensor Cores"
    ],
    "answer": "The available VRAM capacity to store the Key and Value tensors for every generated token",
    "explanation": "The KV cache stores attention states to prevent re-computation. As the context length grows, these caches grow linearly, eventually exhausting GPU VRAM, which is the hard limit on context window size independent of compute speed.",
    "difficulty": "Advanced"
  },
  {
    "id": 72,
    "question": "What distinguishes 'Shadow Deployment' from 'Canary Deployment' in MLOps?",
    "options": [
      "Shadow deployment serves traffic to a small percentage of real users to measure satisfaction",
      "Shadow deployment routes production traffic to the new model while discarding its responses",
      "Shadow deployment requires the new model to be 100% accurate before routing any traffic",
      "Shadow deployment uses a blue-green infrastructure setup where DNS is switched instantly"
    ],
    "answer": "Shadow deployment routes production traffic to the new model while discarding its responses",
    "explanation": "In shadow deployment, the new model receives a copy of real requests and processes them to measure latency and resource usage, but the response is never served to the user. Canary deployment actually serves the new model's response to a subset of users.",
    "difficulty": "Advanced"
  },
  {
    "id": 73,
    "question": "When implementing Parameter-Efficient Fine-Tuning (PEFT) using LoRA (Low-Rank Adaptation), which specific component of the neural network is modified?",
    "options": [
      "The attention heads' projection matrices are replaced with lower-rank matrices",
      "The weights of the pre-trained layers are frozen and trainable rank decomposition matrices are injected into each layer",
      "The final classification layer is re-initialized with random weights",
      "The embedding layer is expanded to include new domain-specific tokens"
    ],
    "answer": "The weights of the pre-trained layers are frozen and trainable rank decomposition matrices are injected into each layer",
    "explanation": "LoRA freezes the pre-trained model weights and injects trainable rank decomposition matrices (representing the weight delta) into the layers. This drastically reduces the number of trainable parameters while maintaining model performance.",
    "difficulty": "Advanced"
  },
  {
    "id": 74,
    "question": "Why is 'Mixed Precision' training (using FP16 or BF16 alongside FP32) primarily used to accelerate modern Deep Learning workloads?",
    "options": [
      "It reduces the size of the final model artifacts on disk",
      "It allows the model to fit onto CPUs that do not have a GPU",
      "It utilizes Tensor Cores to perform faster matrix operations and reduces memory bandwidth pressure",
      "It automatically increases the batch size by 4x"
    ],
    "answer": "It utilizes Tensor Cores to perform faster matrix operations and reduces memory bandwidth pressure",
    "explanation": "Modern GPUs (like NVIDIA Volta/Ampere) have Tensor Cores specifically designed for fast half-precision math. Additionally, storing activations in FP16/BF16 halves the memory footprint, allowing larger batch sizes or models within the same VRAM.",
    "difficulty": "Advanced"
  },
  {
    "id": 75,
    "question": "In the context of data drift monitoring, what is the key statistical difference between 'Covariate Shift' and 'Prior Probability Shift'?",
    "options": [
      "Covariate shift is a change in the distribution of the input features, while Prior shift is a change in the distribution of the target class",
      "Covariate shift is a change in the target labels, while Prior shift is a change in the correlation between features",
      "Covariate shift occurs only in online learning, while Prior shift occurs only in batch learning",
      "Covariate shift refers to model weight updates, while Prior shift refers to hyperparameter tuning"
    ],
    "answer": "Covariate shift is a change in the distribution of the input features, while Prior shift is a change in the distribution of the target class",
    "explanation": "Covariate shift means P(X) changes but P(Y|X) remains constant (input data drift). Prior probability shift means P(Y) changes (class balance changes), which is common in fraud detection when attack rates vary.",
    "difficulty": "Advanced"
  },
  {
    "id": 76,
    "question": "What is the primary function of the 'Flash Attention' mechanism in Transformer architectures?",
    "options": [
      "It discards the least important tokens to reduce the sequence length",
      "It recomputes attention scores during the backward pass to minimize HBM access and increase IO-awareness",
      "It replaces the softmax function with a ReLU activation for faster computation",
      "It compresses the key-value cache using quantization techniques"
    ],
    "answer": "It recomputes attention scores during the backward pass to minimize HBM access and increase IO-awareness",
    "explanation": "Standard attention materializes the large attention matrix in HBM (slow). Flash Attention uses tiling to keep the attention matrix in SRAM (fast) and recomputes parts of it during the backward pass, significantly speeding up training and inference.",
    "difficulty": "Advanced"
  },
  {
    "id": 77,
    "question": "When using 'Continuous Batch Processing' (also known as iteration-level batching) for LLM inference, how does scheduling differ from static batching?",
    "options": [
      "All requests in a batch must have the exact same sequence length",
      "The system waits for the batch to fill up completely before starting any processing",
      "Completed requests in a batch are removed immediately, allowing new requests to join the ongoing batch",
      "Requests are processed one by one in a first-in-first-out (FIFO) order"
    ],
    "answer": "Completed requests in a batch are removed immediately, allowing new requests to join the ongoing batch",
    "explanation": "In continuous batching, when a sequence in a batch finishes generating, its slot is immediately freed for a new incoming request. This maximizes GPU utilization compared to static batching where the whole batch waits for the longest sequence to finish.",
    "difficulty": "Advanced"
  },
  {
    "id": 78,
    "question": "In a distributed training setup using ZeRO (Zero Redundancy Optimizer), Stage 3 optimizes memory by...",
    "options": [
      "Partitioning the optimizer states, gradients, and model parameters across GPUs",
      "Keeping a full copy of the model on every GPU but partitioning only the training data",
      "Switching entirely to CPU offloading for all forward passes",
      "Removing the need for a gradient synchronization step"
    ],
    "answer": "Partitioning the optimizer states, gradients, and model parameters across GPUs",
    "explanation": "ZeRO Stage 3 shards the model parameters, gradients, and optimizer states across all available GPUs, effectively allowing the training of models with trillions of parameters that would not fit in a single GPU's memory.",
    "difficulty": "Advanced"
  },
  {
    "id": 79,
    "question": "What is the 'Training-Serving Skew' specifically referring to in MLOps?",
    "options": [
      "The difference in accuracy between the validation set and the test set",
      "The discrepancy between the data distribution or logic used during training and that encountered in the production environment",
      "The latency introduced by network communication between the training cluster and the serving cluster",
      "The variation in model performance due to different hardware architectures (e.g., NVIDIA vs AMD)"
    ],
    "answer": "The discrepancy between the data distribution or logic used during training and that encountered in the production environment",
    "explanation": "Training-serving skew occurs when the feature engineering code, data distributions, or handling of edge cases in production diverges from the pipeline used to train the model, leading to degraded performance in live traffic.",
    "difficulty": "Advanced"
  },
  {
    "id": 80,
    "question": "When implementing a Feature Store, what is the specific purpose of 'Point-in-Time' correctness?",
    "options": [
      "To ensure features are computed in real-time with millisecond latency",
      "To prevent data leakage by using only feature values that were available at the exact historical timestamp of the training label",
      "To synchronize the feature updates across multiple geographical regions",
      "To compress the feature storage format to save disk space"
    ],
    "answer": "To prevent data leakage by using only feature values that were available at the exact historical timestamp of the training label",
    "explanation": "Without point-in-time joins, a training query might accidentally use feature data from the future (relative to the label) because data was updated late. This bias causes the model to perform well in training but fail in production.",
    "difficulty": "Advanced"
  },
  {
    "id": 81,
    "question": "Which vector indexing algorithm offers the best trade-off between high recall and query speed for large-scale approximate nearest neighbor (ANN) search?",
    "options": [
      "Flat Index (Brute Force)",
      "Hierarchical Navigable Small World (HNSW) Graphs",
      "Inverted File Index (IVF) without quantization",
      "Locality Sensitive Hashing (LSH)"
    ],
    "answer": "Hierarchical Navigable Small World (HNSW) Graphs",
    "explanation": "HNSW creates a multi-layer graph structure allowing for logarithmic search complexity. It typically provides superior recall and speed compared to IVF or LSH for high-dimensional dense vectors, despite using more memory.",
    "difficulty": "Advanced"
  },
  {
    "id": 82,
    "question": "In the context of LLM quantization, what is the definition of 'Zero-point Quantization'?",
    "options": [
      "Quantizing weights to integers without an offset, assuming the data is centered around zero",
      "Mapping floating-point values to integers using an affine transformation: scale * x + zero_point",
      "A technique where quantized errors are accumulated and corrected in subsequent layers",
      "Reducing the model size to zero by removing all layers"
    ],
    "answer": "Mapping floating-point values to integers using an affine transformation: scale * x + zero_point",
    "explanation": "Zero-point quantization introduces an offset (zero_point) to shift the integer range, effectively handling distributions that are not centered perfectly around zero, unlike symmetric quantization which forces the zero-point to be zero.",
    "difficulty": "Advanced"
  },
  {
    "id": 83,
    "question": "What is the primary mechanism by which 'Speculative Decoding' (also known as draft decoding) accelerates LLM inference?",
    "options": [
      "It uses a smaller model to predict multiple tokens quickly, which are then verified in parallel by the larger model",
      "It skips the decoding layers and directly predicts the final embedding",
      "It compresses the prompt before sending it to the model",
      "It dynamically switches to a lower precision format during generation"
    ],
    "answer": "It uses a smaller model to predict multiple tokens quickly, which are then verified in parallel by the larger model",
    "explanation": "A small 'draft' model generates a sequence of tokens. The large 'target' model processes them in a single forward pass to verify probability. If accepted, generation speed approaches the speed of the small model.",
    "difficulty": "Advanced"
  },
  {
    "id": 84,
    "question": "Why is the 'BLEU' score generally considered insufficient for evaluating modern generative AI models compared to 'BERTScore' or embedding-based metrics?",
    "options": [
      "BLEU requires a human to manually review every output",
      "BLEU relies solely on exact n-gram overlap and fails to capture semantic similarity or paraphrasing",
      "BLEU is too computationally expensive to calculate on large datasets",
      "BLEU only works for images and not text"
    ],
    "answer": "BLEU relies solely on exact n-gram overlap and fails to capture semantic similarity or paraphrasing",
    "explanation": "BLEU measures strict lexical overlap. It penalizes valid paraphrases that use different words but convey the same meaning, whereas embedding-based metrics (like BERTScore) measure semantic distance in vector space.",
    "difficulty": "Advanced"
  },
  {
    "id": 85,
    "question": "When configuring a Hyperparameter Tuning job, what distinguishes 'Bayesian Optimization' from 'Random Search'?",
    "options": [
      "Bayesian optimization samples random points without any knowledge of previous trials",
      "Bayesian optimization builds a probabilistic model of the objective function to select the next most promising hyperparameters",
      "Bayesian optimization requires a grid of all possible hyperparameter combinations",
      "Bayesian optimization is only applicable to deep learning models and not tree-based models"
    ],
    "answer": "Bayesian optimization builds a probabilistic model of the objective function to select the next most promising hyperparameters",
    "explanation": "Unlike random or grid search, Bayesian optimization keeps track of past evaluation results to model the performance landscape (surrogate function), allowing it to focus the search on hyperparameter regions likely to yield better results.",
    "difficulty": "Advanced"
  },
  {
    "id": 86,
    "question": "In Transformer architecture, why is 'Rotary Positional Embeddings' (RoPE) generally preferred over 'Learned Absolute Embeddings' for long-context models?",
    "options": [
      "RoPE uses less GPU memory because it is computed on the fly rather than stored as a lookup table",
      "RoPE allows the model to train on sequences of infinite length without any positional information",
      "RoPE encodes relative position information naturally into the query-key dot product attention mechanism",
      "RoPE eliminates the need for the causal mask in the decoder"
    ],
    "answer": "RoPE encodes relative position information naturally into the query-key dot product attention mechanism",
    "explanation": "RoPE rotates the query and key vectors in geometric space based on their position indices. This allows the attention score to decay as relative distance increases, effectively injecting relative position invariance without adding parameters.",
    "difficulty": "Advanced"
  },
  {
    "id": 87,
    "question": "What is the 'Linear Attention' mechanism proposed to solve regarding standard Transformers?",
    "options": [
      "It reduces the vanishing gradient problem in very deep networks",
      "It reduces the quadratic complexity O(N^2) of the attention mechanism to linear complexity O(N) with respect to sequence length",
      "It converts the non-linear activation functions to linear ones for faster inference",
      "It decreases the memory required to store the model weights"
    ],
    "answer": "It reduces the quadratic complexity O(N^2) of the attention mechanism to linear complexity O(N) with respect to sequence length",
    "explanation": "Standard attention computes an N x N matrix. Linear attention (often via kernel tricks) reorders the matrix multiplication associativity to remove the dependency on the sequence length in the intermediate step, enabling much longer context windows.",
    "difficulty": "Advanced"
  },
  {
    "id": 88,
    "question": "What is the specific risk of 'Reward Hacking' in Reinforcement Learning from Human Feedback (RLHF) for LLMs?",
    "options": [
      "The model learns to generate text that maximizes the reward signal but is nonsensical or adversarial to the human intent (gaming the reward model)",
      "The human annotators run out of time and stop providing feedback",
      "The model forgets previously learned languages and regresses to English",
      "The reward model overfits to the training data and loses generalization"
    ],
    "answer": "The model learns to generate text that maximizes the reward signal but is nonsensical or adversarial to the human intent (gaming the reward model)",
    "explanation": "If the reward model is imperfect, the policy (LLM) may find specific outputs that trick the reward model into assigning high scores without actually being helpful or truthful, leading to a degeneration of output quality.",
    "difficulty": "Advanced"
  },
  {
    "id": 89,
    "question": "Which MLOps pattern is specifically designed to handle 'Concept Drift' where the relationship between input data and the target variable changes over time?",
    "options": [
      "Static model versioning",
      "Continuous retraining pipelines with trigger mechanisms based on performance metrics",
      "Data versioning using DVC",
      "One-off manual model redeployment"
    ],
    "answer": "Continuous retraining pipelines with trigger mechanisms based on performance metrics",
    "explanation": "Concept drift means the world has changed (e.g., consumer behavior). To handle this, the system needs a CI/CD pipeline that automatically retrains the model when metrics (like precision/recall) or data drift indicators fall below a threshold.",
    "difficulty": "Advanced"
  },
  {
    "id": 90,
    "question": "What is the function of 'Group Query Attention' (GQA) or 'Multi-Query Attention' (MQA) in LLMs?",
    "options": [
      "To increase the accuracy of the attention calculation by using more heads",
      "To reduce inference latency and memory bandwidth usage by sharing Key and Value projections across multiple attention heads",
      "To allow the model to process images and text in the same modality",
      "To divide the input sequence into groups to enable parallel processing across different GPUs"
    ],
    "answer": "To reduce inference latency and memory bandwidth usage by sharing Key and Value projections across multiple attention heads",
    "explanation": "Standard Multi-Head Attention (MHA) has separate K/V projections for every head, consuming huge memory bandwidth. MQA/QGA share a single or limited set of K/V heads among all query heads, drastically speeding up decoding.",
    "difficulty": "Advanced"
  },
  {
    "id": 91,
    "question": "When implementing a 'Model Card' or 'Data Card', what is the primary engineering goal?",
    "options": [
      "To document the marketing claims of the model for the sales team",
      "To provide transparent documentation of intended use, limitations, and performance metrics across different demographic groups",
      "To serve as the user manual for the end-user application",
      "To list the specific hyperparameters used in the training run"
    ],
    "answer": "To provide transparent documentation of intended use, limitations, and performance metrics across different demographic groups",
    "explanation": "Model Cards are standardized documentation (inspired by nutrition labels) designed to inform stakeholders (engineers, users, auditors) about what the model does, where it shouldn't be used, and potential biases.",
    "difficulty": "Advanced"
  },
  {
    "id": 92,
    "question": "In the context of Prompt Engineering, what is the 'Few-Shot CoT' (Chain of Thought) technique designed to improve?",
    "options": [
      "The creativity of the generated stories",
      "The ability of the model to perform complex reasoning and arithmetic by showing intermediate steps in the prompt examples",
      "The token speed of the generation",
      "The compression of the context window"
    ],
    "answer": "The ability of the model to perform complex reasoning and arithmetic by showing intermediate steps in the prompt examples",
    "explanation": "Few-Shot CoT provides the model with examples that explicitly show the reasoning process (e.g., 'Let's think step by step') rather than just the final answer, guiding the model to decompose complex problems into manageable steps.",
    "difficulty": "Advanced"
  },
  {
    "id": 93,
    "question": "What is a 'Dictionary Attack' in the context of LLM prompt injection security?",
    "options": [
      "Brute-forcing the API key of the hosted model",
      "Using a known set of malicious prompts (a 'jailbreak' dictionary) to probe the model's safety guardrails",
      "Encrypting the prompt so the model cannot read it",
      "Flooding the API with requests to cause a Denial of Service"
    ],
    "answer": "Using a known set of malicious prompts (a 'jailbreak' dictionary) to probe the model's safety guardrails",
    "explanation": "Adversaries maintain lists of prompts known to bypass safety filters (e.g., DAN, Roleplay scenarios). They iterate through this dictionary to test if a specific deployed model is vulnerable to these specific bypasses.",
    "difficulty": "Advanced"
  },
  {
    "id": 94,
    "question": "Which technique explicitly attempts to prevent 'Catastrophic Forgetting' when fine-tuning a Large Language Model on new tasks?",
    "options": [
      "Learning Rate Decay",
      "Elastic Weight Consolidation (EWC) or Experience Replay",
      "Gradient Clipping",
      "Data Augmentation"
    ],
    "answer": "Elastic Weight Consolidation (EWC) or Experience Replay",
    "explanation": "Catastrophic forgetting occurs when learning new data overwrites previously learned knowledge. EWC penalizes changes to important weights for old tasks, while Experience Replay mixes old data with new data during training to retain memory.",
    "difficulty": "Advanced"
  },
  {
    "id": 95,
    "question": "What is the primary disadvantage of using 'Greedy Decoding' (temperature=0) for LLM text generation?",
    "options": [
      "It is computationally too expensive for real-time applications",
      "It produces highly repetitive and deterministic outputs that can get stuck in loops",
      "It generates hallucinations more frequently than high-temperature sampling",
      "It requires a massive context window to function"
    ],
    "answer": "It produces highly repetitive and deterministic outputs that can get stuck in loops",
    "explanation": "Greedy decoding always picks the single most probable next token. This often leads to repetitive loops (e.g., 'I am I am I am') because the model cannot take a lower-probability path to escape the cycle.",
    "difficulty": "Advanced"
  },
  {
    "id": 96,
    "question": "When implementing 'Differential Privacy' in ML training, what is the effect of the 'Privacy Budget' (epsilon)?",
    "options": [
      "It determines the cost of GPU hours required for training",
      "A lower epsilon implies stronger privacy guarantees but usually degrades model accuracy",
      "A higher epsilon increases the size of the training dataset",
      "It limits the number of users who can query the model"
    ],
    "answer": "A lower epsilon implies stronger privacy guarantees but usually degrades model accuracy",
    "explanation": "Epsilon measures the privacy loss. A lower epsilon means the output of the model reveals very little about any specific individual's data (high privacy), but the noise added to achieve this often reduces the model's utility or accuracy.",
    "difficulty": "Advanced"
  },
  {
    "id": 97,
    "question": "In MLOps, what is the primary benefit of 'Feature Parity' between training and inference?",
    "options": [
      "It ensures that the model can be deployed on multiple cloud providers",
      "It eliminates training-serving skew by ensuring transformation logic is identical in both phases",
      "It ensures that the features are stored in the same database format",
      "It guarantees that the model training time is reduced"
    ],
    "answer": "It eliminates training-serving skew by ensuring transformation logic is identical in both phases",
    "explanation": "If you normalize data or compute features differently during inference than you did during training (e.g., online vs. batch calculation), the model receives inputs outside its expected distribution, causing errors. Parity ensures the transformation code is shared.",
    "difficulty": "Advanced"
  },
  {
    "id": 98,
    "question": "What defines 'Compound AI Systems' compared to standalone 'Monolithic Models'?",
    "options": [
      "Systems that use multiple smaller models to replace one large model",
      "Systems that solve tasks using a pipeline involving multiple interacting components: models, retrievers, tools, and schedulers",
      "Systems where the weights of the neural network are physically located across different servers",
      "Systems that are trained using reinforcement learning exclusively"
    ],
    "answer": "Systems that solve tasks using a pipeline involving multiple interacting components: models, retrievers, tools, and schedulers",
    "explanation": "Monolithic models rely on a single LLM. Compound systems (like RAG or Agentic workflows) orchestrate calls to models, external APIs, vector databases, and logic controllers to achieve tasks that a single model cannot perform reliably alone.",
    "difficulty": "Advanced"
  },
  {
    "id": 99,
    "question": "What is the 'Latent Space' in the context of Deep Generative Models (like VAEs or GANs)?",
    "options": [
      "The memory space on the GPU used during training",
      "A lower-dimensional representation of the data where the model learns efficient features and can sample to generate new outputs",
      "The unprocessed raw input data space",
      "The gap between training and inference latency"
    ],
    "answer": "A lower-dimensional representation of the data where the model learns efficient features and can sample to generate new outputs",
    "explanation": "The latent space (or bottleneck) captures the compressed, essential features of the input data. By sampling points in this continuous space and decoding them, the model can generate new, synthetic data instances.",
    "difficulty": "Advanced"
  }
]