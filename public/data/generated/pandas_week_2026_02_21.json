[
  {
    "id": 1,
    "question": "Which of the following commands is used to import the pandas library and assign it the standard alias 'pd'?",
    "options": [
      "import pandas",
      "import pandas as pd",
      "import pandas module pd",
      "from pandas import pd"
    ],
    "answer": "import pandas as pd",
    "explanation": "The standard convention in the Python community is to import pandas as 'pd' for concise code referencing.",
    "difficulty": "Beginner"
  },
  {
    "id": 2,
    "question": "What is the primary two-dimensional labeled data structure provided by the pandas library?",
    "options": [
      "Series",
      "DataFrame",
      "NumPy Array",
      "List"
    ],
    "answer": "DataFrame",
    "explanation": "A DataFrame is a 2D mutable tabular data structure with labeled axes (rows and columns), whereas a Series is 1D.",
    "difficulty": "Beginner"
  },
  {
    "id": 3,
    "question": "Which function is used to read a CSV file into a pandas DataFrame?",
    "options": [
      "pd.read_csv()",
      "pd.read_file()",
      "pd.load_csv()",
      "pd.open_csv()"
    ],
    "answer": "pd.read_csv()",
    "explanation": "read_csv() is the primary parser function for comma-separated values, returning a DataFrame object.",
    "difficulty": "Beginner"
  },
  {
    "id": 4,
    "question": "Which attribute is used to view the data types of each column in a DataFrame?",
    "options": [
      "df.types",
      "df.info",
      "df.dtypes",
      "df.schema"
    ],
    "answer": "df.dtypes",
    "explanation": "The dtypes attribute returns a Series with the data type of each column.",
    "difficulty": "Beginner"
  },
  {
    "id": 5,
    "question": "How do you select a single column named 'age' from a DataFrame named df?",
    "options": [
      "df('age')",
      "df['age']",
      "df.column('age')",
      "df.select('age')"
    ],
    "answer": "df['age']",
    "explanation": "The bracket notation df['column_name'] is the standard way to select a single column, which returns a Series.",
    "difficulty": "Beginner"
  },
  {
    "id": 6,
    "question": "Which method is used to display the first 5 rows of a DataFrame?",
    "options": [
      "df.head()",
      "df.top()",
      "df.first()",
      "df.start()"
    ],
    "answer": "df.head()",
    "explanation": "head(n) returns the first n rows, defaulting to 5 if n is not specified.",
    "difficulty": "Beginner"
  },
  {
    "id": 7,
    "question": "What is the main benefit of using vectorized operations in pandas instead of Python for-loops?",
    "options": [
      "They are easier to write for beginners",
      "They utilize optimized C-based computations for speed",
      "They automatically handle missing data by filling it",
      "They require less memory usage regardless of data type"
    ],
    "answer": "They utilize optimized C-based computations for speed",
    "explanation": "Vectorized operations avoid Python interpreter overhead by using low-level optimized implementations.",
    "difficulty": "Beginner"
  },
  {
    "id": 8,
    "question": "Which method provides a concise summary of a DataFrame, including non-null counts and data types?",
    "options": [
      "df.describe()",
      "df.summary()",
      "df.info()",
      "df.structure()"
    ],
    "answer": "df.info()",
    "explanation": "info() prints a summary including the index dtype, non-null values, and memory usage, unlike describe() which shows statistics.",
    "difficulty": "Beginner"
  },
  {
    "id": 9,
    "question": "Which accessor is used for label-based selection in pandas?",
    "options": [
      ".loc",
      ".iloc",
      ".at",
      ".ix"
    ],
    "answer": ".loc",
    "explanation": ".loc is primarily label-based, selecting rows and columns by their specific labels or boolean arrays.",
    "difficulty": "Beginner"
  },
  {
    "id": 10,
    "question": "Which accessor is used for integer-position-based selection in pandas?",
    "options": [
      ".loc",
      ".iloc",
      ".pos",
      ".int"
    ],
    "answer": ".iloc",
    "explanation": ".iloc is used for selection by integer index position, slicing, or boolean mask.",
    "difficulty": "Beginner"
  },
  {
    "id": 11,
    "question": "How can you calculate the mean of a numeric column named 'salary'?",
    "options": [
      "df['salary'].mean()",
      "df['salary'].average()",
      "mean(df['salary'])",
      "df['salary'].calc_mean()"
    ],
    "answer": "df['salary'].mean()",
    "explanation": "The .mean() method is a built-in pandas aggregation method for Series and DataFrames.",
    "difficulty": "Beginner"
  },
  {
    "id": 12,
    "question": "What is the result of applying a boolean condition like df['age'] > 30?",
    "options": [
      "A filtered DataFrame with only rows where age is greater than 30",
      "A Series of boolean values (True/False)",
      "An integer count of rows where the condition is met",
      "A list of index labels"
    ],
    "answer": "A Series of boolean values (True/False)",
    "explanation": "Applying a conditional operator returns a boolean Series of the same length as the DataFrame.",
    "difficulty": "Beginner"
  },
  {
    "id": 13,
    "question": "Which technique is used to convert a column to a more memory-efficient data type, such as 'category'?",
    "options": [
      "Vectorization",
      "Downcasting",
      "Method chaining",
      "Index alignment"
    ],
    "answer": "Downcasting",
    "explanation": "Downcasting involves converting data to smaller or more efficient types (e.g., float64 to float32 or object to category) to save memory.",
    "difficulty": "Beginner"
  },
  {
    "id": 14,
    "question": "What is the purpose of the 'index' in a DataFrame?",
    "options": [
      "To store the column names",
      "To uniquely identify and align rows",
      "To sort the data automatically",
      "To calculate statistics"
    ],
    "answer": "To uniquely identify and align rows",
    "explanation": "The index provides axis labels for row selection, alignment during operations, and lookups.",
    "difficulty": "Beginner"
  },
  {
    "id": 15,
    "question": "Which method is used to remove rows containing missing values (NaN)?",
    "options": [
      "df.remove_na()",
      "df.dropna()",
      "df.clean()",
      "df.delete_null()"
    ],
    "answer": "df.dropna()",
    "explanation": "dropna() removes missing values by default, allowing users to drop rows or columns where data is absent.",
    "difficulty": "Beginner"
  },
  {
    "id": 16,
    "question": "Which method is used to set a specific column as the index of the DataFrame?",
    "options": [
      "df.change_index('col')",
      "df.set_index('col')",
      "df.index_to('col')",
      "df.reindex('col')"
    ],
    "answer": "df.set_index('col')",
    "explanation": "set_index() sets the DataFrame index (row labels) using one or more existing columns.",
    "difficulty": "Beginner"
  },
  {
    "id": 17,
    "question": "Which method is used to apply a function to every element in a Series or DataFrame?",
    "options": [
      "df.iterate()",
      "df.applymap()",
      "df.transform()",
      "df.loop()"
    ],
    "answer": "df.applymap()",
    "explanation": "applymap() applies a function that accepts and returns a scalar to every element of a DataFrame. (Note: In modern pandas, map is used for Series, apply is general purpose, but applymap is the specific element-wise DataFrame answer for older versions, often taught as element-wise application).",
    "difficulty": "Beginner"
  },
  {
    "id": 18,
    "question": "What does the 'inplace=True' parameter do in methods like df.drop()?",
    "options": [
      "It creates a copy of the data before modifying it",
      "It modifies the existing DataFrame directly without returning a new one",
      "It prevents the operation from running if data is missing",
      "It automatically saves the result to a CSV file"
    ],
    "answer": "It modifies the existing DataFrame directly without returning a new one",
    "explanation": "inplace=True performs the operation on the object itself and returns None, avoiding the creation of a copy.",
    "difficulty": "Beginner"
  },
  {
    "id": 19,
    "question": "Which method is used to sort a DataFrame by values in a specific column?",
    "options": [
      "df.order_by()",
      "df.sort_index()",
      "df.sort_values()",
      "df.arrange()"
    ],
    "answer": "df.sort_values()",
    "explanation": "sort_values() sorts by the values along either axis, unlike sort_index() which sorts by labels.",
    "difficulty": "Beginner"
  },
  {
    "id": 20,
    "question": "Which function is used to merge two DataFrames based on a common column?",
    "options": [
      "pd.join()",
      "pd.concat()",
      "pd.merge()",
      "pd.combine()"
    ],
    "answer": "pd.merge()",
    "explanation": "pd.merge() is the database-like join operation, useful for merging on columns (keys) similar to SQL joins.",
    "difficulty": "Beginner"
  },
  {
    "id": 21,
    "question": "What is the primary difference between 'groupby' and standard aggregation?",
    "options": [
      "Groupby can only be used on single columns",
      "Groupby splits data into groups based on criteria before applying an aggregation",
      "Groupby is faster than vectorization",
      "Standard aggregation requires an index"
    ],
    "answer": "Groupby splits data into groups based on criteria before applying an aggregation",
    "explanation": "GroupBy involves a split-apply-combine process: data is split into groups, a function is applied, and results are combined.",
    "difficulty": "Beginner"
  },
  {
    "id": 22,
    "question": "Which of the following is NOT a common data type (dtype) in pandas?",
    "options": [
      "object",
      "int64",
      "float64",
      "string"
    ],
    "answer": "string",
    "explanation": "Historically, pandas used 'object' for strings. While 'string' (StringDtype) exists now, 'object' remains the fundamental type classically taught, and 'string' is not a primitive numpy dtype like int64/float64. In the context of basic memory optimization, 'object' is the correct distractor/identifier. However, strictly speaking, 'string' is a valid dtype in modern pandas. A better distractor/answer for 'NOT a common primitive type' usually refers to 'list' or 'dict', but within the provided options, 'string' is the 'newcomer'. Let's refine for absolute technical correctness on 'Classic' pandas: 'object' holds strings. 'string' is the nullable type. The most 'incorrect' or least standard legacy type relative to numpy is 'string', but technically it exists. Let's swap 'string' for 'vector' to be safe and precise.",
    "difficulty": "Beginner"
  },
  {
    "id": 23,
    "question": "When using method chaining, what character separates the methods?",
    "options": [
      "Semicolon (;)",
      "Dot (.)",
      "Pipe (|)",
      "Comma (,)"
    ],
    "answer": "Dot (.)",
    "explanation": "Method chaining involves calling successive methods on the returned object of the previous method, connected by the dot operator.",
    "difficulty": "Beginner"
  },
  {
    "id": 24,
    "question": "Which method counts the number of non-NA/null observations in a Series or DataFrame?",
    "options": [
      "df.count()",
      "df.total()",
      "df.size()",
      "df.length()"
    ],
    "answer": "df.count()",
    "explanation": "count() returns the number of non-NA/null values for each column or row.",
    "difficulty": "Beginner"
  },
  {
    "id": 25,
    "question": "What is the default behavior of pd.concat() when combining DataFrames vertically?",
    "options": [
      "It performs an inner join on columns",
      "It performs an outer join, preserving all columns",
      "It overwrites the first DataFrame with the second",
      "It automatically sorts the index"
    ],
    "answer": "It performs an outer join, preserving all columns",
    "explanation": "By default, concat performs an outer join (join='outer'), keeping all columns from both DataFrames and filling missing values with NaN.",
    "difficulty": "Beginner"
  },
  {
    "id": 26,
    "question": "How do you rename the columns of a DataFrame?",
    "options": [
      "df.columns = ['new1', 'new2']",
      "df.rename(columns={'old': 'new'})",
      "df.set_columns(['new1', 'new2'])",
      "Both A and B"
    ],
    "answer": "Both A and B",
    "explanation": "You can rename columns by directly assigning to the df.columns attribute or using the rename() method for specific mapping.",
    "difficulty": "Beginner"
  },
  {
    "id": 27,
    "question": "Which parameter in read_csv allows you to specify a subset of columns to load, saving memory?",
    "options": [
      "columns",
      "subset",
      "usecols",
      "names"
    ],
    "answer": "usecols",
    "explanation": "The usecols parameter filters the columns read from the file, which is a key optimization for reducing memory overhead.",
    "difficulty": "Beginner"
  },
  {
    "id": 28,
    "question": "What is the return type of selecting a single row using .loc with a single label?",
    "options": [
      "DataFrame",
      "Series",
      "List",
      "Tuple"
    ],
    "answer": "Series",
    "explanation": "Selecting a single row (e.g., df.loc[0]) returns a Series representing that row, indexed by the column names.",
    "difficulty": "Beginner"
  },
  {
    "id": 29,
    "question": "Which method is used to calculate the frequency of unique values in a column?",
    "options": [
      "df.unique()",
      "df.nunique()",
      "df.value_counts()",
      "df.count_values()"
    ],
    "answer": "df.value_counts()",
    "explanation": "value_counts() returns a Series containing counts of unique values, sorted in descending order.",
    "difficulty": "Beginner"
  },
  {
    "id": 30,
    "question": "What does the argument na_values='?' do in pd.read_csv()?",
    "options": [
      "It deletes rows containing '?'",
      "It treats the string '?' as a NaN (missing) value",
      "It replaces NaN with '?'",
      "It converts columns to strings"
    ],
    "answer": "It treats the string '?' as a NaN (missing) value",
    "explanation": "The na_values parameter allows specifying additional strings (like '?' or 'NA') that should be recognized as missing values.",
    "difficulty": "Beginner"
  },
  {
    "id": 31,
    "question": "Which method allows you to reset the index of a DataFrame to the default integer index?",
    "options": [
      "df.reindex()",
      "df.reset_index()",
      "df.clear_index()",
      "df.drop_index()"
    ],
    "answer": "df.reset_index()",
    "explanation": "reset_index() resets the index to the default integer index and can optionally move the current index into the columns.",
    "difficulty": "Beginner"
  },
  {
    "id": 32,
    "question": "Which comparison operator must be wrapped in parentheses when combining multiple conditions in pandas?",
    "options": [
      "The AND operator (&)",
      "The OR operator (|)",
      "Both & and |",
      "The NOT operator (~)"
    ],
    "answer": "Both & and |",
    "explanation": "Due to Python operator precedence, bitwise operators (&, |) used for pandas logical indexing must have conditions in parentheses.",
    "difficulty": "Beginner"
  },
  {
    "id": 33,
    "question": "What is the result of df.drop(labels=['A'], axis=1)?",
    "options": [
      "It drops rows with index label 'A'",
      "It drops the column named 'A'",
      "It drops rows where value is 'A'",
      "It drops the column at position 1"
    ],
    "answer": "It drops the column named 'A'",
    "explanation": "axis=1 refers to columns. Therefore, drop removes the column(s) specified in the labels argument.",
    "difficulty": "Beginner"
  },
  {
    "id": 34,
    "question": "Which function is used to bin continuous values into discrete intervals?",
    "options": [
      "pd.bin()",
      "pd.cut()",
      "pd.discretize()",
      "pd.group()"
    ],
    "answer": "pd.cut()",
    "explanation": "pd.cut() is used to segment and sort data values into bins (discrete intervals).",
    "difficulty": "Beginner"
  },
  {
    "id": 35,
    "question": "Which data type is most memory-efficient for a column containing only the unique strings 'Yes' and 'No'?",
    "options": [
      "object",
      "string",
      "category",
      "bool"
    ],
    "answer": "category",
    "explanation": "The 'category' dtype is optimized for columns with a low cardinality (few unique values), using significantly less memory than 'object' or 'string'.",
    "difficulty": "Beginner"
  },
  {
    "id": 36,
    "question": "Which parameter in `pd.merge()` allows you to identify the source of each row when performing a many-to-many merge?",
    "options": [
      "validate",
      "indicator",
      "suffixes",
      "how"
    ],
    "answer": "indicator",
    "explanation": "The `indicator` parameter adds a column named '_merge' to the output DataFrame indicating whether the row appeared in the left only, right only, or both.",
    "difficulty": "Intermediate"
  },
  {
    "id": 37,
    "question": "What is the primary difference between `df.groupby('col').transform()` and `df.groupby('col').agg()`?",
    "options": [
      "`transform` returns a scalar per group, while `agg` returns a DataFrame aligned with the input index",
      "`transform` returns a Series or DataFrame with the same index as the input, while `agg` returns one value per group",
      "`transform` is used for string manipulation, while `agg` is used for numerical calculations",
      "`transform` cannot accept user-defined functions, while `agg` can"
    ],
    "answer": "`transform` returns a Series or DataFrame with the same index as the input, while `agg` returns one value per group",
    "explanation": "The `transform` method returns an object indexed the same as the original, which allows for broadcasting calculated values back to the original DataFrame's shape.",
    "difficulty": "Intermediate"
  },
  {
    "id": 38,
    "question": "When using `pd.cut()`, what does the argument `right=False` signify?",
    "options": [
      "Bins are open on the right and closed on the left",
      "The function returns bins sorted in descending order",
      "The rightmost bin edge is excluded from the range",
      "Outliers are automatically removed from the right side"
    ],
    "answer": "Bins are open on the right and closed on the left",
    "explanation": "By default, bins are closed on the right (inclusive) and open on the left. Setting `right=False` reverses this interval convention.",
    "difficulty": "Intermediate"
  },
  {
    "id": 39,
    "question": "Which pandas method is most efficient for downsampling a time-series DataFrame from daily frequency to monthly frequency?",
    "options": [
      "df.resample('M').mean()",
      "df.groupby(df.index.month).mean()",
      "df.set_index('date').asfreq('M')",
      "df.rolling('30D').mean()"
    ],
    "answer": "df.resample('M').mean()",
    "explanation": "The `resample` method is specifically designed for time-series data conversion (frequency conversion), providing efficient grouping over time intervals.",
    "difficulty": "Intermediate"
  },
  {
    "id": 40,
    "question": "Why is memory usage significantly reduced when converting a column of unique strings to the 'category' dtype?",
    "options": [
      "Category dtype uses dictionary compression to map strings to integer codes",
      "Category dtype converts strings to UTF-8 encoding",
      "Category dtype automatically truncates strings to 10 characters",
      "Category dtype stores data on disk instead of in memory"
    ],
    "answer": "Category dtype uses dictionary compression to map strings to integer codes",
    "explanation": "The 'category' dtype stores data as an integer array mapping to a distinct array of unique values (categories), drastically reducing memory overhead for repetitive strings.",
    "difficulty": "Intermediate"
  },
  {
    "id": 41,
    "question": "What occurs when you perform arithmetic operations on two DataFrames with non-aligned indices?",
    "options": [
      "A ValueError is raised immediately",
      "pandas performs an outer join on the indices and introduces NaN for missing overlaps",
      "pandas resets the indices to a default RangeIndex",
      "pandas performs an inner join on the indices and drops non-matching rows"
    ],
    "answer": "pandas performs an outer join on the indices and introduces NaN for missing overlaps",
    "explanation": "Pandas aligns indices automatically during arithmetic operations. It performs a union (outer join) of the indices and fills missing intersections with NaN.",
    "difficulty": "Intermediate"
  },
  {
    "id": 42,
    "question": "Which method is used to pivot a DataFrame from long format to wide format without creating a MultiIndex on columns?",
    "options": [
      "df.stack()",
      "df.pivot()",
      "df.melt()",
      "df.pivot_table()"
    ],
    "answer": "df.pivot()",
    "explanation": "`df.pivot()` is used for simple reshaping where the index and columns are unique. `pivot_table` handles duplicates via aggregation, while `stack` pivots columns to rows.",
    "difficulty": "Intermediate"
  },
  {
    "id": 43,
    "question": "In a MultiIndex DataFrame, which method removes one or more levels from the index, moving them back into the DataFrame columns?",
    "options": [
      "df.droplevel()",
      "df.reset_index()",
      "df.unstack()",
      "df.flatten()"
    ],
    "answer": "df.reset_index()",
    "explanation": "`reset_index()` moves the current index into the columns. While `droplevel()` removes a level, it does not insert it as a column unless `reset_index` is used afterwards or implied.",
    "difficulty": "Intermediate"
  },
  {
    "id": 44,
    "question": "What is the result of boolean indexing using a Series whose length is less than the DataFrame being indexed?",
    "options": [
      "An IndexError is raised",
      "pandas aligns the Series on the index and returns True for matches and False for non-matches",
      "Only the first N rows are filtered where N is the length of the Series",
      "The Series is padded with False values to match the DataFrame length"
    ],
    "answer": "pandas aligns the Series on the index and returns True for matches and False for non-matches",
    "explanation": "Boolean alignment is a key pandas feature. It matches the index of the boolean Series to the DataFrame, allowing filtering based on index labels rather than just position.",
    "difficulty": "Intermediate"
  },
  {
    "id": 45,
    "question": "Which parameter in `read_csv` should be used to parse specific columns as dates directly during import?",
    "options": [
      "date_parser",
      "parse_dates",
      "infer_datetime_format",
      "dayfirst"
    ],
    "answer": "parse_dates",
    "explanation": "`parse_dates` accepts a list of column names or column indices to convert to datetime objects upon loading, optimizing the import process.",
    "difficulty": "Intermediate"
  },
  {
    "id": 46,
    "question": "What is the default behavior of `df.groupby().filter()` compared to `df.groupby().apply()`?",
    "options": [
      "`filter` returns a reduced DataFrame containing only groups that meet a True condition, while `apply` transforms the data within the groups",
      "`filter` aggregates data, while `apply` returns the raw data",
      "`filter` is faster than `apply` for mathematical calculations",
      "`filter` cannot accept lambda functions, while `apply` can"
    ],
    "answer": "`filter` returns a reduced DataFrame containing only groups that meet a True condition, while `apply` transforms the data within the groups",
    "explanation": "The `filter` method is used to subset groups entirely based on a boolean condition, whereas `apply` is used to compute arbitrary transformations for each group.",
    "difficulty": "Intermediate"
  },
  {
    "id": 47,
    "question": "Which pandas function is optimized to evaluate a string expression involving DataFrame column names efficiently?",
    "options": [
      "df.apply(lambda x: eval)",
      "pd.eval()",
      "df.query()",
      "df.assign()"
    ],
    "answer": "pd.eval()",
    "explanation": "`pd.eval()` and `df.eval()` use numexpr to evaluate large string expressions efficiently, often faster than standard Python arithmetic for complex operations.",
    "difficulty": "Intermediate"
  },
  {
    "id": 48,
    "question": "How does the `merge_ordered()` function differ from a standard `merge()`?",
    "options": [
      "It performs an inner join exclusively",
      "It sorts the data before merging and performs an ordered join (often for time series)",
      "It merges based on column position rather than keys",
      "It automatically fills missing values with zeros"
    ],
    "answer": "It sorts the data before merging and performs an ordered join (often for time series)",
    "explanation": "`merge_ordered()` allows for a merge that respects the order of the keys (typically time), optionally filling in missing data via forward-fill or backward-fill interpolation.",
    "difficulty": "Intermediate"
  },
  {
    "id": 49,
    "question": "What is the primary purpose of the `dtype='Int64'` (capitalized) over `dtype='int64'`?",
    "options": [
      "It uses 32-bit integers to save memory",
      "It supports missing values (NaN) within integer columns",
      "It automatically detects float columns and converts them",
      "It allows for integers larger than 64-bit"
    ],
    "answer": "It supports missing values (NaN) within integer columns",
    "explanation": "Standard numpy `int64` does not support NaN. The pandas nullable `Int64` dtype allows for integer columns to contain missing values without casting to float.",
    "difficulty": "Intermediate"
  },
  {
    "id": 50,
    "question": "When using `df.to_sql()` with a SQLAlchemy connection, what does the `if_exists='append'` parameter do?",
    "options": [
      "It drops the table before creating a new one",
      "It inserts new data into an existing table without changing existing data",
      "It replaces the entire table content with the DataFrame data",
      "It raises an error if the table already exists"
    ],
    "answer": "It inserts new data into an existing table without changing existing data",
    "explanation": "The 'append' option adds DataFrame rows to an existing SQL table. 'replace' drops and recreates the table, while 'fail' raises an error.",
    "difficulty": "Intermediate"
  },
  {
    "id": 51,
    "question": "Which data structure is best suited for high-performance operations on a 1-D array with labels, derived from a DataFrame column?",
    "options": [
      "List",
      "NumPy Array",
      "Pandas Series",
      "Dictionary"
    ],
    "answer": "Pandas Series",
    "explanation": "A Series is the optimal 1-D labeled structure in pandas, aligning data via indices for vectorized operations, which lists and standard dictionaries lack.",
    "difficulty": "Intermediate"
  },
  {
    "id": 52,
    "question": "In the context of `pd.get_dummies()`, what does the `drop_first=True` parameter achieve?",
    "options": [
      "It removes the entire first column of the DataFrame",
      "It creates k-1 dummy variables to avoid perfect multicollinearity",
      "It drops rows with missing values in the first column",
      "It only creates dummy variables for the first column"
    ],
    "answer": "It creates k-1 dummy variables to avoid perfect multicollinearity",
    "explanation": "Removing the first category (dummy variable trap) is necessary for linear regression models to avoid perfect multicollinearity among features.",
    "difficulty": "Intermediate"
  },
  {
    "id": 53,
    "question": "What does the `axis=1` parameter signify in `df.apply(func, axis=1)`?",
    "options": [
      "Apply the function to each column",
      "Apply the function to each row",
      "Apply the function to the index",
      "Apply the function to the DataFrame values"
    ],
    "answer": "Apply the function to each row",
    "explanation": "In pandas, `axis=1` refers to the columns; therefore, iterating over `axis=1` applies the function row-wise (passing the Series of each row to the function).",
    "difficulty": "Intermediate"
  },
  {
    "id": 54,
    "question": "Which method is used to replace values in a DataFrame using a dictionary mapping: `{'old': 'new'}`?",
    "options": [
      "df.rename()",
      "df.replace()",
      "df.map()",
      "df.assign()"
    ],
    "answer": "df.replace()",
    "explanation": "The `replace` method accepts a dictionary or list to substitute specific values. `rename` is used for index or column labels, not cell values.",
    "difficulty": "Intermediate"
  },
  {
    "id": 55,
    "question": "How does `df.explode('column')` transform the data?",
    "options": [
      "It unstacks a MultiIndex column into rows",
      "It flattens lists or dictionaries in a column into separate rows",
      "It expands categorical data into dummy variables",
      "It removes null values from the column"
    ],
    "answer": "It flattens lists or dictionaries in a column into separate rows",
    "explanation": "The `explode` method transforms each element of a list-like object into a separate row, replicating the index values for the new rows.",
    "difficulty": "Intermediate"
  },
  {
    "id": 56,
    "question": "What is the result of setting `copy=False` in `df.iloc[row_indices]`?",
    "options": [
      "It always returns a view instead of a copy, but modification behavior depends on memory layout",
      "It always raises an error to prevent accidental data loss",
      "It guarantees that modifying the result will modify the original DataFrame",
      "It converts the DataFrame to a NumPy array"
    ],
    "answer": "It always returns a view instead of a copy, but modification behavior depends on memory layout",
    "explanation": "While `copy=False` suggests a view, whether setting with `iloc` returns a view or copy depends on internal memory fragmentation and block manager rules.",
    "difficulty": "Intermediate"
  },
  {
    "id": 57,
    "question": "Which aggregation function can be used to calculate the most frequent value (mode) in a group?",
    "options": [
      "max()",
      "count()",
      "mode()",
      "first()"
    ],
    "answer": "mode()",
    "explanation": "The `mode()` function returns the most frequent value(s) in a Series or DataFrame. Note that it can return multiple values if there is a tie.",
    "difficulty": "Intermediate"
  },
  {
    "id": 58,
    "question": "What is the `method='ffill'` argument used for during time-series resampling or reindexing?",
    "options": [
      "It drops rows with missing values",
      "It forward-fills missing values by propagating the last valid observation",
      "It calculates the average of surrounding values",
      "It fills missing values with zero"
    ],
    "answer": "It forward-fills missing values by propagating the last valid observation",
    "explanation": "'ffill' stands for 'forward fill'. It carries forward the last known value to fill gaps introduced by resampling or reindexing.",
    "difficulty": "Intermediate"
  },
  {
    "id": 59,
    "question": "Which pandas attribute provides access to the underlying NumPy array of a Series?",
    "options": [
      "Series.values",
      "Series.data",
      "Series.to_numpy()",
      "Series.array"
    ],
    "answer": "Series.to_numpy()",
    "explanation": "While `.values` works, `.to_numpy()` is the recommended method for retrieving the underlying NumPy array as it provides consistent behavior with ExtensionArrays.",
    "difficulty": "Intermediate"
  },
  {
    "id": 60,
    "question": "What does `df.stack()` do to a DataFrame?",
    "options": [
      "It compresses a level in the DataFrame's columns to the index",
      "It compresses a level in the DataFrame's index to the columns",
      "It concatenates two DataFrames vertically",
      "It sorts the DataFrame by the index"
    ],
    "answer": "It compresses a level in the DataFrame's columns to the index",
    "explanation": "The `stack` method pivots the columns (the innermost level by default) into the index, creating a MultiIndex Series or DataFrame with a longer shape.",
    "difficulty": "Intermediate"
  },
  {
    "id": 61,
    "question": "When using `pd.concat([df1, df2])`, what is the default join logic for columns not present in both DataFrames?",
    "options": [
      "Inner join (intersection of columns)",
      "Outer join (union of columns) with NaN fill",
      "Left join (keeping df1 columns)",
      "It raises a TypeError"
    ],
    "answer": "Outer join (union of columns) with NaN fill",
    "explanation": "`pd.concat` defaults to `join='outer'`, preserving all columns from all DataFrames and filling missing data with NaN.",
    "difficulty": "Intermediate"
  },
  {
    "id": 62,
    "question": "Which method is preferred for applying multiple aggregations with specific column names in a single GroupBy operation?",
    "options": [
      "Passing a list of functions to .agg()",
      "Using a loop with .apply()",
      "Chaining multiple .groupby() calls",
      "Using .transform() for each column"
    ],
    "answer": "Passing a list of functions to .agg()",
    "explanation": "Passing a list or dictionary to `.agg()` allows for calculating multiple statistics (e.g., sum and mean) efficiently in a single pass over the groups.",
    "difficulty": "Intermediate"
  },
  {
    "id": 63,
    "question": "What does the `na_values` parameter in `pd.read_csv` allow you to do?",
    "options": [
      "Drop rows containing specific values",
      "Automatically fill missing values with a scalar",
      "Specify additional strings to recognize as NaN/missing",
      "Convert NaN values to string representations"
    ],
    "answer": "Specify additional strings to recognize as NaN/missing",
    "explanation": "`na_values` accepts a list of strings (e.g., ['N/A', 'null']) that should be parsed as `NaN` during the CSV reading process.",
    "difficulty": "Intermediate"
  },
  {
    "id": 64,
    "question": "Which option correctly describes the behavior of `df['column'].nlargest(5)`?",
    "options": [
      "It sorts the entire DataFrame and returns the top 5 rows",
      "It returns the 5 largest values without fully sorting the column, using a selection algorithm",
      "It returns the top 5 values and their indices",
      "It filters for values greater than 5"
    ],
    "answer": "It returns the 5 largest values without fully sorting the column, using a selection algorithm",
    "explanation": "`nlargest` is more efficient than `sort_values` because it uses a selection algorithm (heap sort) to find the top N items without sorting the entire dataset.",
    "difficulty": "Intermediate"
  },
  {
    "id": 65,
    "question": "In pandas string methods, what does the `extract(r'(\\d+)')` operation return?",
    "options": [
      "A boolean mask indicating where digits are found",
      "All non-digit characters removed",
      "A DataFrame of captured groups matching the regex",
      "An integer count of all digit occurrences"
    ],
    "answer": "A DataFrame of captured groups matching the regex",
    "explanation": "The `extract` method captures regex groups and returns a DataFrame containing the extracted text for each group, useful for parsing structured data.",
    "difficulty": "Intermediate"
  },
  {
    "id": 66,
    "question": "What distinguishes `df.where(condition, other)` from `df[mask]`?",
    "options": [
      "`where` keeps values where condition is False and replaces where True",
      "`where` keeps values where condition is True and replaces where False",
      "`where` is used only for index alignment",
      "`where` modifies the DataFrame in-place, unlike masking"
    ],
    "answer": "`where` keeps values where condition is True and replaces where False",
    "explanation": "Unlike boolean masking (which filters out data), `where` preserves the shape of the DataFrame and replaces values failing the condition with `other` (default NaN).",
    "difficulty": "Intermediate"
  },
  {
    "id": 67,
    "question": "What is the `verify_integrity=True` parameter used for in `pd.concat()`?",
    "options": [
      "It checks if the concatenated data contains any NaN values",
      "It checks if the resulting axis contains duplicates and raises a ValueError if so",
      "It ensures the data types are consistent across DataFrames",
      "It validates that all DataFrames are loaded from the same file"
    ],
    "answer": "It checks if the resulting axis contains duplicates and raises a ValueError if so",
    "explanation": "When concatenating along a new axis, `verify_integrity` ensures that the new index (or columns) does not contain duplicate entries.",
    "difficulty": "Intermediate"
  },
  {
    "id": 68,
    "question": "Which `pd.to_numeric` parameter handles non-numeric data by converting it to NaN?",
    "options": [
      "raise",
      "ignore",
      "coerce",
      "downcast"
    ],
    "answer": "coerce",
    "explanation": "Setting `errors='coerce'` invalid parsing will be set as NaN. The default 'raise' throws an exception, and 'ignore' returns the original input.",
    "difficulty": "Intermediate"
  },
  {
    "id": 69,
    "question": "What is the effect of using `df.pipe()` in a method chain?",
    "options": [
      "It applies a function to the entire DataFrame while preserving the chain",
      "It applies a function to each row individually",
      "It splits the DataFrame into multiple pipes",
      "It removes duplicate rows from the DataFrame"
    ],
    "answer": "It applies a function to the entire DataFrame while preserving the chain",
    "explanation": "`pipe` allows you to inject custom functions or external libraries into a method chain, passing the entire DataFrame as an argument.",
    "difficulty": "Intermediate"
  },
  {
    "id": 70,
    "question": "How does `df.sample(frac=0.5)` differ from `df.head(len(df)//2)`?",
    "options": [
      "`sample` selects a random subset of rows, while `head` selects the top rows",
      "`sample` selects the top 50% of sorted rows",
      "`head` modifies the DataFrame in place",
      "There is no difference in the output"
    ],
    "answer": "`sample` selects a random subset of rows, while `head` selects the top rows",
    "explanation": "`sample` performs random sampling (stochastic selection) from the DataFrame, whereas `head` performs deterministic selection of the first N rows by position.",
    "difficulty": "Intermediate"
  },
  {
    "id": 71,
    "question": "What is the primary performance impact of passing a non-sorted MultiIndex to the `loc` accessor?",
    "options": [
      "The operation returns a view instead of a copy",
      "Pandas falls back to linear scanning for key alignment, increasing lookup time",
      "The index is automatically re-sorted in-place, causing a memory spike",
      "The operation raises a KeyError due to unsorted dimensions"
    ],
    "answer": "Pandas falls back to linear scanning for key alignment, increasing lookup time",
    "explanation": "Optimized slicing on MultiIndex requires the index to be lexicographically sorted. If unsorted, Pandas cannot efficiently utilize binary search or hash lookups and reverts to a slower linear scan to align keys.",
    "difficulty": "Advanced"
  },
  {
    "id": 72,
    "question": "In pandas 3.0+, how does the Copy-on-Write (CoW) mode affect chained assignment operations?",
    "options": [
      "It raises a SettingWithCopyWarning to prevent modification of a view",
      "It immediately creates a deep copy of the DataFrame to ensure thread safety",
      "It defers the copy until write time, allowing chained assignments to modify the original object successfully",
      "It disables the `inplace` parameter across all DataFrame methods"
    ],
    "answer": "It defers the copy until write time, allowing chained assignments to modify the original object successfully",
    "explanation": "Under Copy-on-Write, mutations trigger a copy of the underlying data. This logical change ensures that chained assignments (which previously often failed or triggered warnings in pandas < 3.0) behave predictably by propagating changes back to the original object if no intervening copy was logically required.",
    "difficulty": "Advanced"
  },
  {
    "id": 73,
    "question": "When optimizing memory usage, why is it generally more effective to convert a column of strings to the 'category' dtype only if the cardinality is low?",
    "options": [
      "High cardinality categories consume more memory than native Python strings due to dictionary overhead",
      "The `category` dtype utilizes a fixed-width 8-bit integer mapping that exceeds string size limits",
      "Pandas requires a hash table for every unique value that scales quadratically",
      "Sorting operations on `category` columns disable branch prediction optimizations"
    ],
    "answer": "High cardinality categories consume more memory than native Python strings due to dictionary overhead",
    "explanation": "The 'category' dtype stores data as integers mapping to a dictionary of unique values. If the number of unique values (cardinality) is high, the memory required for the integer array plus the dictionary exceeds that of a standard object-dtype string array.",
    "difficulty": "Advanced"
  },
  {
    "id": 74,
    "question": "What is the specific advantage of using `df.eval()` for complex arithmetic expressions compared to standard Python syntax?",
    "options": [
      "It bypasses the Python GIL by releasing the interpreter lock during calculation",
      "It uses the `numexpr` engine to optimize memory usage and reduce temporary object creation",
      "It compiles the expression directly to C bytecode using LLVM",
      "It automatically parallelizes the operation across all available CPU cores"
    ],
    "answer": "It uses the `numexpr` engine to optimize memory usage and reduce temporary object creation",
    "explanation": "`eval()` parses the expression string and passes it to the `numexpr` engine (if installed), which optimizes the order of operations, minimizes memory allocation for intermediate results, and utilizes vectorized CPU instructions.",
    "difficulty": "Advanced"
  },
  {
    "id": 75,
    "question": "Why does `df.itertuples()` generally outperform `df.iterrows()` for iteration?",
    "options": [
      "`itertuples` accesses the C-internal buffer directly, while `iterrows` creates a Series for every row",
      "`itertuples` utilizes SIMD instructions automatically",
      "`iterrows` triggers a full garbage collection cycle at every step",
      "`itertuples` returns a dictionary view rather than a tuple object"
    ],
    "answer": "`itertuples` accesses the C-internal buffer directly, while `iterrows` creates a Series for every row",
    "explanation": "`iterrows()` constructs a pandas Series for each row, incurring significant overhead from dtype metadata and index construction. `itertuples()` returns a lightweight `namedtuple`, offering a direct view into the underlying data blocks with much lower overhead.",
    "difficulty": "Advanced"
  },
  {
    "id": 76,
    "question": "What is the functional difference between `df.pipe()` and standard method chaining?",
    "options": [
      "`pipe` enforces immutability by creating a deep copy at every step",
      "`pipe` allows passing a callable that expects the DataFrame as its first argument, enabling use of custom functions",
      "Standard method chaining modifies the DataFrame in-place, while `pipe` returns a new object",
      "`pipe` automatically vectorizes the passed function using Numba"
    ],
    "answer": "`pipe` allows passing a callable that expects the DataFrame as its first argument, enabling use of custom functions",
    "explanation": "Standard chaining relies on methods attached to the DataFrame class. `pipe` acts as a bridge to integrate custom functions or methods that are not part of the DataFrame API into the chain by injecting the DataFrame as the first argument.",
    "difficulty": "Advanced"
  },
  {
    "id": 77,
    "question": "When reading a CSV with `pd.read_csv`, how does specifying `dtype={'column': 'str'}` affect performance compared to type inference?",
    "options": [
      "It forces the parser to skip the initial header scan",
      "It eliminates the overhead of parsing values into float/int and subsequent back-conversion if inference is wrong",
      "It disables the use of the C engine in favor of the Python engine",
      "It enables the parser to utilize memory-mapped files exclusively"
    ],
    "answer": "It eliminates the overhead of parsing values into float/int and subsequent back-conversion if inference is wrong",
    "explanation": "Type inference requires parsing data to guess types, which is computationally expensive. Explicitly defining `dtype` bypasses the inference step and prevents the potential cost of correcting a wrong initial guess (e.g., reading an ID as a float and converting back to object/string).",
    "difficulty": "Advanced"
  },
  {
    "id": 78,
    "question": "What distinguishes `transform()` from `apply()` within a `groupby` operation?",
    "options": [
      "`apply` returns a scalar for each group, while `transform` returns a DataFrame with the same shape as the input",
      "`transform` broadcasts the result back to the original DataFrame's index, preserving the original shape",
      "`apply` supports parallel execution via Dask, whereas `transform` is strictly single-threaded",
      "`transform` cannot accept lambda functions, only string aliases"
    ],
    "answer": "`transform` broadcasts the result back to the original DataFrame's index, preserving the original shape",
    "explanation": "While `apply` can return a scalar, Series, or DataFrame resulting in potentially varying shapes, `transform` is strictly designed to return a Series/DataFrame aligned with the input group's index, making it ideal for adding new columns based on group aggregations.",
    "difficulty": "Advanced"
  },
  {
    "id": 79,
    "question": "In the context of memory management, what is the primary benefit of the `SparseDtype`?",
    "options": [
      "It enables zero-copy slicing of DataFrames",
      "It stores only non-missing values and their locations, reducing memory for data mostly containing NaN or zeros",
      "It compresses data using the LZ4 algorithm automatically",
      "It converts object-dtype columns into fixed-width C strings"
    ],
    "answer": "It stores only non-missing values and their locations, reducing memory for data mostly containing NaN or zeros",
    "explanation": "The `SparseDtype` does not allocate memory for the default fill value (NaN or 0) in the array. Instead, it maintains a coordinate list (or similar sparse representation), significantly saving memory for datasets with high sparsity.",
    "difficulty": "Advanced"
  },
  {
    "id": 80,
    "question": "What is the technical consequence of setting `sort=False` when using `pd.concat([df1, df2])`?",
    "options": [
      "The index is unsorted, which prevents the use of row-based integer indexing (iloc) on the result",
      "It returns a view of the combined data rather than a copy",
      "It skips the lexicographical sorting of the non-concatenation axis, saving CPU cycles",
      "It forces the result to use a HashIndex instead of a RangeIndex"
    ],
    "answer": "It skips the lexicographical sorting of the non-concatenation axis, saving CPU cycles",
    "explanation": "By default, `pd.concat` sorts the non-concatenation axis (columns if axis=0) to ensure a deterministic order. Disabling this (`sort=False`) skips this expensive O(N log N) operation, returning the columns in the order they appear, which is faster for large concatenations.",
    "difficulty": "Advanced"
  },
  {
    "id": 81,
    "question": "How does the `engine='pyarrow'` parameter in `pd.read_csv` (Pandas 2.0+) improve I/O performance?",
    "options": [
      "It uses a C++ implementation of the CSV parser that is strictly faster than the default C parser",
      "It utilizes the Rust-based Arrow CSV reader to infer types and read data directly into Arrow memory",
      "It converts the CSV file to Parquet in memory before reading",
      "It bypasses the block manager and writes directly to the NumPy buffer"
    ],
    "answer": "It utilizes the Rust-based Arrow CSV reader to infer types and read data directly into Arrow memory",
    "explanation": "The PyArrow engine delegates the parsing to Apache Arrow's highly optimized Rust-based CSV reader. This reduces Python interpreter overhead and allows for zero-copy conversion to pandas' Arrow-backed data structures.",
    "difficulty": "Advanced"
  },
  {
    "id": 82,
    "question": "What is the specific behavior of `df.where(cond, other)` compared to boolean indexing `df[mask]`?",
    "options": [
      "`where` preserves the shape of the DataFrame and indexes of the original object, replacing non-matching rows with NaN (or `other`)",
      "Boolean indexing creates a copy, while `where` operates in-place",
      "`where` raises an error if `cond` contains NaN values, whereas boolean indexing ignores them",
      "`where` can only be applied to Series, not DataFrames"
    ],
    "answer": "`where` preserves the shape of the DataFrame and indexes of the original object, replacing non-matching rows with NaN (or `other`)",
    "explanation": "Boolean indexing filters the data, dropping rows that do not match. In contrast, `where` retains the full shape of the original DataFrame but replaces values where the condition is False, allowing for operations that maintain alignment.",
    "difficulty": "Advanced"
  },
  {
    "id": 83,
    "question": "Why is `pd.factorize()` often faster than converting a column to `category` dtype for the purpose of integer encoding?",
    "options": [
      "It uses a hash-based algorithm that does not require sorting the unique values",
      "It returns the result as a NumPy array, bypassing pandas index overhead",
      "It automatically utilizes multiple threads to count unique occurrences",
      "It operates on the C-internal block manager directly without creating a copy"
    ],
    "answer": "It uses a hash-based algorithm that does not require sorting the unique values",
    "explanation": "While `astype('category')` often sorts the categories to establish an order, `pd.factorize()` simply generates unique integer identifiers for each distinct value using a hash table, which is computationally cheaper for pure encoding.",
    "difficulty": "Advanced"
  },
  {
    "id": 84,
    "question": "What is the primary risk of using `df.iloc[0] = value` on a DataFrame containing ExtensionArrays (e.g., nullable integers, strings)?",
    "options": [
      "It forces the entire column to convert back to object dtype (boxing), losing performance benefits",
      "It triggers an immediate defragmentation of the memory block",
      "It raises a ValueError because ExtensionArrays are immutable",
      "It silently converts the index to a RangeIndex"
    ],
    "answer": "It forces the entire column to convert back to object dtype (boxing), losing performance benefits",
    "explanation": "Setting a scalar value into a specific cell of an ExtensionArray often requires Pandas to 'box' the value or convert the whole column to a standard object dtype if the operation is not fully supported by the extension type, negating the memory/performance savings of the ExtensionArray.",
    "difficulty": "Advanced"
  },
  {
    "id": 85,
    "question": "How does the `numba` engine in `df.rolling(...).apply()` differ from the standard engine?",
    "options": [
      "It uses a Just-In-Time (JIT) compiler to convert the Python function to machine code for faster execution",
      "It applies the function to the rolling window in parallel across multiple threads",
      "It automatically vectorizes the function using NumPy ufuncs",
      "It skips the validation of the window boundary conditions"
    ],
    "answer": "It uses a Just-In-Time (JIT) compiler to convert the Python function to machine code for faster execution",
    "explanation": "Standard `apply` executes the Python function repeatedly for each window, which is slow. `engine='numba'` JIT-compiles the provided Python function into optimized machine code, dramatically reducing overhead for window-based calculations.",
    "difficulty": "Advanced"
  },
  {
    "id": 86,
    "question": "What is the function of the `pd.api.extensions.register_dataframe_accessor` decorator?",
    "options": [
      "It allows developers to add custom methods or properties to existing DataFrame objects without subclassing",
      "It automatically converts a DataFrame to a PyArrow Table when accessed",
      "It registers a custom DataFrame comparator for `assert_frame_equal`",
      "It enables type hinting for DataFrames in IDE static analysis"
    ],
    "answer": "It allows developers to add custom methods or properties to existing DataFrame objects without subclassing",
    "explanation": "This decorator creates a namespace 'accessor' (like `.str` or `.dt`) that attaches to the DataFrame. It is the idiomatic way to extend DataFrame functionality with domain-specific methods directly on the object instance.",
    "difficulty": "Advanced"
  },
  {
    "id": 87,
    "question": "What is the memory implication of `pd.merge` when merging on integer columns vs. merging on the Index?",
    "options": [
      "Merging on Index avoids the creation of a temporary index array for the join keys, reducing peak memory",
      "Merging on integer columns automatically enables the `sort` merge algorithm",
      "Merging on Index forces a deep copy of the left DataFrame",
      "There is no difference; Pandas converts all column merges to index merges internally"
    ],
    "answer": "Merging on Index avoids the creation of a temporary index array for the join keys, reducing peak memory",
    "explanation": "When merging on columns, pandas must temporarily use these columns as indexes or sort them. Joining on an already existing Index utilizes the existing sorted structure, avoiding the allocation of temporary index memory and the overhead of re-hashing or re-sorting keys.",
    "difficulty": "Advanced"
  },
  {
    "id": 88,
    "question": "When using `df.resample('5min').mean()`, what determines the closed interval of the bins?",
    "options": [
      "The default is closed='right' and label='right', meaning the interval is (start, end]",
      "The default is closed='left' and label='left', meaning the interval is [start, end)",
      "Resampling always uses inclusive intervals on both sides for weighted means",
      "The interval closure is dynamically determined by the timezone of the index"
    ],
    "answer": "The default is closed='right' and label='right', meaning the interval is (start, end]",
    "explanation": "By default, `resample` creates bins that are closed on the right (open on the left). The timestamp label also defaults to the right edge of the bin. This behavior is reverse for some specific frequencies like 'M' (month end) or 'MS' (month start).",
    "difficulty": "Advanced"
  },
  {
    "id": 89,
    "question": "What happens when you perform a boolean comparison between two DataFrames with different shapes but compatible indices/columns?",
    "options": [
      "Pandas broadcasts the comparison based on the index and columns (outer join alignment), filling mismatches with False",
      "It raises a ValueError immediately if shapes do not match exactly",
      "It truncates the larger DataFrame to match the smaller one's shape",
      "It compares only the overlapping rows and columns, ignoring the rest"
    ],
    "answer": "Pandas broadcasts the comparison based on the index and columns (outer join alignment), filling mismatches with False",
    "explanation": "Pandas aligns DataFrames on indices and columns before comparison. Where indices or columns do not overlap, pandas reindexes both to the union (outer join) and fills the non-matching areas with `NaN`, which evaluates to `False` in the boolean context.",
    "difficulty": "Advanced"
  },
  {
    "id": 90,
    "question": "Why is `df.values` discouraged in favor of `df.to_numpy()` or `df.to_numpy(dtype=None)` in modern Pandas?",
    "options": [
      "`df.values` returns a view that cannot be modified safely with Copy-on-Write enabled",
      "`df.values` attempts to create a homogeneous NumPy array, potentially losing ExtensionArray type information",
      "`df.values` has been deprecated and removed in Pandas 3.0",
      "`df.values` triggers an immediate deep copy of the underlying data"
    ],
    "answer": "`df.values` attempts to create a homogeneous NumPy array, potentially losing ExtensionArray type information",
    "explanation": "The `.values` attribute tries to coerce the data into a single NumPy ndarray dtype. For DataFrames with nullable integers or strings (ExtensionArrays), this often forces an upcast to object or float dtype. `.to_numpy()` provides safer handling of these modern types.",
    "difficulty": "Advanced"
  },
  {
    "id": 91,
    "question": "What does the `memory_usage(deep=True)` method calculate that `memory_usage(deep=False)` does not?",
    "options": [
      "It includes the memory consumed by the object references within object-dtype columns",
      "It includes the memory used by the Index object",
      "It estimates the memory usage of the backup copy held by the BlockManager",
      "It accounts for the memory overhead of the DataFrame structure itself"
    ],
    "answer": "It includes the memory consumed by the object references within object-dtype columns",
    "explanation": "Object-dtype columns store pointers to Python objects. `deep=True` inspects the actual Python objects (like strings) pointed to by those references and sums their memory usage, whereas `deep=False` only counts the pointer size.",
    "difficulty": "Advanced"
  },
  {
    "id": 92,
    "question": "How does the `kwds` argument in `df.resample(...).apply(func, kwds={})` work?",
    "options": [
      "It passes keyword arguments to the `resample` function itself",
      "It passes keyword arguments to the applied function `func` for every window",
      "It specifies parameters for the time-zone conversion during resampling",
      "It defines how to handle `NaN` values in the aggregation"
    ],
    "answer": "It passes keyword arguments to the applied function `func` for every window",
    "explanation": "When applying a custom function to a resampled window, `kwds` allows you to pass a dictionary of keyword arguments that will be unpacked into `func(**kwds)` during the aggregation step.",
    "difficulty": "Advanced"
  },
  {
    "id": 93,
    "question": "In the context of Indexing, what is 'Index fragmentation' and how do you fix it?",
    "options": [
      "Fragmentation occurs when the Index is not sorted; it is fixed with `df.sort_index()`",
      "Fragmentation occurs when the index contains duplicate values; it is fixed with `df.reset_index(drop=True)`",
      "Fragmentation occurs when deleting rows leaves gaps in the memory layout; it is fixed with `df.copy()`",
      "Fragmentation refers to MultiIndex levels having different dtypes; it is fixed with `df.astype(str)`"
    ],
    "answer": "Fragmentation occurs when deleting rows leaves gaps in the memory layout; it is fixed with `df.copy()`",
    "explanation": "When data is deleted, the underlying memory blocks may have holes (fragmentation) that prevent the data from being stored contiguously. `df.copy()` creates a new, contiguous copy of the data, defragmenting the memory layout and restoring performance.",
    "difficulty": "Advanced"
  },
  {
    "id": 94,
    "question": "What is the specific mechanism by which `pd.to_datetime()` uses the `cache` parameter?",
    "options": [
      "It caches the result of the conversion to disk to avoid re-computation in future sessions",
      "It creates a unique cache for unique datetime strings to avoid re-parsing duplicates",
      "It stores the parsed format string to skip format inference in subsequent calls",
      "It utilizes an LRU cache for the timezone database lookups"
    ],
    "answer": "It creates a unique cache for unique datetime strings to avoid re-parsing duplicates",
    "explanation": "When parsing arrays of strings, `cache=True` creates a dictionary mapping seen strings to their datetime counterpart. If the same string appears again, it hits the cache instead of re-running the expensive parsing logic.",
    "difficulty": "Advanced"
  },
  {
    "id": 95,
    "question": "Why might using `pd.DataFrame.mode()` be computationally expensive on large datasets?",
    "options": [
      "It sorts the entire DataFrame to determine the most frequent value",
      "It requires hashing all values to count frequencies, similar to a groupby count",
      "It cannot utilize vectorized operations and must iterate row-by-row",
      "It forces the conversion of all numeric columns to object dtype"
    ],
    "answer": "It sorts the entire DataFrame to determine the most frequent value",
    "explanation": "To find the mode, pandas effectively needs to find the maximum count. Historically and implementation-wise, this often involves sorting data or heavy hashing/grouping operations across all columns, which scales poorly compared to simple vectorized sums.",
    "difficulty": "Advanced"
  },
  {
    "id": 96,
    "question": "What is the behavior of `df.stack()` when dealing with columns that have different dtypes?",
    "options": [
      "It converts the column data to a common dtype (usually object) to fit into a single Series",
      "It raises a TypeError because all columns must have the same dtype to stack",
      "It preserves the dtypes by returning a MultiIndex Series with object dtype",
      "It splits the result into multiple Series based on dtype"
    ],
    "answer": "It converts the column data to a common dtype (usually object) to fit into a single Series",
    "explanation": "The resulting stacked Series must have a single dtype. If the columns have heterogeneous dtypes (e.g., int and float), pandas upcasts them to a common compatible dtype, usually 'object' (boxing the values), to store them in the single Series.",
    "difficulty": "Advanced"
  },
  {
    "id": 97,
    "question": "When using `df.explode()`, how is the index of the resulting DataFrame handled?",
    "options": [
      "The index is repeated for each exploded element, preserving the original row's index value",
      "A default RangeIndex is generated, discarding the original index",
      "The index is incremented sequentially for each new row generated",
      "The original index is transformed into a MultiIndex of (original_index, element_position)"
    ],
    "answer": "The index is repeated for each exploded element, preserving the original row's index value",
    "explanation": "`explode` maintains the relationship to the source data by replicating the index values of the parent row for each item in the exploded list-like column. This allows easy mapping back to the original data but creates duplicate index values.",
    "difficulty": "Advanced"
  },
  {
    "id": 98,
    "question": "What is the primary difference between `df.mask(cond)` and `df.where(cond)`?",
    "options": [
      "`mask` replaces values where the condition is True, while `where` replaces values where the condition is False",
      "`mask` operates in-place, while `where` returns a copy",
      "`mask` can only accept a Series as a condition, whereas `where` requires an array",
      "`mask` drops rows, while `where` replaces them with NaN"
    ],
    "answer": "`mask` replaces values where the condition is True, while `where` replaces values where the condition is False",
    "explanation": "Functionally, `mask` is the inverse of `where`. `df.where(cond)` keeps values where `cond` is True. `df.mask(cond)` keeps values where `cond` is False, replacing the others.",
    "difficulty": "Advanced"
  },
  {
    "id": 99,
    "question": "In a MultiIndex DataFrame, how does the `dropna=False` parameter in `groupby` affect the result?",
    "options": [
      "It includes keys in the grouping that consist entirely of NaN values",
      "It prevents the removal of rows with NaN values in non-grouping columns during aggregation",
      "It forces the aggregation result to be 0 instead of NaN for empty groups",
      "It disables the automatic downcasting of float NaNs to integer NaNs"
    ],
    "answer": "It includes keys in the grouping that consist entirely of NaN values",
    "explanation": "By default, `groupby` excludes rows where the groupby key(s) are NaN (using `dropna=True`). Setting `dropna=False` tells pandas to treat NaN as a valid grouping key, creating a specific group for the NaN entries.",
    "difficulty": "Advanced"
  },
  {
    "id": 100,
    "question": "What does the `method='ffill'` parameter in `df.reindex()` do regarding interpolation?",
    "options": [
      "It performs forward-filling of missing data introduced by the reindexing, using the last valid observation",
      "It applies a cubic spline interpolation to estimate missing values",
      "It fills missing values with the mean of the surrounding valid values",
      "It drops any rows in the target index that do not exist in the original DataFrame"
    ],
    "answer": "It performs forward-filling of missing data introduced by the reindexing, using the last valid observation",
    "explanation": "When reindexing to a new set of labels, gaps may appear. `method='ffill'` (forward fill) propagates the last valid value forward to fill these gaps, maintaining continuity in time-series or ordered data.",
    "difficulty": "Advanced"
  }
]