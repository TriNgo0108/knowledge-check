[
  {
    "id": 1,
    "question": "Which architectural constraint requires that service instances must not store client session data locally to enable horizontal scaling?",
    "options": [
      "Statelessness",
      "Loose Coupling",
      "Strong Consistency",
      "Idempotency"
    ],
    "answer": "Statelessness",
    "explanation": "Statelessness allows any instance to handle any request, enabling dynamic horizontal scaling by removing dependency on specific server context. Loose coupling refers to module interaction, Strong Consistency to data state, and Idempotency to operation effects.",
    "difficulty": "Beginner"
  },
  {
    "id": 2,
    "question": "In a Master-Slave database replication topology, which operation is strictly prohibited on the Slave nodes to maintain data integrity?",
    "options": [
      "Read operations",
      "Data replication",
      "Write operations",
      "Failover promotion"
    ],
    "answer": "Write operations",
    "explanation": "In standard master-slave replication, writes must occur on the Master to ensure the transaction is replicated to slaves. Allowing writes on slaves causes data divergence, breaking the replication flow.",
    "difficulty": "Beginner"
  },
  {
    "id": 3,
    "question": "What is the primary trade-off when increasing the 'Replication Factor' in a distributed database system?",
    "options": [
      "Increased write latency",
      "Decreased read availability",
      "Loss of data durability",
      "Increased network chattiness for reads"
    ],
    "answer": "Increased write latency",
    "explanation": "A higher replication factor requires the system to synchronize data across more nodes before acknowledging a write, increasing latency. It generally improves read availability and durability but consumes more storage.",
    "difficulty": "Beginner"
  },
  {
    "id": 4,
    "question": "Which property of the CAP theorem is compromised when a distributed system chooses to remain 'Available' during a network partition?",
    "options": [
      "Scalability",
      "Consistency",
      "Partition Tolerance",
      "Durability"
    ],
    "answer": "Consistency",
    "explanation": "During a partition, a system chooses Availability (AP) by accepting writes that may conflict, sacrificing Strong Consistency (C). Partition Tolerance (P) is a requirement for distributed systems, leaving C as the sacrifice.",
    "difficulty": "Beginner"
  },
  {
    "id": 5,
    "question": "Which mechanism allows a system to handle traffic spikes by distributing incoming requests across multiple servers based on a predefined algorithm?",
    "options": [
      "API Gateway",
      "Load Balancer",
      "Message Queue",
      "Circuit Breaker"
    ],
    "answer": "Load Balancer",
    "explanation": "A Load Balancer distributes network traffic across multiple servers to ensure no single server bears too much load. An API Gateway manages traffic entry and routing, a Message Queue decouples services, and a Circuit Breaker prevents cascading failures.",
    "difficulty": "Beginner"
  },
  {
    "id": 6,
    "question": "In a microservices architecture, what is the primary function of an 'API Gateway'?",
    "options": [
      "To store database backups",
      "To act as a reverse proxy and aggregate requests from multiple clients",
      "To manage inter-service communication latency",
      "To host the frontend static files"
    ],
    "answer": "To act as a reverse proxy and aggregate requests from multiple clients",
    "explanation": "The API Gateway serves as the single entry point, handling request routing, composition, and authentication. It abstracts the backend microservices structure from the client.",
    "difficulty": "Beginner"
  },
  {
    "id": 7,
    "question": "Which caching strategy writes data to the cache and the backing database simultaneously in a single transaction?",
    "options": [
      "Write-Through",
      "Write-Back",
      "Write-Around",
      "Refresh-Ahead"
    ],
    "answer": "Write-Through",
    "explanation": "Write-Through updates the cache and database synchronously, ensuring data consistency. Write-Back updates only the cache and asynchronously writes to the database, improving write speed but risking data loss.",
    "difficulty": "Beginner"
  },
  {
    "id": 8,
    "question": "What is the primary purpose of a 'Message Queue' in a distributed system?",
    "options": [
      "To synchronize database transactions",
      "To enable synchronous communication between services",
      "To allow asynchronous processing and decouple services",
      "To balance load between servers"
    ],
    "answer": "To allow asynchronous processing and decouple services",
    "explanation": "Message Queues enable asynchronous communication by buffering messages, allowing producers to send data without waiting for consumers to process it. Load balancing is handled by Load Balancers, while synchronous communication is typically REST or gRPC.",
    "difficulty": "Beginner"
  },
  {
    "id": 9,
    "question": "Which term describes the scenario where a system updates data in a database but the cache retains the old, stale value?",
    "options": [
      "Cache Stampede",
      "Race Condition",
      "Cache Inconsistency",
      "Thundering Herd"
    ],
    "answer": "Cache Inconsistency",
    "explanation": "Cache Inconsistency occurs when the data source (DB) is updated but the cache is not invalidated or updated, leading to stale reads. A Thundering Herd is a mass request overload, and a Race Condition is a general timing error.",
    "difficulty": "Beginner"
  },
  {
    "id": 10,
    "question": "In database sharding, which approach determines the shard destination based on a range of column values?",
    "options": [
      "Consistent Hashing",
      "Vertical Sharding",
      "Range-Based Sharding",
      "Directory-Based Sharding"
    ],
    "answer": "Range-Based Sharding",
    "explanation": "Range-Based Sharding distributes rows based on value intervals (e.g., A-M in shard 1, N-Z in shard 2). Consistent Hashing maps data to a hash ring, Vertical Sharding splits tables, and Directory-Based uses a lookup table.",
    "difficulty": "Beginner"
  },
  {
    "id": 11,
    "question": "Which HTTP status code should a client receive when a rate limiter blocks a request due to quota exhaustion?",
    "options": [
      "403 Forbidden",
      "429 Too Many Requests",
      "503 Service Unavailable",
      "401 Unauthorized"
    ],
    "answer": "429 Too Many Requests",
    "explanation": "429 is the standard status code indicating the user has sent too many requests in a given amount of time. 403 relates to permissions, 503 to server unavailability, and 401 to authentication.",
    "difficulty": "Beginner"
  },
  {
    "id": 12,
    "question": "What is the primary disadvantage of Vertical Scaling (scaling up) compared to Horizontal Scaling (scaling out)?",
    "options": [
      "Increased complexity in configuration",
      "Hard limit on hardware capacity",
      "Higher network latency",
      "Inconsistent data storage"
    ],
    "answer": "Hard limit on hardware capacity",
    "explanation": "Vertical Scaling is eventually limited by the maximum specs of a single machine (CPU, RAM). Horizontal Scaling distributes load across cheaper commodity servers, offering theoretically limitless expansion.",
    "difficulty": "Beginner"
  },
  {
    "id": 13,
    "question": "Which algorithm is used by a Load Balancer to ensure that a specific client consistently connects to the same server during a session?",
    "options": [
      "Round Robin",
      "Least Connections",
      "IP Hash",
      "Random"
    ],
    "answer": "IP Hash",
    "explanation": "IP Hash uses the client's IP address to calculate a hash, mapping it to a specific server to ensure session persistence. Round Robin and Least Connections distribute requests without regard to client identity.",
    "difficulty": "Beginner"
  },
  {
    "id": 14,
    "question": "In the context of system design, what does 'Idempotency' ensure?",
    "options": [
      "An operation can be applied multiple times without changing the result beyond the initial application",
      "Data is written to multiple nodes simultaneously",
      "The system remains available during network partitions",
      "The database schema can be modified without downtime"
    ],
    "answer": "An operation can be applied multiple times without changing the result beyond the initial application",
    "explanation": "Idempotency is crucial for retries in distributed systems, ensuring that duplicate requests (e.g., payment processing) do not cause duplicate side effects. It refers to the effect of the operation, not consistency or availability.",
    "difficulty": "Beginner"
  },
  {
    "id": 15,
    "question": "Which communication pattern involves a single producer sending a message to multiple consumers simultaneously?",
    "options": [
      "Point-to-Point",
      "Fan-Out",
      "Request-Response",
      "Half-Duplex"
    ],
    "answer": "Fan-Out",
    "explanation": "Fan-Out describes a pattern where one message is delivered to multiple independent queues or consumers (e.g., Pub/Sub). Point-to-Point delivers to one consumer, and Request-Response is synchronous.",
    "difficulty": "Beginner"
  },
  {
    "id": 16,
    "question": "Why is a 'Read Replica' database configuration generally used?",
    "options": [
      "To improve write performance",
      "To improve read performance and availability",
      "To enforce strong consistency",
      "To simplify database backups"
    ],
    "answer": "To improve read performance and availability",
    "explanation": "Read replicas offload read traffic from the primary write node, allowing horizontal scaling of read capacity. Writes still go to the primary, and consistency is often 'eventual' due to replication lag.",
    "difficulty": "Beginner"
  },
  {
    "id": 17,
    "question": "What is the primary function of a 'Circuit Breaker' pattern?",
    "options": [
      "To compress data packets",
      "To prevent cascading failures by failing fast when a service is down",
      "To reroute traffic to a backup data center",
      "To encrypt communication channels"
    ],
    "answer": "To prevent cascading failures by failing fast when a service is down",
    "explanation": "The Circuit Breaker stops requests to a failing service, allowing it to recover rather than overwhelming it with retries. It protects the system from resource exhaustion and latency propagation.",
    "difficulty": "Beginner"
  },
  {
    "id": 18,
    "question": "Which database characteristic ensures that a transaction is 'all or nothing', meaning if one part fails, the entire transaction fails?",
    "options": [
      "Isolation",
      "Durability",
      "Atomicity",
      "Consistency"
    ],
    "answer": "Atomicity",
    "explanation": "Atomicity guarantees that a series of operations within a transaction are treated as a single unit. If any operation fails, the database is rolled back to its previous state.",
    "difficulty": "Beginner"
  },
  {
    "id": 19,
    "question": "Which design pattern helps mitigate the 'Thundering Herd' problem caused by a sudden influx of requests regenerating a cache key simultaneously?",
    "options": [
      "Request Coalescing",
      "Write-Through Cache",
      "Exponential Backoff",
      "Database Sharding"
    ],
    "answer": "Request Coalescing",
    "explanation": "Request Coalescing (or locking) allows only one request to regenerate the stale value while others wait for the result. This prevents the database from being overwhelmed by duplicate generation requests.",
    "difficulty": "Beginner"
  },
  {
    "id": 20,
    "question": "In a Content Delivery Network (CDN), what mechanism routes a user to the geographically closest edge server?",
    "options": [
      "DNS Resolution",
      "BGP Protocol",
      "TCP Handshake",
      "ARP Table"
    ],
    "answer": "DNS Resolution",
    "explanation": "CDNs use DNS to resolve a domain name to an IP address of the edge server nearest to the user's location. BGP handles routing between autonomous systems, while TCP and ARP handle transport and local address resolution.",
    "difficulty": "Beginner"
  },
  {
    "id": 21,
    "question": "Which consistency model guarantees that if a write is acknowledged, all subsequent reads will see that write?",
    "options": [
      "Eventual Consistency",
      "Strong Consistency",
      "Causal Consistency",
      "Weak Consistency"
    ],
    "answer": "Strong Consistency",
    "explanation": "Strong consistency ensures that any read operation following a successful write returns the latest data. Eventual consistency only guarantees that data will propagate eventually, introducing a delay.",
    "difficulty": "Beginner"
  },
  {
    "id": 22,
    "question": "What is the primary disadvantage of using a SQL database for massive-scale, unstructured data storage compared to NoSQL?",
    "options": [
      "Lack of transaction support",
      "Difficulty in horizontal scaling (sharding)",
      "Slower read performance",
      "Complexity of schema design"
    ],
    "answer": "Difficulty in horizontal scaling (sharding)",
    "explanation": "SQL databases rely on rigid schemas and relational integrity, making horizontal scaling (sharding) complex and often requiring expensive vertical scaling. NoSQL is designed for horizontal scaling and flexible schemas.",
    "difficulty": "Beginner"
  },
  {
    "id": 23,
    "question": "Which system component acts as an intermediary that forwards client requests to appropriate backend services, typically hiding backend details?",
    "options": [
      "Reverse Proxy",
      "Message Broker",
      "Load Balancer",
      "Database Connector"
    ],
    "answer": "Reverse Proxy",
    "explanation": "A Reverse Proxy sits in front of web servers, forwarding client requests to those servers. While Load Balancers distribute traffic, a Reverse Proxy's primary role is acting as a gateway and handling security/compression.",
    "difficulty": "Beginner"
  },
  {
    "id": 24,
    "question": "Which technique involves storing a computed result to avoid the cost of recomputing it in the future?",
    "options": [
      "Indexing",
      "Memoization",
      "Partitioning",
      "Batching"
    ],
    "answer": "Memoization",
    "explanation": "Memoization is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again.",
    "difficulty": "Beginner"
  },
  {
    "id": 25,
    "question": "What is the primary benefit of using 'Long Polling' over standard 'Polling' for receiving real-time updates?",
    "options": [
      "It reduces bandwidth usage",
      "It allows the server to hold the connection open until data is available",
      "It uses UDP for faster transmission",
      "It eliminates the need for a client request"
    ],
    "answer": "It allows the server to hold the connection open until data is available",
    "explanation": "Long Polling reduces latency and network chatter compared to short polling by keeping the request open until the server has data to send. Unlike WebSockets, it still relies on HTTP requests.",
    "difficulty": "Beginner"
  },
  {
    "id": 26,
    "question": "Which CAP theorem configuration allows for data conflicts during a partition but guarantees the system remains operational?",
    "options": [
      "CP (Consistency/Partition Tolerance)",
      "AP (Availability/Partition Tolerance)",
      "CA (Consistency/Availability)",
      "BP (Byzantine/Partition)"
    ],
    "answer": "AP (Availability/Partition Tolerance)",
    "explanation": "AP systems prioritize availability, accepting writes on all nodes even if they cannot sync, risking data inconsistency. CP systems would reject writes to maintain consistency, potentially appearing 'down'.",
    "difficulty": "Beginner"
  },
  {
    "id": 27,
    "question": "Which database sharding method is easiest to implement but risks creating 'hot spots' (uneven distribution)?",
    "options": [
      "Hash-based Sharding",
      "Consistent Hashing",
      "Key Range/Range-based Sharding",
      "Geographic Sharding"
    ],
    "answer": "Key Range/Range-based Sharding",
    "explanation": "Range-based sharding is intuitive but prone to hot spots if specific ranges (e.g., recent dates) are accessed more frequently. Hash-based sharding distributes data more evenly.",
    "difficulty": "Beginner"
  },
  {
    "id": 28,
    "question": "What is the role of a 'Leader Election' algorithm in a distributed system?",
    "options": [
      "To authorize user login",
      "To elect a master node to coordinate tasks",
      "To balance load between services",
      "To compress network traffic"
    ],
    "answer": "To elect a master node to coordinate tasks",
    "explanation": "Leader election ensures that in a cluster of nodes, one specific node is chosen as the leader to manage distributed locks, replication, or coordination, avoiding split-brain scenarios.",
    "difficulty": "Beginner"
  },
  {
    "id": 29,
    "question": "Which caching eviction policy removes the item that has not been used for the longest time?",
    "options": [
      "LFU (Least Frequently Used)",
      "LRU (Least Recently Used)",
      "FIFO (First In, First Out)",
      "LIFO (Last In, First Out)"
    ],
    "answer": "LRU (Least Recently Used)",
    "explanation": "LRU evicts the cache entry that has not been accessed for the longest period. LFU removes the least frequently accessed item, and FIFO removes the oldest item by insertion time.",
    "difficulty": "Beginner"
  },
  {
    "id": 30,
    "question": "In an Event-Driven Architecture, what mechanism prevents a consumer from being overwhelmed by a sudden burst of events?",
    "options": [
      "Idempotency",
      "Backpressure",
      "Circuit Breaker",
      "Indexing"
    ],
    "answer": "Backpressure",
    "explanation": "Backpressure is a mechanism where a consumer signals the producer to slow down or stop sending data to prevent resource exhaustion. A Circuit Breaker stops calls entirely to a failing service.",
    "difficulty": "Beginner"
  },
  {
    "id": 31,
    "question": "Which protocol utilizes persistent, bidirectional communication channels allowing servers to push data to clients without being polled?",
    "options": [
      "HTTP/1.1",
      "WebSocket",
      "gRPC",
      "SMTP"
    ],
    "answer": "WebSocket",
    "explanation": "WebSocket provides a full-duplex communication channel over a single TCP connection, allowing servers to proactively send data. HTTP is request-response, and gRPC is an RPC framework usually over HTTP/2.",
    "difficulty": "Beginner"
  },
  {
    "id": 32,
    "question": "What is the primary function of the 'Sidecar' pattern in a microservices architecture?",
    "options": [
      "To separate database traffic from application logic",
      "To offload infrastructure features (logging, monitoring) from the main application",
      "To act as a user interface",
      "To scale the database vertically"
    ],
    "answer": "To offload infrastructure features (logging, monitoring) from the main application",
    "explanation": "A Sidecar container runs alongside the main application container in the same pod to handle cross-cutting concerns like networking, logging, or security, keeping the app logic clean.",
    "difficulty": "Beginner"
  },
  {
    "id": 33,
    "question": "Which term refers to the process of transforming data into a format that cannot be read by unauthorized users to ensure security?",
    "options": [
      "Encoding",
      "Hashing",
      "Encryption",
      "Serialization"
    ],
    "answer": "Encryption",
    "explanation": "Encryption translates data into a scrambled format using a key, allowing reversal via decryption for authorized users. Hashing is one-way, Encoding is for format compatibility, and Serialization is for transport.",
    "difficulty": "Beginner"
  },
  {
    "id": 34,
    "question": "Which reliability strategy involves running a system in multiple geographically distinct locations to survive regional failures?",
    "options": [
      "Vertical Scaling",
      "Active-Active Setup",
      "Read Replicas",
      "Sharding"
    ],
    "answer": "Active-Active Setup",
    "explanation": "Active-Active involves deploying traffic-serving infrastructure in multiple regions simultaneously. Read Replicas only scale reads, and Sharding distributes data but not necessarily for regional disaster recovery.",
    "difficulty": "Beginner"
  },
  {
    "id": 35,
    "question": "In database normalization, which anomaly is solved by ensuring that non-key attributes are dependent only on the primary key (3NF)?",
    "options": [
      "Update Anomaly",
      "Insertion Anomaly",
      "Transitive Dependency",
      "Redundancy"
    ],
    "answer": "Transitive Dependency",
    "explanation": "Third Normal Form (3NF) specifically removes transitive dependencies where non-prime attributes depend on other non-prime attributes. 2NF removes partial dependencies.",
    "difficulty": "Beginner"
  },
  {
    "id": 36,
    "question": "In the context of the CAP theorem, what behavior does a 'CP' (Consistency and Partition Tolerance) system exhibit during a network partition?",
    "options": [
      "The system remains available for all reads and writes, returning potentially stale data.",
      "The system returns errors or timeouts on writes to ensure data consistency across nodes.",
      "The system automatically merges conflicting data versions using vector clocks.",
      "The system routes all traffic to the nearest data center to minimize latency."
    ],
    "answer": "The system returns errors or timeouts on writes to ensure data consistency across nodes.",
    "explanation": "A CP system prioritizes consistency over availability; if nodes cannot agree, it rejects requests to prevent divergence. An AP system would serve stale data to maintain availability. Vector clocks are a conflict resolution mechanism, not a definition of CAP behavior.",
    "difficulty": "Intermediate"
  },
  {
    "id": 37,
    "question": "Which cache eviction policy is most suitable for a system requiring high hit rates for frequently accessed, non-temporal data?",
    "options": [
      "Least Recently Used (LRU)",
      "First-In-First-Out (FIFO)",
      "Last-In-First-Out (LIFO)",
      "Random Replacement"
    ],
    "answer": "Least Recently Used (LRU)",
    "explanation": "LRU evicts items that haven't been used for the longest time, assuming past access patterns predict future ones. FIFO and LIFO ignore access frequency, leading to cache thrashing. Random Replacement offers no performance guarantees.",
    "difficulty": "Intermediate"
  },
  {
    "id": 38,
    "question": "Why is the 'Two-Phase Commit' (2PC) protocol generally considered unsuitable for high-volume, distributed microservices architectures?",
    "options": [
      "It does not guarantee atomicity across distributed transactions.",
      "It requires a synchronous locking mechanism that blocks resources and reduces availability.",
      "It is limited to transactions within a single database shard.",
      "It relies on asynchronous messaging which creates eventual consistency."
    ],
    "answer": "It requires a synchronous locking mechanism that blocks resources and reduces availability.",
    "explanation": "2PC is a blocking protocol; if the coordinator fails, participants may hold locks indefinitely, degrading system availability. It guarantees atomicity but sacrifices scalability and latency. Sagas are preferred for long-running distributed transactions.",
    "difficulty": "Intermediate"
  },
  {
    "id": 39,
    "question": "What is the primary benefit of using a 'Consistent Hashing' ring for load balancing in a distributed cache cluster?",
    "options": [
      "It ensures an equal distribution of keys regardless of the number of nodes.",
      "It minimizes the number of keys remapped when nodes are added or removed.",
      "It guarantees that all hot keys are stored on the same physical node.",
      "It eliminates the need for replication across cache nodes."
    ],
    "answer": "It minimizes the number of keys remapped when nodes are added or removed.",
    "explanation": "Consistent hashing maps data to a continuum of points, ensuring that adding/removing a node only affects its immediate neighbors. Standard hashing modulo-N requires massive data remapping during scaling events.",
    "difficulty": "Intermediate"
  },
  {
    "id": 40,
    "question": "In a 'Write-Through' caching strategy, how is data synchronization handled between the cache and the persistent database?",
    "options": [
      "Data is written to the cache first and lazily synced to the database during off-peak hours.",
      "Data is written to the database and the cache simultaneously in a single transaction.",
      "Data is written to the cache, and the database is updated only after the cache entry is evicted.",
      "The cache invalidates the data, and the client writes directly to the database."
    ],
    "answer": "Data is written to the database and the cache simultaneously in a single transaction.",
    "explanation": "Write-through ensures data consistency by writing to the cache and backing store synchronously. Write-back writes to the cache only, syncing later. Look-aside leaves the cache out of the write path entirely.",
    "difficulty": "Intermediate"
  },
  {
    "id": 41,
    "question": "Which scenario best describes the 'Thundering Herd' problem, and how does it manifest?",
    "options": [
      "A database deadlock occurs when multiple transactions attempt to update the same row.",
      "A network storm causes latency spikes due to excessive packet retransmissions.",
      "A mass of expired cache requests simultaneously hits the origin database, overwhelming it.",
      "Multiple load balancers route traffic to a decommissioned server instance."
    ],
    "answer": "A mass of expired cache requests simultaneously hits the origin database, overwhelming it.",
    "explanation": "This occurs when a highly requested cached item expires, causing numerous concurrent requests to 'miss' and generate queries against the backend. It is mitigated by request coalescing or jittered expiration times.",
    "difficulty": "Intermediate"
  },
  {
    "id": 42,
    "question": "What is the function of a 'Circuit Breaker' in a microservices architecture?",
    "options": [
      "To route traffic based on the content of the request payload.",
      "To stop cascading failures by preventing requests to a failing service for a timeout period.",
      "To balance the load between different instances of a service using a round-robin algorithm.",
      "To transform the protocol of a request from HTTP to gRPC."
    ],
    "answer": "To stop cascading failures by preventing requests to a failing service for a timeout period.",
    "explanation": "A Circuit Breaker detects failure thresholds (e.g., timeouts) and trips, failing fast locally to protect the caller and give the downstream service time to recover. Load balancers distribute traffic; API Gateways handle routing.",
    "difficulty": "Intermediate"
  },
  {
    "id": 43,
    "question": "In a consistent hashing ring, what is the primary purpose of adding virtual nodes (vnodes) to physical servers?",
    "options": [
      "To ensure data encryption during transmission between nodes",
      "To distribute the data load more evenly and minimize the impact of server rebalancing",
      "To create a redundant backup of the hash table in a separate data center",
      "To allow the database to support SQL transactions instead of eventual consistency"
    ],
    "answer": "To distribute the data load more evenly and minimize the impact of server rebalancing",
    "explanation": "Virtual nodes reduce the variance in data distribution caused by a small number of physical nodes, ensuring a more uniform load across the cluster. They also simplify the rebalancing process; when a node is added or removed, only its specific virtual nodes' key ranges are affected, rather than a massive contiguous chunk of the ring.",
    "difficulty": "Advanced"
  },
  {
    "id": 44,
    "question": "When comparing the Token Bucket and Leaky Bucket algorithms for rate limiting, which statement accurately describes their fundamental difference regarding traffic bursts?",
    "options": [
      "Leaky Bucket allows for sustained bursts up to the bucket capacity, while Token Bucket strictly paces traffic at a constant rate.",
      "Token Bucket allows bursts up to the bucket capacity, while Leaky Bucket smooths traffic into a constant rate.",
      "Both algorithms block all traffic that exceeds the average rate, regardless of burst capacity configuration.",
      "Leaky Bucket is primarily used for ingress traffic, while Token Bucket is exclusively for egress traffic shaping."
    ],
    "answer": "Token Bucket allows bursts up to the bucket capacity, while Leaky Bucket smooths traffic into a constant rate.",
    "explanation": "The Token Bucket algorithm accumulates tokens (up to a max capacity) during inactivity, allowing a burst of traffic to pass immediately as long as tokens are available. The Leaky Bucket algorithm enforces a strict departure rate regardless of burst arrival, emptying the bucket at a fixed interval and smoothing the output traffic.",
    "difficulty": "Advanced"
  },
  {
    "id": 45,
    "question": "In the context of the CAP theorem, what characterizes a 'CA' system (Consistent and Available) in a distributed network?",
    "options": [
      "The system guarantees both consistency and availability during network partitions.",
      "The system is theoretically impossible in a distributed environment because network partitions are a reality (P is mandatory).",
      "The system sacrifices latency to ensure strong consistency and high availability simultaneously.",
      "The system uses Two-Phase Commit (2PC) to ensure atomicity across all nodes."
    ],
    "answer": "The system is theoretically impossible in a distributed environment because network partitions are a reality (P is mandatory).",
    "explanation": "According to the CAP theorem, a distributed system must tolerate partitions (P); therefore, you cannot have a true CA system in a distributed context. 'CA' systems effectively describe single-node databases (RDBMS) where partitioning is not a factor.",
    "difficulty": "Advanced"
  },
  {
    "id": 46,
    "question": "How does the 'Write Skew' anomaly manifest in a database under Snapshot Isolation?",
    "options": [
      "Two concurrent transactions update the same row, resulting in a lost update.",
      "A transaction reads a committed value from another transaction, leading to a non-repeatable read.",
      "Two concurrent transactions read overlapping data sets and make disjoint updates based on the read, violating a constraint.",
      "A transaction reads uncommitted data from a concurrent transaction that eventually rolls back."
    ],
    "answer": "Two concurrent transactions read overlapping data sets and make disjoint updates based on the read, violating a constraint.",
    "explanation": "Write Skew occurs when two transactions read the same consistent snapshot but modify disjoint sets of data based on that read. For example, two on-call doctors both resign concurrently after seeing the other is still on-call; the system checks 'count >= 2' individually but allows both to resign, leaving zero doctors.",
    "difficulty": "Advanced"
  },
  {
    "id": 47,
    "question": "What is the specific advantage of using a Merkle Tree (Hash Tree) for data synchronization in distributed filesystems like IPFS or Cassandra?",
    "options": [
      "It reduces the latency of disk writes by utilizing a B-Tree structure.",
      "It allows nodes to efficiently compare differing datasets by only exchanging root hashes.",
      "It ensures that data is encrypted at rest using a cryptographic hash function.",
      "It enables the system to prioritize read queries over write queries automatically."
    ],
    "answer": "It allows nodes to efficiently compare differing datasets by only exchanging root hashes.",
    "explanation": "If the root hashes of two trees differ, the branches can be traversed recursively to find the specific blocks of data that differ, significantly reducing the amount of data transferred for synchronization compared to transferring entire files or datasets.",
    "difficulty": "Advanced"
  },
  {
    "id": 48,
    "question": "In a Publish-Subscribe system using a message broker like Kafka, why is 'ordering of messages' strictly guaranteed only within a single partition?",
    "options": [
      "Because offsets are managed globally by the Zookeeper coordinator.",
      "Because partitions are assigned to specific consumer groups, preventing parallel consumption.",
      "Because consumers within a consumer group read from a topic in a round-robin fashion.",
      "Because parallel processing across multiple partitions introduces non-deterministic ordering."
    ],
    "answer": "Because parallel processing across multiple partitions introduces non-deterministic ordering.",
    "explanation": "While a partition ensures messages are stored in the order they are received, different partitions are processed independently and concurrently by different consumers. Consequently, the system cannot guarantee the global order of messages across the entire topic, only the order within a specific partition.",
    "difficulty": "Advanced"
  },
  {
    "id": 49,
    "question": "What is the primary trade-off when implementing a 'Write-Through' cache policy compared to a 'Write-Back' (Write-Behind) policy?",
    "options": [
      "Write-Through has higher write latency but higher data consistency; Write-Back has lower write latency but risks data loss.",
      "Write-Through increases CPU utilization; Write-Back reduces network throughput.",
      "Write-Through is only suitable for immutable data; Write-Back is exclusively for read-heavy workloads.",
      "Write-Through guarantees strong consistency on reads; Write-Back guarantees stale reads."
    ],
    "answer": "Write-Through has higher write latency but higher data consistency; Write-Back has lower write latency but risks data loss.",
    "explanation": "Write-Through synchronously updates both the cache and the persistent storage before confirming the write, ensuring consistency but adding latency. Write-Back updates only the cache and lazily syncs to storage, offering fast writes but posing a risk of data loss if the cache fails before syncing.",
    "difficulty": "Advanced"
  },
  {
    "id": 50,
    "question": "In the context of distributed transactions, why is the Two-Phase Commit (2PC) protocol considered 'blocking'?",
    "options": [
      "The protocol blocks all read operations on the database during the commit phase.",
      "If the coordinator fails, participants cannot reach a decision and must remain locked waiting for recovery.",
      "The protocol requires all network packets to be processed sequentially (blocking I/O).",
      "Participants block the client application until the transaction timeout expires."
    ],
    "answer": "If the coordinator fails, participants cannot reach a decision and must remain locked waiting for recovery.",
    "explanation": "In Phase 1 (Prepare), participants vote and then lock their resources. If the coordinator crashes after receiving votes but before sending the final commit/rollback command, participants are stuck in an 'uncertain' state, holding locks indefinitely until the coordinator recovers.",
    "difficulty": "Advanced"
  },
  {
    "id": 51,
    "question": "When designing a Key-Value store, why might you choose a Log-Structured Merge-tree (LSM tree) over a B+ Tree?",
    "options": [
      "LSM trees offer faster read performance for point-lookup queries on large datasets.",
      "LSM trees convert random writes into sequential writes, optimizing for write-heavy workloads.",
      "B+ Trees cannot handle range queries effectively, whereas LSM trees are optimized for them.",
      "LSM trees do not require compaction, whereas B+ Trees do."
    ],
    "answer": "LSM trees convert random writes into sequential writes, optimizing for write-heavy workloads.",
    "explanation": "LSM trees buffer writes in memory (MemTable) and flush them as sorted immutable files (SSTables) to disk, turning random disk I/O into sequential I/O. B+ Trees perform random writes for every update to maintain the balanced tree structure, which can be a bottleneck on spinning disks.",
    "difficulty": "Advanced"
  },
  {
    "id": 52,
    "question": "What is the 'Split-Brain' problem in distributed systems, and how is it typically mitigated?",
    "options": [
      "It occurs when two nodes think they are the leader; mitigated by requiring a majority quorum for election.",
      "It occurs when a database partition leads to data corruption; mitigated by RAID 5.",
      "It occurs when a network loop prevents message delivery; mitigated by Spanning Tree Protocol.",
      "It occurs when a process exceeds its memory limit; mitigated by setting an OOM killer."
    ],
    "answer": "It occurs when two nodes think they are the leader; mitigated by requiring a majority quorum for election.",
    "explanation": "Split-Brain occurs during a network partition where subsets of nodes cannot communicate, potentially leading to multiple 'leaders' in a clustered system. Requiring a majority (quorum) ensures that only one partition (the one with >50% of nodes) can form a valid government and continue processing.",
    "difficulty": "Advanced"
  },
  {
    "id": 53,
    "question": "Why is 'Exponential Backoff with Jitter' preferred over fixed-interval retries when handling transient failures in distributed systems?",
    "options": [
      "To guarantee that the request will eventually succeed after N attempts.",
      "To prevent synchronized retry storms (thundering herd) across multiple clients.",
      "To ensure that the server processes retries in FIFO order.",
      "To reduce the total amount of data sent over the network."
    ],
    "answer": "To prevent synchronized retry storms (thundering herd) across multiple clients.",
    "explanation": "If multiple clients fail simultaneously and retry on the same fixed schedule, they will overload the server in perfect sync (Thundering Herd). Jitter (randomness) scatters the retry times, smoothing out the load on the recovering system.",
    "difficulty": "Advanced"
  },
  {
    "id": 54,
    "question": "What is the role of the 'Sidecar' pattern in a microservices architecture (e.g., using Envoy or NGINX)?",
    "options": [
      "To act as a primary database for the microservice to ensure data persistence.",
      "To abstract infrastructure features (TLS termination, logging, monitoring) from the application logic.",
      "To serve as a user-facing frontend for the backend microservices.",
      "To replace the need for an API Gateway by handling external routing."
    ],
    "answer": "To abstract infrastructure features (TLS termination, logging, monitoring) from the application logic.",
    "explanation": "A sidecar proxy runs alongside the application service (sharing the lifecycle) and handles cross-cutting concerns like network communication, observability, and security. This offloads these tasks from the business logic, allowing polyglot architectures.",
    "difficulty": "Advanced"
  },
  {
    "id": 55,
    "question": "How does a 'Bloom Filter' optimize the read path in a storage engine like Cassandra or HBase?",
    "options": [
      "It compresses the data on disk to reduce I/O latency.",
      "It eliminates the need for a Write-Ahead Log (WAL).",
      "It rapidly checks if an element is definitely not in a set, reducing unnecessary disk reads.",
      "It sorts the data in memory to facilitate binary search."
    ],
    "answer": "It rapidly checks if an element is definitely not in a set, reducing unnecessary disk reads.",
    "explanation": "A Bloom Filter is a probabilistic memory structure. Before accessing the disk, the storage engine checks the filter; if it says the key is not present, the disk seek is skipped. A positive result is probabilistic (might be a false positive), so a disk check is still required.",
    "difficulty": "Advanced"
  },
  {
    "id": 56,
    "question": "What is the primary function of the 'SSTable' (Sorted String Table) in LSM-tree based databases?",
    "options": [
      "To store data in memory for fast random access before flushing to disk.",
      "To persist data on disk in sorted, immutable order for efficient range scans and merging.",
      "To handle client connections and rate limiting.",
      "To manage the replication log between master and slave nodes."
    ],
    "answer": "To persist data on disk in sorted, immutable order for efficient range scans and merging.",
    "explanation": "When the MemTable (in-memory buffer) fills up, it is written to disk as an SSTable. Because SSTables are immutable and sorted by key, merging them (compaction) is efficient, and finding data is fast (usually via a sparse index and bloom filter).",
    "difficulty": "Advanced"
  },
  {
    "id": 57,
    "question": "In the Saga pattern for distributed transactions, what is a 'Compensating Transaction'?",
    "options": [
      "A transaction that retries the failed operation until it succeeds.",
      "A transaction that commits changes to a backup database.",
      "A transaction that reverses the effects of a previously committed step in the saga.",
      "A transaction that locks the resource across all microservices."
    ],
    "answer": "A transaction that reverses the effects of a previously committed step in the saga.",
    "explanation": "Since Sagas break transactions into multiple local steps (without a global lock), if a step fails, the system must run compensating transactions (undo logic) for all preceding steps that successfully committed, to restore the system to a consistent state.",
    "difficulty": "Advanced"
  },
  {
    "id": 58,
    "question": "What is 'read-your-writes' consistency, and how does it affect user experience?",
    "options": [
      "It ensures that a user never sees stale data, even after reading from a replica.",
      "It guarantees that a user immediately sees their own updates on subsequent reads, regardless of replication lag.",
      "It prevents concurrent writers from overwriting each other's data.",
      "It ensures that once a write is acknowledged, all other users see it immediately."
    ],
    "answer": "It guarantees that a user immediately sees their own updates on subsequent reads, regardless of replication lag.",
    "explanation": "This consistency model ensures that for a specific user, the session is 'sticky' to the version of data they just wrote or the primary node. It solves the annoyance of a user updating a profile and then immediately seeing the old data due to reading from a lagged replica.",
    "difficulty": "Advanced"
  },
  {
    "id": 59,
    "question": "Why is UDP (User Datagram Protocol) often preferred over TCP for real-time streaming applications like gaming or live video?",
    "options": [
      "UDP guarantees packet delivery, ensuring no video artifacts.",
      "UDP implements congestion control to prevent network flooding.",
      "UDP sacrifices reliability for speed and lower latency by avoiding handshake and retransmission overhead.",
      "UDP packets are encrypted by default, whereas TCP packets are not."
    ],
    "answer": "UDP sacrifices reliability for speed and lower latency by avoiding handshake and retransmission overhead.",
    "explanation": "Real-time applications are latency-sensitive. It is better to drop a few frames (packets) and keep the stream moving with low latency than to pause the stream to retransmit lost packets (TCP's behavior), which causes jitter and buffering.",
    "difficulty": "Advanced"
  },
  {
    "id": 60,
    "question": "What is the 'HyperLogLog' (HLL) data structure primarily used for in large-scale system design?",
    "options": [
      "To find the exact intersection of two large datasets.",
      "To estimate the cardinality (number of unique elements) of a set using minimal memory.",
      "To store time-series data for monitoring metrics.",
      "To implement distributed locking across a cluster."
    ],
    "answer": "To estimate the cardinality (number of unique elements) of a set using minimal memory.",
    "explanation": "HyperLogLog is a probabilistic algorithm that can approximate the number of unique items (e.g., daily active users) in a multiset with very high accuracy (standard error < 1%) using constant space (roughly 12kb), regardless of the dataset size.",
    "difficulty": "Advanced"
  },
  {
    "id": 61,
    "question": "What is the 'Gossip Protocol' (or Epidemic Protocol) used for in distributed database clusters like Cassandra or Consul?",
    "options": [
      "To establish a TCP connection between the client and the database.",
      "To disseminate state information and cluster membership data in a decentralized, peer-to-peer manner.",
      "To elect the leader node in a centralized master-slave architecture.",
      "To serialize database writes to the transaction log."
    ],
    "answer": "To disseminate state information and cluster membership data in a decentralized, peer-to-peer manner.",
    "explanation": "Gossip protocols allow nodes to periodically exchange state information with random peers, ensuring that cluster metadata eventually propagates to all nodes. This makes the system robust and decentralized, avoiding single points of failure in the discovery service.",
    "difficulty": "Advanced"
  },
  {
    "id": 62,
    "question": "In the context of stream processing (e.g., Apache Flink, Kafka Streams), what is 'Event Time' versus 'Processing Time'?",
    "options": [
      "Event Time is when the server processes the record; Processing Time is when the client sends the request.",
      "Event Time is the timestamp included in the data record when it occurred; Processing Time is when the stream processor sees it.",
      "Event Time is used for batch processing; Processing Time is used for real-time streaming.",
      "Event Time is always exactly 5 seconds behind Processing Time."
    ],
    "answer": "Event Time is the timestamp included in the data record when it occurred; Processing Time is when the stream processor sees it.",
    "explanation": "Processing Time is nondeterministic (affected by network lag, queueing), while Event Time is deterministic based on the business event. Calculations (like aggregations) based on Event Time handle out-of-order data and late arrivals correctly.",
    "difficulty": "Advanced"
  },
  {
    "id": 63,
    "question": "What is the 'Phantom Read' anomaly in database isolation levels?",
    "options": [
      "Reading uncommitted dirty data from another transaction.",
      "Reading the same row twice and getting different values.",
      "Executing the same range query twice within a transaction and seeing a different set of rows due to an insert.",
      "Reading data that has been deleted by another transaction."
    ],
    "answer": "Executing the same range query twice within a transaction and seeing a different set of rows due to an insert.",
    "explanation": "A Phantom Read occurs when a transaction reads a set of rows matching a condition, and another transaction inserts (or deletes) a row that matches that condition. If the first transaction re-runs the query, it sees new (or missing) 'phantom' rows.",
    "difficulty": "Advanced"
  },
  {
    "id": 64,
    "question": "When designing a global system, how does 'GeoDNS' (Latency-based Routing) typically decide which IP address to return to a user?",
    "options": [
      "It returns the IP of the server with the highest available bandwidth.",
      "It returns the IP of the server that is geographically closest to the user's resolved location.",
      "It returns the IP of the server that has the most free CPU cycles.",
      "It returns the IP of the Master database node regardless of location."
    ],
    "answer": "It returns the IP of the server that is geographically closest to the user's resolved location.",
    "explanation": "GeoDNS observes the source IP of the DNS query to determine the user's approximate location and returns an IP address for a data center (PoP) nearest to them. This reduces network latency by connecting the user to the nearest frontend server.",
    "difficulty": "Advanced"
  },
  {
    "id": 65,
    "question": "What is 'Space Amplification' in the context of storage engines?",
    "options": [
      "The ratio of logical data size to the physical storage utilized (including overhead and fragmentation).",
      "The extra memory required to index the data on disk.",
      "The amount of bandwidth consumed by replication traffic.",
      "The increase in latency caused by storage seeking."
    ],
    "answer": "The ratio of logical data size to the physical storage utilized (including overhead and fragmentation).",
    "explanation": "Space amplification represents the inefficiency of storage usage. An LSM tree with multiple levels of SSTables might keep multiple versions of the same key or require extra space for compaction, meaning the actual disk usage is much higher than the raw data size (e.g., 10x amplification).",
    "difficulty": "Advanced"
  },
  {
    "id": 66,
    "question": "Why is 'Collocated Compute' (Data Locality) crucial for big data processing frameworks like Hadoop or Spark?",
    "options": [
      "It reduces network congestion by moving the processing code to where the data resides.",
      "It ensures that the data is encrypted before being processed.",
      "It allows the CPU to process data faster than the memory bus speed.",
      "It prevents the data from being replicated across the cluster."
    ],
    "answer": "It reduces network congestion by moving the processing code to where the data resides.",
    "explanation": "Moving petabytes of data over the network is prohibitively expensive and slow. Instead, the scheduler assigns tasks to nodes that already host the specific data blocks (HDFS blocks) in local disks, minimizing network I/O and processing time.",
    "difficulty": "Advanced"
  },
  {
    "id": 67,
    "question": "What is 'sticky session' (session affinity) in load balancing, and when is it required?",
    "options": [
      "Directing traffic to the server with the fewest active connections; required for long-lived connections.",
      "Routing a user's requests to the same server instance for the duration of a session; required for stateful applications.",
      "Keeping a session open indefinitely; required for high security.",
      "Balancing traffic based on HTTP headers; required for RESTful APIs."
    ],
    "answer": "Routing a user's requests to the same server instance for the duration of a session; required for stateful applications.",
    "explanation": "If a server stores user session state (e.g., HTTP Session) in local memory, subsequent requests from that user must hit that specific server to access the state. Stateless applications (using external stores like Redis) do not require sticky sessions.",
    "difficulty": "Advanced"
  },
  {
    "id": 68,
    "question": "What distinguishes a 'Zero-Copy' network stack (like `sendfile`) from a traditional network I/O approach?",
    "options": [
      "Zero-Copy compresses data before sending to reduce bandwidth.",
      "Zero-Copy avoids copying data between the kernel space and user space buffers, reducing CPU overhead.",
      "Zero-Copy uses UDP instead of TCP for faster transmission.",
      "Zero-Copy requires dedicated hardware to function."
    ],
    "answer": "Zero-Copy avoids copying data between the kernel space and user space buffers, reducing CPU overhead.",
    "explanation": "Traditional I/O involves reading a file into kernel space, copying to user space, and copying back to kernel space for the socket. `sendfile` transfers data directly from the file descriptor to the socket within kernel space, drastically reducing context switching and CPU cycles.",
    "difficulty": "Advanced"
  },
  {
    "id": 69,
    "question": "In a system using CRDTs (Conflict-free Replicated Data Types), what is the mathematical property that ensures convergence?",
    "options": [
      "Commutativity, Associativity, and Idempotence.",
      "Symmetry and Transitivity.",
      "Encryption and Decryption.",
      "Atomicity and Consistency."
    ],
    "answer": "Commutativity, Associativity, and Idempotence.",
    "explanation": "For state to converge without coordination, the merge operation must be order-independent (Commutative), groupable (Associative), and safe to apply multiple times (Idempotent). This ensures that regardless of message delivery order or duplication, all replicas eventually reach the same state.",
    "difficulty": "Advanced"
  },
  {
    "id": 70,
    "question": "What is the 'Forward Proxy' pattern primarily used for in enterprise network architectures?",
    "options": [
      "To balance incoming traffic among a pool of web servers.",
      "To act on behalf of a client to access resources on the internet (e.g., content filtering, anonymity).",
      "To terminate SSL connections for web servers.",
      "To cache database queries for the application layer."
    ],
    "answer": "To act on behalf of a client to access resources on the internet (e.g., content filtering, anonymity).",
    "explanation": "A Forward Proxy sits in front of clients (internal network), managing outbound traffic to the internet. A Reverse Proxy sits in front of servers, managing inbound traffic from the internet.",
    "difficulty": "Advanced"
  },
  {
    "id": 71,
    "question": "What is the 'Write Amplification' factor in SSDs and Log-Structured storage?",
    "options": [
      "The ratio of logical data written to the physical data written to storage.",
      "The number of times data is read from the cache versus the disk.",
      "The speed increase gained by writing in parallel to multiple disks.",
      "The number of replicas created for disaster recovery."
    ],
    "answer": "The ratio of logical data written to the physical data written to storage.",
    "explanation": "Due to erasure blocks (SSD) or compaction/merging (LSM trees), a single logical write might trigger multiple physical writes. A high write amplification reduces device lifespan and degrades throughput.",
    "difficulty": "Advanced"
  },
  {
    "id": 72,
    "question": "In the context of Microservices, what is the 'Anti-Corruption Layer' (ACL) pattern?",
    "options": [
      "A security layer that prevents SQL injection attacks.",
      "A translation layer that isolates a new domain model from a legacy or external subsystem's model.",
      "A firewall that blocks unauthorized API calls.",
      "A cache layer that prevents stale data from corrupting the main database."
    ],
    "answer": "A translation layer that isolates a new domain model from a legacy or external subsystem's model.",
    "explanation": "When integrating with a legacy system or external API with a poorly designed or incompatible contract, the ACL implements the new system's domain logic internally but translates communication to the legacy protocol externally, preventing the legacy system's 'bad' design from 'corrupting' the new clean architecture.",
    "difficulty": "Advanced"
  }
]