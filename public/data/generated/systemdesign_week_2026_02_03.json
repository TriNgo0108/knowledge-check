[
  {
    "id": 1,
    "question": "What is the primary difference between vertical scaling (scaling up) and horizontal scaling (scaling out)?",
    "options": [
      "Vertical scaling adds more copies of the existing resources, while horizontal scaling upgrades the hardware of a single node.",
      "Vertical scaling increases the capacity of a single machine (CPU/RAM), while horizontal scaling adds more machines to the pool.",
      "Vertical scaling is limited to cloud environments, while horizontal scaling is restricted to physical on-premise servers.",
      "Horizontal scaling requires code refactoring, whereas vertical scaling is automatic and requires no configuration changes."
    ],
    "answer": "Vertical scaling increases the capacity of a single machine (CPU/RAM), while horizontal scaling adds more machines to the pool.",
    "explanation": "Vertical scaling involves adding more power to an existing machine (e.g., more RAM), whereas horizontal scaling involves adding more nodes to a system to distribute the load. Horizontal scaling generally offers better fault tolerance and limits imposed by single hardware specs.",
    "difficulty": "Beginner"
  },
  {
    "id": 2,
    "question": "In the context of the CAP Theorem for distributed databases, which requirement ensures that every read receives the most recent write or an error?",
    "options": [
      "Availability",
      "Partition Tolerance",
      "Consistency",
      "Latency"
    ],
    "answer": "Consistency",
    "explanation": "Consistency in CAP implies that every request receives a response that is up-to-date and consistent across all nodes. Availability guarantees a response (even if stale), and Partition Tolerance ensures the system operates despite network failures.",
    "difficulty": "Beginner"
  },
  {
    "id": 3,
    "question": "Which architectural pattern is primarily used to distribute incoming network traffic across multiple servers to ensure no single server bears too much demand?",
    "options": [
      "Caching",
      "Load Balancing",
      "Sharding",
      "Replication"
    ],
    "answer": "Load Balancing",
    "explanation": "Load balancers act as the traffic cop, sitting in front of servers and routing client requests across all servers capable of fulfilling those requests. Caching stores data for faster access, sharding splits databases, and replication copies data.",
    "difficulty": "Beginner"
  },
  {
    "id": 4,
    "question": "What is the main advantage of using a Content Delivery Network (CDN)?",
    "options": [
      "To store the primary database for the application.",
      "To execute server-side business logic closer to the user.",
      "To reduce latency by delivering static content from edge servers geographically closer to the user.",
      "To secure the application against DDoS attacks exclusively."
    ],
    "answer": "To reduce latency by delivering static content from edge servers geographically closer to the user.",
    "explanation": "CDNs cache content (images, videos, CSS) in edge locations around the world. This minimizes the physical distance data travels, reducing latency and improving load times. While they can help with DDoS mitigation, their primary design goal is latency reduction.",
    "difficulty": "Beginner"
  },
  {
    "id": 5,
    "question": "Which database characteristic ensures that a transaction is all-or-nothing, meaning that if one part of the transaction fails, the entire transaction fails?",
    "options": [
      "Isolation",
      "Durability",
      "Atomicity",
      "Consistency"
    ],
    "answer": "Atomicity",
    "explanation": "Atomicity guarantees that each transaction is treated as a single unit, which either succeeds completely or fails completely. It is the 'A' in ACID properties. Consistency refers to database validity, Isolation to concurrent transactions, and Durability to saved data.",
    "difficulty": "Beginner"
  },
  {
    "id": 6,
    "question": "What distinguishes a SQL database from a NoSQL database regarding data structure?",
    "options": [
      "SQL databases store data in tables with rows and columns, while NoSQL databases use varied data models (document, key-value, graph).",
      "SQL databases are primarily used for caching, while NoSQL databases are used for permanent storage.",
      "SQL databases cannot scale horizontally, whereas NoSQL databases cannot scale vertically.",
      "NoSQL databases enforce strict schemas (relations), while SQL databases are schema-less."
    ],
    "answer": "SQL databases store data in tables with rows and columns, while NoSQL databases use varied data models (document, key-value, graph).",
    "explanation": "SQL databases are relational and table-based. NoSQL databases are non-relational and offer flexibility for unstructured data, using formats like JSON documents.",
    "difficulty": "Beginner"
  },
  {
    "id": 7,
    "question": "What is the primary function of 'caching' in system design?",
    "options": [
      "To permanently store user data for long-term analysis.",
      "To store the results of expensive computations or frequently accessed data to reduce future access time.",
      "To encrypt data as it moves between the client and the server.",
      "To divide a large database into smaller chunks to improve manageability."
    ],
    "answer": "To store the results of expensive computations or frequently accessed data to reduce future access time.",
    "explanation": "Caching stores subsets of data in high-speed storage (RAM) so that future requests for that data can be served faster than accessing the slower primary database. It does not replace persistent storage or perform encryption.",
    "difficulty": "Beginner"
  },
  {
    "id": 8,
    "question": "Which type of database replication involves copying data from a primary node to one or more secondary nodes, where secondaries typically serve read traffic?",
    "options": [
      "Sharding",
      "Leader-Follower (Master-Slave) Replication",
      "Vertical Partitioning",
      "Consistent Hashing"
    ],
    "answer": "Leader-Follower (Master-Slave) Replication",
    "explanation": "Leader-Follower replication designates one node as the primary source of truth (write) and copies data to replicas (read). Sharding partitions data, vertical partitioning splits tables, and consistent hashing is a routing technique.",
    "difficulty": "Beginner"
  },
  {
    "id": 9,
    "question": "In a microservices architecture, how do services typically communicate with each other?",
    "options": [
      "Through direct memory access.",
      "By sharing a single large monolithic database.",
      "Via well-defined APIs (HTTP/REST) or message queues (event-driven).",
      "By copying code libraries into each service container."
    ],
    "answer": "Via well-defined APIs (HTTP/REST) or message queues (event-driven).",
    "explanation": "Microservices are decoupled and communicate over the network using lightweight protocols like HTTP/REST for synchronous calls or message brokers for asynchronous events. Direct memory access or shared databases create tight coupling.",
    "difficulty": "Beginner"
  },
  {
    "id": 10,
    "question": "What is the purpose of a 'Round Robin' algorithm in load balancing?",
    "options": [
      "To route requests based on the current server with the fewest active connections.",
      "To route requests based on the server with the highest CPU availability.",
      "To distribute requests sequentially to each server in a circular order.",
      "To route all requests to a single server until it reaches maximum capacity."
    ],
    "answer": "To distribute requests sequentially to each server in a circular order.",
    "explanation": "Round Robin iterates through a list of servers in order, distributing requests evenly. Unlike 'Least Connections,' it does not account for the current load or connection count of the server, only the count of requests assigned.",
    "difficulty": "Beginner"
  },
  {
    "id": 11,
    "question": "What is the primary trade-off when using database indexes?",
    "options": [
      "Increased read latency vs. decreased write latency.",
      "Increased read speed vs. decreased write speed and increased storage usage.",
      "Increased data consistency vs. decreased availability.",
      "Increased security vs. decreased scalability."
    ],
    "answer": "Increased read speed vs. decreased write speed and increased storage usage.",
    "explanation": "Indexes speed up data retrieval (SELECT queries) but slow down data modification (INSERT/UPDATE/DELETE) because the index itself must be updated, and they consume additional disk space.",
    "difficulty": "Beginner"
  },
  {
    "id": 12,
    "question": "Which system design concept describes the time delay between a client sending a request and receiving a response?",
    "options": [
      "Throughput",
      "Latency",
      "Bandwidth",
      "Jitter"
    ],
    "answer": "Latency",
    "explanation": "Latency measures the time it takes for data to travel from source to destination. Throughput measures the amount of data transferred in a given time, Bandwidth is capacity, and Jitter is the variance in latency.",
    "difficulty": "Beginner"
  },
  {
    "id": 13,
    "question": "What is the primary purpose of 'Sharding' a database?",
    "options": [
      "To create multiple copies of the same database for redundancy.",
      "To improve query performance by distributing data across multiple distinct databases based on a shard key.",
      "To compress data to save storage space.",
      "To ensure ACID compliance across distributed transactions."
    ],
    "answer": "To improve query performance by distributing data across multiple distinct databases based on a shard key.",
    "explanation": "Sharding partitions a large database into smaller, faster, more easily managed parts called data shards, distributed across multiple servers. Replication copies data for redundancy, not partitioning.",
    "difficulty": "Beginner"
  },
  {
    "id": 14,
    "question": "What is 'Eventual Consistency' in distributed systems?",
    "options": [
      "The guarantee that all nodes see the same data at the exact same time.",
      "The guarantee that the system will return an error if data is not consistent.",
      "The guarantee that, given enough time (and no new updates), all replicas will converge to the same state.",
      "The guarantee that data is written to the primary node before being acknowledged."
    ],
    "answer": "The guarantee that, given enough time (and no new updates), all replicas will converge to the same state.",
    "explanation": "Eventual consistency is a model where reads are not guaranteed to return the latest write immediately, but the system guarantees that updates propagate to all nodes eventually. Strong consistency requires immediate synchronization.",
    "difficulty": "Beginner"
  },
  {
    "id": 15,
    "question": "Which HTTP status code indicates that the server could not understand the request due to invalid syntax (often a client error)?",
    "options": [
      "200",
      "500",
      "400",
      "301"
    ],
    "answer": "400",
    "explanation": "HTTP 400 Bad Request is a client-side error status code indicating the server cannot process the request due to malformed syntax. 200 is OK, 500 is a server error, and 301 is a permanent redirect.",
    "difficulty": "Beginner"
  },
  {
    "id": 16,
    "question": "What is the primary benefit of stateless services in a scalable architecture?",
    "options": [
      "They store user session data locally on the server for faster access.",
      "They simplify horizontal scaling because any server can handle any request.",
      "They require sticky sessions to function correctly.",
      "They eliminate the need for a load balancer."
    ],
    "answer": "They simplify horizontal scaling because any server can handle any request.",
    "explanation": "Stateless services do not retain client context between requests. This allows any instance in a pool to respond to any request, making scaling and failover significantly easier.",
    "difficulty": "Beginner"
  },
  {
    "id": 17,
    "question": "What is the role of an API Gateway in a microservices architecture?",
    "options": [
      "To act as a database for all microservices.",
      "To serve static files like HTML and CSS.",
      "To provide a single entry point for clients, handling requests, routing, and cross-cutting concerns (auth, rate limiting).",
      "To compile code for the microservices dynamically."
    ],
    "answer": "To provide a single entry point for clients, handling requests, routing, and cross-cutting concerns (auth, rate limiting).",
    "explanation": "API Gateways manage request routing, composition, and protocol translation, and handle cross-cutting concerns like security and rate limiting, shielding the client from the complexity of internal microservices.",
    "difficulty": "Beginner"
  },
  {
    "id": 18,
    "question": "Which caching strategy writes data to the cache and the persistent database simultaneously?",
    "options": [
      "Cache-Aside (Lazy Loading)",
      "Write-Through",
      "Write-Back (Write-Behind)",
      "Write-Around"
    ],
    "answer": "Write-Through",
    "explanation": "In Write-Through caching, data is written to the cache and the backing store at the same time. Cache-Aside loads on miss, Write-Back writes only to cache initially, and Write-Around writes directly to DB.",
    "difficulty": "Beginner"
  },
  {
    "id": 19,
    "question": "What is 'Throughput' in the context of system performance?",
    "options": [
      "The total time taken to complete a specific task.",
      "The amount of data that can be transferred in a specific time period.",
      "The number of requests or transactions a system can handle per unit of time.",
      "The delay between a request and a response."
    ],
    "answer": "The number of requests or transactions a system can handle per unit of time.",
    "explanation": "Throughput measures the rate of processing (e.g., requests per second). Latency measures the duration of a single request. While data transfer rate is bandwidth, throughput usually refers to transaction processing rate.",
    "difficulty": "Beginner"
  },
  {
    "id": 20,
    "question": "Why might a system designer choose a 'Master-Slave' replication topology?",
    "options": [
      "To allow writes on any node.",
      "To offload read traffic to slave nodes, reducing the load on the master.",
      "To eliminate the risk of data inconsistency.",
      "To ensure that if the master fails, no data is ever lost."
    ],
    "answer": "To offload read traffic to slave nodes, reducing the load on the master.",
    "explanation": "Master-Slave replication allows the master to handle all writes while slaves handle read queries, improving performance. It does not allow multi-master writes, guarantees immediate consistency, or eliminates failure risks (without complex failover).",
    "difficulty": "Beginner"
  },
  {
    "id": 21,
    "question": "What is the function of a 'Reverse Proxy'?",
    "options": [
      "To hide the internal server architecture and forward client requests to the appropriate backend server.",
      "To forward requests from the internal network to the internet.",
      "To cache responses on the client-side browser.",
      "To act as a firewall blocking internal employees."
    ],
    "answer": "To hide the internal server architecture and forward client requests to the appropriate backend server.",
    "explanation": "A reverse proxy sits in front of web servers and forwards client requests (e.g., Nginx) to those servers. It provides security, load balancing, and acceleration. A forward proxy acts for the client.",
    "difficulty": "Beginner"
  },
  {
    "id": 22,
    "question": "Which hashing strategy is commonly used to minimize data redistribution when adding or removing servers in a distributed cache?",
    "options": [
      "Modulo Hashing",
      "Consistent Hashing",
      "Linear Hashing",
      "Static Hashing"
    ],
    "answer": "Consistent Hashing",
    "explanation": "Consistent hashing maps data to a ring, ensuring that only a small fraction of keys are remapped when a server is added or removed. Modulo hashing causes a massive reshuffling of data whenever the number of nodes changes.",
    "difficulty": "Beginner"
  },
  {
    "id": 23,
    "question": "What is the primary use case for a Message Queue (like RabbitMQ or Kafka)?",
    "options": [
      "To store relational data permanently.",
      "To enable asynchronous communication and decouple services.",
      "To synchronize database transactions across nodes.",
      "To route real-time video traffic."
    ],
    "answer": "To enable asynchronous communication and decouple services.",
    "explanation": "Message queues allow services to communicate asynchronously, buffering messages until the consumer is ready to process them. This decouples producers from consumers and improves resilience to traffic spikes.",
    "difficulty": "Beginner"
  },
  {
    "id": 24,
    "question": "In database terms, what is a 'Denial of Service' (DoS) attack attempting to do?",
    "options": [
      "Steal sensitive data.",
      "Impersonate a valid user.",
      "Make a service unavailable by overwhelming it with traffic.",
      "Inject malicious SQL code."
    ],
    "answer": "Make a service unavailable by overwhelming it with traffic.",
    "explanation": "A DoS attack aims to exhaust system resources (CPU, memory, bandwidth) so that legitimate users cannot access the service. It is an availability attack, not a confidentiality or integrity attack like SQL injection.",
    "difficulty": "Beginner"
  },
  {
    "id": 25,
    "question": "What is 'Fan-out' in the context of messaging and data distribution?",
    "options": [
      "Aggregating multiple inputs into a single output.",
      "Delivering a single message to multiple consumers or destinations.",
      "Filtering messages based on content.",
      "Deleting messages after a single read."
    ],
    "answer": "Delivering a single message to multiple consumers or destinations.",
    "explanation": "Fan-out refers to broadcasting a single message to multiple recipients (e.g., sending a notification to many users). Aggregation is the opposite (fan-in).",
    "difficulty": "Beginner"
  },
  {
    "id": 26,
    "question": "What is the main benefit of using 'Connection Pooling' in database interactions?",
    "options": [
      "It encrypts the database credentials.",
      "It reduces the overhead of establishing a new connection for every request.",
      "It allows the database to handle more writes than reads.",
      "It automatically shards the database tables."
    ],
    "answer": "It reduces the overhead of establishing a new connection for every request.",
    "explanation": "Creating a database connection is expensive. Connection pooling maintains a cache of connections to reuse, significantly reducing latency and resource usage for frequent DB access.",
    "difficulty": "Beginner"
  },
  {
    "id": 27,
    "question": "Which deployment strategy involves creating a new, duplicate environment (Green) alongside the existing one (Blue) and switching traffic to it?",
    "options": [
      "Rolling Deployment",
      "Blue-Green Deployment",
      "Canary Deployment",
      "Big Bang Deployment"
    ],
    "answer": "Blue-Green Deployment",
    "explanation": "Blue-Green deployment creates two identical environments (Blue and Green). Traffic is switched from the old version (Blue) to the new version (Green), allowing for instant rollback if errors occur. Canary rolls out to a subset first.",
    "difficulty": "Beginner"
  },
  {
    "id": 28,
    "question": "What does 'Idempotency' mean in API design?",
    "options": [
      "The request requires a unique ID.",
      "Making the same request multiple times has the same effect as making it once.",
      "The request must be processed in the exact order it was received.",
      "The request is encrypted."
    ],
    "answer": "Making the same request multiple times has the same effect as making it once.",
    "explanation": "An idempotent operation can be applied multiple times without changing the result beyond the initial application (e.g., HTTP PUT or DELETE). This is crucial for reliability in distributed systems where retries are common.",
    "difficulty": "Beginner"
  },
  {
    "id": 29,
    "question": "Which service is primarily responsible for translating a domain name (like google.com) into an IP address?",
    "options": [
      "CDN",
      "Load Balancer",
      "DNS (Domain Name System)",
      "Proxy Server"
    ],
    "answer": "DNS (Domain Name System)",
    "explanation": "DNS acts as the internet's phonebook, translating human-readable domain names into machine-readable IP addresses. CDNs serve content, Load Balancers distribute traffic, and Proxies forward requests.",
    "difficulty": "Beginner"
  },
  {
    "id": 30,
    "question": "What is the primary characteristic of a 'NoSQL' Document Store (like MongoDB)?",
    "options": [
      "Data is stored in tables with foreign keys.",
      "Data is stored as flexible, JSON-like documents.",
      "Data is stored as key-value pairs only.",
      "Data is represented as nodes and edges."
    ],
    "answer": "Data is stored as flexible, JSON-like documents.",
    "explanation": "Document stores organize data into documents (JSON/BSON) that map to objects in application code. Key-Value stores are simpler, Graph stores use nodes/edges, and Relational stores use tables.",
    "difficulty": "Beginner"
  },
  {
    "id": 31,
    "question": "Which of the following best describes 'Rate Limiting'?",
    "options": [
      "Increasing the speed of the network.",
      "Controlling the rate of traffic sent or received by a service to prevent overload.",
      "Limiting the size of the database.",
      "Prioritizing specific users over others."
    ],
    "answer": "Controlling the rate of traffic sent or received by a service to prevent overload.",
    "explanation": "Rate limiting restricts how many requests a user or client can make in a given timeframe. It protects resources from being exhausted by abusive usage or traffic spikes.",
    "difficulty": "Beginner"
  },
  {
    "id": 32,
    "question": "What is the 'Third-Party Copy' problem in the context of cloud storage?",
    "options": [
      "Data being copied to the cloud without proper encryption.",
      "The inability to move data directly between two cloud providers without routing through the client.",
      "Vendors keeping copies of data for backup purposes.",
      "Copying data to a third-party tape drive."
    ],
    "answer": "The inability to move data directly between two cloud providers without routing through the client.",
    "explanation": "The 'Third-Party Copy' problem (or Egress fees) involves the cost and technical difficulty of moving data between clouds. Standard mechanisms often require downloading data locally (ingress/egress) and re-uploading.",
    "difficulty": "Beginner"
  },
  {
    "id": 33,
    "question": "Which architectural principle suggests keeping the web tier and database tier on separate servers?",
    "options": [
      "Scalability",
      "Separation of Concerns",
      "Reliability",
      "Portability"
    ],
    "answer": "Scalability",
    "explanation": "Separating the web tier (stateless, easy to scale) from the database tier (stateful, harder to scale) allows each layer to scale independently based on specific load requirements. While it also relates to separation of concerns, the primary driver in scaling web apps is independent scaling.",
    "difficulty": "Beginner"
  },
  {
    "id": 34,
    "question": "In a Key-Value Store, how is data typically retrieved?",
    "options": [
      "By querying a specific index.",
      "By using a unique Key associated with the Value.",
      "By joining multiple tables.",
      "By traversing a graph of relationships."
    ],
    "answer": "By using a unique Key associated with the Value.",
    "explanation": "Key-Value stores (like Redis or DynamoDB) are designed for O(1) retrieval by providing a unique key. They do not support complex queries like SQL joins or graph traversals.",
    "difficulty": "Beginner"
  },
  {
    "id": 35,
    "question": "In the context of the CAP theorem, when a network partition occurs (P) in a distributed database system, which combination of properties is impossible to simultaneously guarantee?",
    "options": [
      "Consistency and Availability",
      "Availability and Partition Tolerance",
      "Consistency and Partition Tolerance",
      "Low Latency and High Throughput"
    ],
    "answer": "Consistency and Availability",
    "explanation": "During a partition, a system must choose between Consistency (returning an error or timeout) and Availability (proceeding with the request despite potential data divergence). It is mathematically impossible to guarantee both C and A when P is active.",
    "difficulty": "Intermediate"
  },
  {
    "id": 36,
    "question": "Which caching strategy updates the cache only when the cached data is requested and found to be stale, minimizing writes to the cache when data is infrequently accessed?",
    "options": [
      "Write-Through Cache",
      "Write-Back (Write-Behind) Cache",
      "Cache-Aside (Lazy Loading)",
      "Write-Around Cache"
    ],
    "answer": "Cache-Aside (Lazy Loading)",
    "explanation": "Cache-Aside treats the cache as a separate layer; the application checks the cache, misses, then fetches from the DB and populates the cache. Write-Through synchronously updates cache on DB write, while Write-Back asynchronously updates the DB.",
    "difficulty": "Intermediate"
  },
  {
    "id": 37,
    "question": "In a horizontally sharded database, what is the primary risk of using a 'timestamp-based' sharding key without additional logic?",
    "options": [
      "Increased query latency for single-row lookups",
      "Database 'hot spotting' due to write intensity skew",
      "Inability to perform joins across shards",
      "Loss of referential integrity constraints"
    ],
    "answer": "Database 'hot spotting' due to write intensity skew",
    "explanation": "Time-based keys often cause all new writes to route to a single shard (the current time interval), creating a hot spot. This negates the load distribution benefits of horizontal scaling.",
    "difficulty": "Intermediate"
  },
  {
    "id": 38,
    "question": "Which load balancing algorithm assigns incoming requests to the server with the fewest active connections, making it ideal for long-lived connections?",
    "options": [
      "Round Robin",
      "Least Connection Method",
      "IP Hash",
      "Weighted Response Time"
    ],
    "answer": "Least Connection Method",
    "explanation": "The Least Connection method dynamically directs traffic to the server handling the fewest requests. This prevents a server with long-running connections from becoming a bottleneck compared to a simple Round Robin approach.",
    "difficulty": "Intermediate"
  },
  {
    "id": 39,
    "question": "A system requires that a read operation must reflect the most recent write. Which consistency model is being strictly enforced?",
    "options": [
      "Eventual Consistency",
      "Causal Consistency",
      "Strong Consistency",
      "Tunable Consistency"
    ],
    "answer": "Strong Consistency",
    "explanation": "Strong consistency guarantees that a read returns the most recent write for any client. Eventual consistency allows stale reads, and Causal consistency only preserves the order of related operations.",
    "difficulty": "Intermediate"
  },
  {
    "id": 40,
    "question": "When using a Chord Consistent Hash ring for a distributed cache, what is the primary advantage over modulo hashing (% N)?",
    "options": [
      "O(1) lookup time regardless of cluster size",
      "Minimal data movement when nodes are added or removed",
      "Guarantee of Strong Consistency across nodes",
      "Reduced network latency between client and server"
    ],
    "answer": "Minimal data movement when nodes are added or removed",
    "explanation": "Consistent hashing ensures that adding/removing a node only affects the mapping of keys associated with that node (and its neighbors). Modulo hashing requires reshuffling a significant portion (1/N) of the keyspace.",
    "difficulty": "Intermediate"
  },
  {
    "id": 41,
    "question": "In a Publish-Subscribe messaging system, what is the function of a 'Topic'?",
    "options": [
      "To ensure message ordering across all consumers",
      "To act as a mechanism for categorizing messages and decoupling publishers from subscribers",
      "To persist messages indefinitely for replay",
      "To load balance messages evenly across all publishers"
    ],
    "answer": "To act as a mechanism for categorizing messages and decoupling publishers from subscribers",
    "explanation": "Publishers send messages to a Topic, and subscribers receive messages from the Topic. This decouples the senders from the receivers; the system handles delivery to interested parties rather than the publisher knowing who they are.",
    "difficulty": "Intermediate"
  },
  {
    "id": 42,
    "question": "What is the primary function of a 'Reverse Proxy' (e.g., Nginx, HAProxy) in a standard web architecture?",
    "options": [
      "To initiate requests to external APIs on behalf of the client",
      "To protect the backend server identity and handle SSL termination/load balancing",
      "To cache static assets on the client's browser",
      "To translate SQL queries into NoSQL calls"
    ],
    "answer": "To protect the backend server identity and handle SSL termination/load balancing",
    "explanation": "A reverse proxy sits in front of web servers, forwarding client requests to those servers. It abstracts the backend origin, provides load balancing, and handles computationally expensive tasks like SSL termination.",
    "difficulty": "Intermediate"
  },
  {
    "id": 43,
    "question": "Which database property ensures that a transaction is an 'all-or-nothing' proposition, where a sequence of operations must all succeed or all fail?",
    "options": [
      "Atomicity",
      "Consistency",
      "Isolation",
      "Durability"
    ],
    "answer": "Atomicity",
    "explanation": "Atomicity guarantees that each transaction is treated as a single unit, which either succeeds completely or fails completely. If any part of the transaction fails, the entire database state is rolled back.",
    "difficulty": "Intermediate"
  },
  {
    "id": 44,
    "question": "In the context of an API Gateway, what is 'Rate Limiting' primarily designed to protect?",
    "options": [
      "The encryption protocols from being deprecated",
      "Upstream services from denial-of-service attacks or resource exhaustion",
      "The database schema from unauthorized modifications",
      "The client's application from crashing"
    ],
    "answer": "Upstream services from denial-of-service attacks or resource exhaustion",
    "explanation": "Rate limiting restricts the number of requests a user can make in a specific timeframe. This prevents a single user or bot from overwhelming the backend servers and degrading service for others.",
    "difficulty": "Intermediate"
  },
  {
    "id": 45,
    "question": "Why are 'Long Polling' and 'WebSockets' preferred over standard REST polling for real-time chat applications?",
    "options": [
      "They reduce bandwidth consumption by eliminating the request-response overhead of empty checks",
      "They allow the server to initiate connections to the client",
      "They utilize UDP instead of TCP for faster transmission",
      "They automatically compress all JSON payloads"
    ],
    "answer": "They reduce bandwidth consumption by eliminating the request-response overhead of empty checks",
    "explanation": "Standard polling creates continuous HTTP overhead even when no data is available. Long Polling keeps a connection open until data arrives, and WebSockets provide a persistent full-duplex channel, drastically reducing latency and overhead.",
    "difficulty": "Intermediate"
  },
  {
    "id": 46,
    "question": "Which storage pattern describes a setup where data is written to a local node first and propagated to replicas in the background, prioritizing write speed over immediate data safety?",
    "options": [
      "Synchronous Replication",
      "Leaderless Replication with Quorum",
      "Asynchronous Replication",
      "Multi-Leader Replication"
    ],
    "answer": "Asynchronous Replication",
    "explanation": "Asynchronous replication confirms the write to the client immediately after writing to the primary node, without waiting for replicas. This offers low latency but risks data loss if the primary fails before propagation.",
    "difficulty": "Intermediate"
  },
  {
    "id": 47,
    "question": "What is the main advantage of using a 'Bloom Filter' as a pre-check in a database read path?",
    "options": [
      "It ensures Strong Consistency across distributed nodes",
      "It guarantees that the requested data exists in the cache",
      "It rapidly determines if an element is definitely not in a set, avoiding expensive disk lookups",
      "It compresses the database logs to reduce storage costs"
    ],
    "answer": "It rapidly determines if an element is definitely not in a set, avoiding expensive disk lookups",
    "explanation": "A Bloom Filter is a memory-efficient probabilistic data structure. It is excellent for checking non-existence (0% false negatives), saving the system from querying the database if the key doesn't exist.",
    "difficulty": "Intermediate"
  },
  {
    "id": 48,
    "question": "In the design of a Content Delivery Network (CDN), what does 'Edge Caching' specifically refer to?",
    "options": [
      "Storing the master copy of data in the central database",
      "Distributing content to geographically dispersed servers closer to the user",
      "Routing traffic through the internal private network",
      "Load balancing requests across multiple data centers"
    ],
    "answer": "Distributing content to geographically dispersed servers closer to the user",
    "explanation": "Edge caching places content at the 'edge' of the network (Points of Presence), physically closer to the end-user. This reduces latency by minimizing the distance data must travel.",
    "difficulty": "Intermediate"
  },
  {
    "id": 49,
    "question": "Which reliability pattern functions as a 'circuit breaker' in a distributed system?",
    "options": [
      "Retrying the failed request immediately with exponential backoff",
      "Detecting failure and preventing requests to a failing service to allow recovery",
      "Redirecting traffic to a backup region automatically",
      "Increasing the timeout duration for the downstream service"
    ],
    "answer": "Detecting failure and preventing requests to a failing service to allow recovery",
    "explanation": "The Circuit Breaker pattern stops calls to a service if failures reach a threshold. This prevents the cascading effect of overwhelming a struggling service with retries and allows it time to recover.",
    "difficulty": "Intermediate"
  },
  {
    "id": 50,
    "question": "When comparing SQL and NoSQL databases, which consistency model is typically associated with Document Stores (like MongoDB) or Key-Value stores (like DynamoDB) in their default configuration?",
    "options": [
      "ACID Strong Consistency",
      "Immediate Consistency",
      "Eventual Consistency",
      "Monotonic Reads"
    ],
    "answer": "Eventual Consistency",
    "explanation": "While many NoSQL databases can be configured for strong consistency, their default configuration prioritizes availability and partition tolerance (AP), leading to eventual consistency where data propagation takes time.",
    "difficulty": "Intermediate"
  },
  {
    "id": 51,
    "question": "What specific problem does the 'Sidecar Pattern' solve in a Microservices architecture?",
    "options": [
      "It centralizes the database connections to reduce latency",
      "It decouples infrastructure logic (monitoring, logging, networking) from the business logic application",
      "It ensures all services run in a single monolithic process",
      "It handles the authentication of users across different UIs"
    ],
    "answer": "It decouples infrastructure logic (monitoring, logging, networking) from the business logic application",
    "explanation": "A sidecar runs alongside the main service container in the same pod/address space, sharing the lifecycle. It abstracts 'cross-cutting concerns' like networking or monitoring, allowing the main app to remain language-agnostic.",
    "difficulty": "Intermediate"
  },
  {
    "id": 52,
    "question": "What is the primary trade-off when choosing a 'Wide Column Store' (like Cassandra) over a 'Relational Database'?",
    "options": [
      "Schema flexibility vs. transactional integrity and complex query support",
      "Read performance vs. storage cost",
      "Security vs. availability",
      "Open source licensing vs. commercial support"
    ],
    "answer": "Schema flexibility vs. transactional integrity and complex query support",
    "explanation": "Wide Column Stores offer high scalability and flexible schemas but typically lack ACID transaction support and complex querying capabilities (like multi-table JOINs) found in Relational Databases.",
    "difficulty": "Intermediate"
  },
  {
    "id": 53,
    "question": "What is the purpose of the 'Leader Election' mechanism in distributed systems like Kubernetes or Apache Zookeeper?",
    "options": [
      "To load balance ingress traffic across nodes",
      "To assign a single coordinating node to avoid split-brain conflicts and duplicate processing",
      "To elect the most powerful server for processing",
      "To rotate encryption keys for the cluster"
    ],
    "answer": "To assign a single coordinating node to avoid split-brain conflicts and duplicate processing",
    "explanation": "Leader election ensures that only one node in a distributed group acts as the leader or coordinator. This prevents race conditions and the 'split-brain' scenario where multiple nodes act as master simultaneously.",
    "difficulty": "Intermediate"
  },
  {
    "id": 54,
    "question": "Which scenario describes a 'Split-Brain' error in a distributed system?",
    "options": [
      "When a database shard runs out of disk space",
      "When network partition causes two subsets of nodes to believe they are the active leader",
      "When a client sends a request to the wrong microservice",
      "When a zombie process consumes all CPU resources"
    ],
    "answer": "When network partition causes two subsets of nodes to believe they are the active leader",
    "explanation": "Split-brain occurs when a network failure separates the cluster, causing multiple nodes to assume leadership. This leads to data conflicts and corruption as updates are committed independently to different partitions.",
    "difficulty": "Intermediate"
  },
  {
    "id": 55,
    "question": "Why is 'Idempotency' a critical requirement for API design in distributed systems?",
    "options": [
      "To ensure data cannot be read by unauthorized users",
      "To allow a client to safely retry the same operation multiple times without changing the result beyond the first attempt",
      "To minimize the size of the JSON payload",
      "To enable the server to compress the response automatically"
    ],
    "answer": "To allow a client to safely retry the same operation multiple times without changing the result beyond the first attempt",
    "explanation": "Network failures often trigger automatic retries. If an operation (like a payment charge) is not idempotent, retries will result in duplicate charges or data corruption.",
    "difficulty": "Intermediate"
  },
  {
    "id": 56,
    "question": "In a distributed hash table (DHT), what does the formula 'R + W > N' ensure regarding quorum?",
    "options": [
      "The system achieves optimal read latency",
      "The system guarantees that at least one node acknowledges the latest data",
      "The write operation is asynchronous to all nodes",
      "The network partition is ignored"
    ],
    "answer": "The system guarantees that at least one node acknowledges the latest data",
    "explanation": "In a system with N replicas, if R nodes are read and W nodes are written, R + W > N ensures there is an overlap in the replication sets. This guarantees that the latest write is present in the read set.",
    "difficulty": "Intermediate"
  },
  {
    "id": 57,
    "question": "Which component is responsible for maintaining a mapping between a symbolic hostname (e.g., www.example.com) and an IP address?",
    "options": [
      "Load Balancer",
      "Content Delivery Network",
      "Domain Name System (DNS)",
      "API Gateway"
    ],
    "answer": "Domain Name System (DNS)",
    "explanation": "DNS translates human-readable domain names into machine-readable IP addresses. It is the foundational distributed lookup system that routes traffic to the correct entry point of a web infrastructure.",
    "difficulty": "Intermediate"
  },
  {
    "id": 58,
    "question": "When designing for 'High Availability', what is the primary purpose of implementing a 'Health Check' endpoint?",
    "options": [
      "To authenticate the user before they access the service",
      "To allow load balancers/orchestrators to route traffic away from failing instances",
      "To measure the CPU usage of the server",
      "To log every incoming request for analytics"
    ],
    "answer": "To allow load balancers/orchestrators to route traffic away from failing instances",
    "explanation": "Health checks provide a lightweight way to determine if a service (or specific component) is alive and ready to serve traffic. Load balancers use this status to remove failing nodes from the rotation.",
    "difficulty": "Intermediate"
  },
  {
    "id": 59,
    "question": "What is 'Read Repair' in the context of Dynamo-style databases?",
    "options": [
      "A mechanism to rewrite the database schema without downtime",
      "A background process that synchronizes stale data replicas during a read request",
      "A tool to fix corrupted SQL syntax",
      "A method to cache read results on the client side"
    ],
    "answer": "A background process that synchronizes stale data replicas during a read request",
    "explanation": "When a read detects inconsistent versions of data across replicas (based on vector clocks or timestamps), the system returns the latest version and pushes that update to the stale replicas in the background.",
    "difficulty": "Intermediate"
  },
  {
    "id": 60,
    "question": "Which architectural pattern is primarily concerned with separating read and write operations to different database instances?",
    "options": [
      "Command and Query Responsibility Segregation (CQRS)",
      "Model-View-Controller (MVC)",
      "Sharding",
      "Polyglot Persistence"
    ],
    "answer": "Command and Query Responsibility Segregation (CQRS)",
    "explanation": "CQRS splits the application into two parts: the Command side (write) and the Query side (read). This allows them to be scaled, optimized, and secured independently.",
    "difficulty": "Intermediate"
  },
  {
    "id": 61,
    "question": "In a Message Queue system, what distinguishes a 'Topic' from a 'Queue' regarding consumption?",
    "options": [
      "A Queue delivers a message to multiple consumers; a Topic delivers to only one",
      "A Topic delivers a copy of a message to every subscriber; a Queue typically delivers to a single consumer",
      "Queues are faster than Topics",
      "Topics cannot store messages persistently"
    ],
    "answer": "A Topic delivers a copy of a message to every subscriber; a Queue typically delivers to a single consumer",
    "explanation": "Topics follow the Pub/Sub model (broadcasting to all interested subscribers). Queues follow the Point-to-Point model (load balancing work among consumers; a message is consumed once).",
    "difficulty": "Intermediate"
  },
  {
    "id": 62,
    "question": "What is the main disadvantage of 'Client-Side Discovery' (where the client knows the network locations of service instances) compared to 'Server-Side Discovery'?",
    "options": [
      "It introduces a single point of failure in the load balancer",
      "It couples the client to the service registry and requires logic to handle load balancing",
      "It prevents the use of SSL/TLS encryption",
      "It is slower because the request must pass through more hops"
    ],
    "answer": "It couples the client to the service registry and requires logic to handle load balancing",
    "explanation": "In client-side discovery, the client must query the registry and implement load balancing logic. This creates tight coupling and makes client implementation complex compared to using a centralized router.",
    "difficulty": "Intermediate"
  },
  {
    "id": 63,
    "question": "Which of the following describes 'Throttling' as opposed to 'Rate Limiting'?",
    "options": [
      "Throttling controls the rate of data usage, while Rate Limiting controls the count of requests",
      "Throttling limits requests after a threshold is breached by dropping them, whereas Rate Limiting queues them",
      "Throttling is applied to the server CPU, while Rate Limiting is applied to the network",
      "They are synonymous concepts with no difference"
    ],
    "answer": "Throttling controls the rate of data usage, while Rate Limiting controls the count of requests",
    "explanation": "While often used interchangeably, Rate Limiting typically restricts the *number* of requests per time unit (e.g., 100 req/min). Throttling often refers to regulating the *resource usage* or throughput to ensure fair allocation.",
    "difficulty": "Intermediate"
  },
  {
    "id": 64,
    "question": "What is the primary reason for using a 'Time-Series Database' (TSDB) instead of a standard relational database for metrics?",
    "options": [
      "TSDBs support complex JOIN operations better",
      "TSDBs optimize for high-volume writes and time-range queries with compression",
      "TSDBs guarantee ACID transactions for all data types",
      "Relational databases cannot store timestamps"
    ],
    "answer": "TSDBs optimize for high-volume writes and time-range queries with compression",
    "explanation": "Time-series data is append-heavy and queried by time ranges. TSDBs use specialized compression and indexing (often based on time buckets) to handle this specific access pattern efficiently.",
    "difficulty": "Intermediate"
  },
  {
    "id": 65,
    "question": "What is the specific trade-off of using 'Sparse Indexes' in a database?",
    "options": [
      "They consume more disk space than dense indexes",
      "They cannot be used to satisfy queries that are not covered by the index",
      "They are slower for range scans than dense indexes",
      "They can only be created on primary key columns"
    ],
    "answer": "They cannot be used to satisfy queries that are not covered by the index",
    "explanation": "A sparse index (often seen in MongoDB) only contains entries for documents that have the indexed field. It saves space but cannot be used if the field is missing or for certain types of queries (sorting) where the index is incomplete.",
    "difficulty": "Intermediate"
  },
  {
    "id": 66,
    "question": "In the context of System Design interviews, what does 'Back-of-the-envelope Estimation' primarily validate?",
    "options": [
      "The exact cost of cloud providers",
      "The candidate's ability to do mental math and determine the order of magnitude for scaling",
      "The precise latency of the database",
      "The number of bugs in the code"
    ],
    "answer": "The candidate's ability to do mental math and determine the order of magnitude for scaling",
    "explanation": "These estimations are not about exact numbers but about reasoning skills. They validate whether the candidate understands the scale (e.g., GB/s vs TB/s) to choose appropriate technologies.",
    "difficulty": "Intermediate"
  },
  {
    "id": 67,
    "question": "Which protocol feature of HTTP/2 significantly reduces latency compared to HTTP/1.1?",
    "options": [
      "Text-based headers",
      "Binary framing and Multiplexing over a single TCP connection",
      "Stateful connections",
      "New HTTP methods"
    ],
    "answer": "Binary framing and Multiplexing over a single TCP connection",
    "explanation": "HTTP/1.1 suffers from head-of-line-blocking. HTTP/2 uses binary framing and multiplexing to allow multiple requests to be sent concurrently over a single TCP connection without waiting for previous responses.",
    "difficulty": "Intermediate"
  },
  {
    "id": 68,
    "question": "When using 'Twemproxy' or 'Redis Cluster', what is the fundamental purpose of 'Hash Tags' (e.g., {user123}:timeline)?",
    "options": [
      "To compress the data stored in Redis",
      "To force multiple keys to reside on the same shard for multi-key operations",
      "To speed up the hashing calculation",
      "To encrypt the keys in memory"
    ],
    "answer": "To force multiple keys to reside on the same shard for multi-key operations",
    "explanation": "In distributed systems, related keys might hash to different nodes. Hash tags allow a subset of the key to be used for hashing, ensuring related keys map to the same node to support atomic operations or Lua scripts.",
    "difficulty": "Intermediate"
  },
  {
    "id": 69,
    "question": "Which database engine is typically preferred for a write-heavy workload with flexible schema requirements?",
    "options": [
      "Row-oriented Database (e.g., MySQL, PostgreSQL)",
      "Column-oriented Database (e.g., Cassandra, Redshift)",
      "Graph Database (e.g., Neo4j)",
      "In-memory Grid (e.g., Redis)"
    ],
    "answer": "Column-oriented Database (e.g., Cassandra, Redshift)",
    "explanation": "Columnar stores are highly efficient for write-heavy and analytical workloads (OLAP) because they compress data better and only read the specific columns needed for a query, rather than entire rows.",
    "difficulty": "Intermediate"
  },
  {
    "id": 70,
    "question": "In the context of LSM (Log-Structured Merge) trees used in databases like RocksDB or Cassandra, what is the primary trade-off compared to a B-Tree implementation?",
    "options": [
      "LSM trees offer faster read performance but significantly slower write throughput.",
      "LSM trees optimize for write throughput by appending data, at the cost of higher write amplification and potential read amplification during compaction.",
      "LSM trees eliminate the need for a Write-Ahead Log (WAL) by persisting data directly to the memtable.",
      "LSM trees guarantee strong consistency by locking the memtable during SSTable compaction."
    ],
    "answer": "LSM trees optimize for write throughput by appending data, at the cost of higher write amplification and potential read amplification during compaction.",
    "explanation": "LSM trees convert random writes into sequential writes by buffering them in memory (memtable) and flushing to SSTables, which improves write speed. However, this necessitates a background compaction process to merge SSTables, causing write amplification and potentially slowing down reads if data is spread across many files.",
    "difficulty": "Advanced"
  },
  {
    "id": 71,
    "question": "When designing a rate limiter for a high-throughput distributed API, why is the 'Token Bucket' algorithm generally preferred over the 'Fixed Window' algorithm?",
    "options": [
      "Token Bucket completely eliminates the need for distributed state or Atomic operations.",
      "Token Bucket allows for bursting traffic while maintaining a strict long-term rate, whereas Fixed Window suffers from boundary conditions (spikes at window edges).",
      "Fixed Window is more computationally expensive because it requires sorting timestamps.",
      "Token Bucket ensures that requests are processed in FIFO order, preventing reordering."
    ],
    "answer": "Token Bucket allows for bursting traffic while maintaining a strict long-term rate, whereas Fixed Window suffers from boundary conditions (spikes at window edges).",
    "explanation": "The Token Bucket algorithm allows a user to consume saved tokens to handle bursts up to the bucket capacity, smoothing out traffic. Fixed Window counters suffer from 'boundary spikes' where 2x the rate limit can be hit at the transition between two windows (e.g., the end of one and start of the next).",
    "difficulty": "Advanced"
  },
  {
    "id": 72,
    "question": "In a distributed system using the Saga pattern for transaction management, how is compensation logic fundamentally different from a 2-Phase Commit (2PC) rollback?",
    "options": [
      "Saga uses compensating transactions to manually undo the physical changes of committed steps, whereas 2PC atomicly rolls back all participants if any phase fails.",
      "Saga relies on a single central coordinator to lock all database rows, while 2PC uses local locks only.",
      "Saga is an AC-compliant pattern ensuring atomicity, while 2PC is only Eventually Consistent.",
      "There is no difference; both patterns guarantee atomicity and isolation across microservices."
    ],
    "answer": "Saga uses compensating transactions to manually undo the physical changes of committed steps, whereas 2PC atomicly rolls back all participants if any phase fails.",
    "explanation": "2PC provides atomicity (all or nothing) using locking and blocking mechanisms. Saga patterns (choreography or orchestration) execute local transactions sequentially and rely on application-level 'compensating actions' to undo the effects of previously committed transactions if a later step fails.",
    "difficulty": "Advanced"
  },
  {
    "id": 73,
    "question": "What is the 'Write Amplification' factor in the context of SSD storage and databases, and why is it critical for system longevity?",
    "options": [
      "It is the ratio of logical data written by the application to the actual physical data written to the storage medium; high amplification reduces SSD lifespan.",
      "It is the number of replicas written in a distributed cluster; higher amplification increases read availability.",
      "It refers to the compression ratio of data before being written to disk; lower amplification saves space but costs CPU.",
      "It is the delay between a write request and acknowledgment; lower amplification increases write latency."
    ],
    "answer": "It is the ratio of logical data written by the application to the actual physical data written to the storage medium; high amplification reduces SSD lifespan.",
    "explanation": "Write amplification occurs when the storage controller or database writes more physical data than the user intended (e.g., due to garbage collection, journaling, or page updates). Since SSDs have a finite number of program/erase cycles, high write amplification significantly shortens the hardware's lifespan.",
    "difficulty": "Advanced"
  },
  {
    "id": 74,
    "question": "Why is a 'Long Polling' strategy often preferred over standard 'Short Polling' for a chat application notification system?",
    "options": [
      "Long Polling uses UDP instead of TCP to reduce latency.",
      "Long Polling holds the HTTP connection open until the server has data or a timeout occurs, reducing network chatter and resource usage compared to continuous requests.",
      "Short Polling requires a persistent WebSocket connection, which is harder to scale than Long Polling.",
      "Long Polling allows the server to push data to the client without the client needing to establish a connection first."
    ],
    "answer": "Long Polling holds the HTTP connection open until the server has data or a timeout occurs, reducing network chatter and resource usage compared to continuous requests.",
    "explanation": "Short polling repeatedly hits the server at high frequency (wasting bandwidth and CPU). Long polling keeps the request open, effectively creating a pseudo-push mechanism over HTTP, ensuring the client receives updates immediately without constant requests.",
    "difficulty": "Advanced"
  },
  {
    "id": 75,
    "question": "In the Raft consensus algorithm, what is the specific purpose of the 'Leader Append-Only' property regarding the log?",
    "options": [
      "It allows followers to modify their logs to correct corruption.",
      "It ensures that only the Leader can accept write requests and append entries to the log, simplifying consistency management.",
      "It prevents the Leader from overwriting entries in its own log once they are committed.",
      "It mandates that logs must be stored in an SSD for performance."
    ],
    "answer": "It ensures that only the Leader can accept write requests and append entries to the log, simplifying consistency management.",
    "explanation": "Raft delegates all management of the replicated log to the Leader. By restricting log appends to the Leader, Raft turns the consensus problem into a problem of managing the Leader, avoiding the split-brain conflicts that can arise if multiple nodes can modify the log concurrently.",
    "difficulty": "Advanced"
  },
  {
    "id": 76,
    "question": "When using 'Consistent Hashing' with virtual nodes (vnodes) in a distributed cache cluster, what is the primary benefit of adding many vnodes per physical machine?",
    "options": [
      "It increases the total storage capacity of the cluster linearly.",
      "It ensures that the data is evenly distributed across the physical nodes and minimizes the amount of data remapped when a node is added or removed.",
      "It eliminates the need for a replication factor.",
      "It guarantees that the hash function never results in a collision."
    ],
    "answer": "It ensures that the data is evenly distributed across the physical nodes and minimizes the amount of data remapped when a node is added or removed.",
    "explanation": "Without vnodes, the range of keys owned by a node is large, leading to imbalanced loads. Vnodes break the hash ring into smaller segments, ensuring that when a physical node fails, its workload is dispersed across many remaining nodes, preventing hotspots.",
    "difficulty": "Advanced"
  },
  {
    "id": 77,
    "question": "What is the 'Split-Brain' problem in distributed databases, and how does a 'Quorum' (R + W > N) strategy help mitigate it?",
    "options": [
      "Split-Brain occurs when two nodes become the Leader; Quorum prevents this by encrypting the data.",
      "Split-Brain occurs when the network partitions and two subsets of the cluster believe they are the active group; Quorum ensures only the partition with the majority of nodes can commit writes.",
      "Split-Brain is a deadlock condition; Quorum resolves it by prioritizing writes over reads.",
      "Split-Brain refers to data corruption; Quorum fixes this by compressing data before storage."
    ],
    "answer": "Split-Brain occurs when the network partitions and two subsets of the cluster believe they are the active group; Quorum ensures only the partition with the majority of nodes can commit writes.",
    "explanation": "In a split-brain scenario, network isolation prevents communication. A Quorum requires a majority (or specific count) of replicas to acknowledge a write. This ensures that if a minority partition is isolated, it cannot accept writes, preserving data consistency.",
    "difficulty": "Advanced"
  },
  {
    "id": 78,
    "question": "In a database row, you have a 'last_modified' timestamp column. You notice that querying for rows updated in the last 24 hours is slow. What is the most likely technical cause?",
    "options": [
      "The timestamp data type uses too much storage space.",
      "The query performs a full table scan because the 'last_modified' column is not indexed or is not supported by a clustered index strategy.",
      "The database is using UDP instead of TCP for the connection.",
      "The timestamp precision is too high (milliseconds vs seconds)."
    ],
    "answer": "The query performs a full table scan because the 'last_modified' column is not indexed or is not supported by a clustered index strategy.",
    "explanation": "Without an index on the filtering column (last_modified), the database engine must perform a Full Table Scan to evaluate the predicate for every row. An index allows the engine to jump directly to the relevant range of data.",
    "difficulty": "Advanced"
  },
  {
    "id": 79,
    "question": "Why are 'Bloom Filters' frequently used in read-path optimizations for databases like Cassandra or HBase?",
    "options": [
      "To compress the data before it is sent over the network.",
      "To determine if an SSTable (data file) definitely does NOT contain a specific key, avoiding unnecessary disk reads.",
      "To sort the data in memory before writing it to disk.",
      "To encrypt the primary keys to prevent data snooping."
    ],
    "answer": "To determine if an SSTable (data file) definitely does NOT contain a specific key, avoiding unnecessary disk reads.",
    "explanation": "Bloom Filters are probabilistic data structures that test set membership. They are fast and memory-efficient, allowing the database to skip searching SSTables that definitely do not contain the requested key, significantly reducing I/O for missing keys.",
    "difficulty": "Advanced"
  },
  {
    "id": 80,
    "question": "How does 'Sharding' differ from 'Partitioning' (in a single database instance) in terms of implementation scope?",
    "options": [
      "They are identical terms used interchangeably.",
      "Sharding distributes data across multiple distinct database instances (horizontal scaling), whereas partitioning splits data within a single instance.",
      "Sharding creates copies of data for redundancy, while partitioning splits a single copy.",
      "Sharding is a vertical scaling technique, while partitioning is horizontal."
    ],
    "answer": "Sharding distributes data across multiple distinct database instances (horizontal scaling), whereas partitioning splits data within a single instance.",
    "explanation": "While the logical mechanism (splitting data) is similar, partitioning is a logical subdivision of data within one database server to improve manageability. Sharding implies that the data is spread across multiple physical/database nodes to distribute the load.",
    "difficulty": "Advanced"
  },
  {
    "id": 81,
    "question": "What is the 'Phi Accrual Failure Detector' used in distributed systems (like Cassandra or Akka), and how does it improve on traditional Heartbeats?",
    "options": [
      "It uses a binary (alive/dead) state to trigger immediate failover.",
      "It calculates a confidence interval (phi) based on the distribution of heartbeat arrival times to adapt to network conditions and avoid flapping.",
      "It forces the leader to step down if network latency exceeds 100ms.",
      "It requires a GPS clock synchronization to function."
    ],
    "answer": "It calculates a confidence interval (phi) based on the distribution of heartbeat arrival times to adapt to network conditions and avoid flapping.",
    "explanation": "Traditional heartbeats fail if the network is temporarily congested, causing 'flapping' (marking a node down and up repeatedly). Phi Accrual considers the history of inter-arrival times to compute a probability of failure, making it more resilient to temporary network glitches.",
    "difficulty": "Advanced"
  },
  {
    "id": 82,
    "question": "When implementing 'Idempotency Keys' for client-side retries, where must the state of the processed keys be stored to guarantee correctness?",
    "options": [
      "In the client's local browser cache.",
      "In a persistent, highly available data store (like Redis or DB) that is atomic with the transaction execution.",
      "In a volatile in-memory map on the application server.",
      "Idempotency keys are passed via headers and require no server-side state."
    ],
    "answer": "In a persistent, highly available data store (like Redis or DB) that is atomic with the transaction execution.",
    "explanation": "To ensure a retry is not executed as a duplicate charge or action, the server must check if the key was seen before. If the server crashes after processing but before storing the state, or if the state is local to a specific node in a cluster, a retry on a different node would fail the idempotency check.",
    "difficulty": "Advanced"
  },
  {
    "id": 83,
    "question": "In the context of Microservices, what specific problem does the 'Sidecar Pattern' solve?",
    "options": [
      "It separates business logic from infrastructure/observability features (like logging, monitoring, network config) by sharing the lifecycle with the main service.",
      "It creates a backup of the database in case of failure.",
      "It acts as a load balancer in front of the service.",
      "It compiles the application code into machine language."
    ],
    "answer": "It separates business logic from infrastructure/observability features (like logging, monitoring, network config) by sharing the lifecycle with the main service.",
    "explanation": "The Sidecar pattern deploys a utility container alongside the main service container. It abstracts away complex 'cross-cutting concerns' (service mesh features, retries, circuit breaking) so the business logic remains language-agnostic and clean.",
    "difficulty": "Advanced"
  },
  {
    "id": 84,
    "question": "Why is 'Write Amplification' a specific concern when choosing a database for an SSD-backed workload compared to an HDD workload?",
    "options": [
      "HDDs cannot handle write amplification, so the database must buffer writes in RAM.",
      "SSDs have a finite number of program/erase cycles per cell; high write amplification physically degrades the drive faster.",
      "Write amplification causes the read speed of SSDs to drop to zero.",
      "SSDs are smaller, so write amplification fills them up too quickly."
    ],
    "answer": "SSDs have a finite number of program/erase cycles per cell; high write amplification physically degrades the drive faster.",
    "explanation": "While write amplification wastes IOPS on both HDDs and SSDs, it is a critical longevity issue for SSDs. Excessive writes (amplification) consume the NAND flash cells' endurance, leading to premature hardware failure.",
    "difficulty": "Advanced"
  },
  {
    "id": 85,
    "question": "What distinguishes 'Optimistic Locking' from 'Pessimistic Locking' in database concurrency control?",
    "options": [
      "Optimistic locking reads the record, checks a version/timestamp on write, and fails if changed; Pessimistic locking locks the record for the duration of the transaction.",
      "Optimistic locking assumes conflicts will happen and uses database row-level locks; Pessimistic locking assumes conflicts are rare.",
      "Optimistic locking is only used for reading data, never for updates.",
      "Pessimistic locking does not require a transaction, while Optimistic locking does."
    ],
    "answer": "Optimistic locking reads the record, checks a version/timestamp on write, and fails if changed; Pessimistic locking locks the record for the duration of the transaction.",
    "explanation": "Optimistic locking relies on conflict detection (usually version numbers) at commit time and is better for low-contention scenarios. Pessimistic locking (SELECT FOR UPDATE) relies on conflict prevention by taking a database lock immediately, which can cause deadlocks in high-contention scenarios.",
    "difficulty": "Advanced"
  },
  {
    "id": 86,
    "question": "In a 'Publisher-Subscriber' system using a Message Broker like Kafka, how does 'Partitioning' increase parallelism?",
    "options": [
      "It copies the same message to every consumer.",
      "It allows a single topic to be split across multiple brokers and consumers to read from specific partitions concurrently.",
      "It encrypts parts of the message for security.",
      "It breaks a large message into smaller chunks to fit the network MTU."
    ],
    "answer": "It allows a single topic to be split across multiple brokers and consumers to read from specific partitions concurrently.",
    "explanation": "Partitions are the unit of parallelism in Kafka. A partition can only be consumed by one consumer in a consumer group. By having multiple partitions, multiple consumers can process the topic data in parallel, increasing throughput.",
    "difficulty": "Advanced"
  },
  {
    "id": 87,
    "question": "What is the 'Leaky Bucket' algorithm primarily used for in network traffic shaping?",
    "options": [
      "To allow traffic to burst indefinitely.",
      "To smooth out bursty traffic into a constant rate by leaking data at a fixed rate and dropping excess packets.",
      "To compress data packets before transmission.",
      "To prioritize large packets over small packets."
    ],
    "answer": "To smooth out bursty traffic into a constant rate by leaking data at a fixed rate and dropping excess packets.",
    "explanation": "The Leaky Bucket analogy describes a bucket with a hole: water (packets) arrives in bursts, but leaks out at a constant rate. If the bucket overfills (burst exceeds capacity), the excess water is dropped, ensuring the output stream is highly predictable and shaped.",
    "difficulty": "Advanced"
  },
  {
    "id": 88,
    "question": "When designing a REST API, what is the primary functional difference between using the HTTP status codes '409 Conflict' and '422 Unprocessable Entity'?",
    "options": [
      "409 indicates the request was formatted incorrectly; 422 indicates the user is not logged in.",
      "409 indicates a business logic or state conflict (e.g., duplicate ID); 422 indicates semantic errors in the request body despite correct syntax.",
      "422 means the server crashed; 409 means the database is down.",
      "There is no difference; they are synonyms."
    ],
    "answer": "409 indicates a business logic or state conflict (e.g., duplicate ID); 422 indicates semantic errors in the request body despite correct syntax.",
    "explanation": "409 Conflict implies the current state of the target resource conflicts with the request (e.g., trying to create a user with an email that already exists). 422 Unprocessable Entity implies the JSON/REST structure is correct (200 OK parsing), but the data violates business rules (e.g., email field is empty).",
    "difficulty": "Advanced"
  },
  {
    "id": 89,
    "question": "In distributed systems, what is the 'Gamma' of a CAP theorem configuration?",
    "options": [
      "The latency penalty paid to achieve Consistency.",
      "The throughput gained by sacrificing Partition Tolerance.",
      "It is not a standard term; the theorem relates Consistency, Availability, and Partition Tolerance.",
      "The probability of network failure."
    ],
    "answer": "It is not a standard term; the theorem relates Consistency, Availability, and Partition Tolerance.",
    "explanation": "The CAP theorem explicitly defines three properties: Consistency, Availability, and Partition Tolerance. While there are extended discussions about latency (PACELC), 'Gamma' is not a defined term within the classic CAP theorem.",
    "difficulty": "Advanced"
  },
  {
    "id": 90,
    "question": "What is the primary disadvantage of using 'Database Sharding' based on a hash of the User ID?",
    "options": [
      "It ensures that all data is stored on a single server.",
      "It makes range queries (e.g., find all users with names A-C) inefficient because data is scattered randomly across shards.",
      "It prevents the system from being scalable.",
      "It causes data duplication across all nodes."
    ],
    "answer": "It makes range queries (e.g., find all users with names A-C) inefficient because data is scattered randomly across shards.",
    "explanation": "Hash-based sharding distributes data evenly (good for load balancing) but destroys data locality. To perform a range query, the application must query every shard (scatter-gather), which is slow and expensive compared to range-based sharding.",
    "difficulty": "Advanced"
  },
  {
    "id": 91,
    "question": "How does 'Read Repair' mechanism work in Dynamo-style databases (e.g., Cassandra, Riak)?",
    "options": [
      "It triggers a full re-replication of the data to a new node when a read fails.",
      "During a read, if versions differ, the system updates the outdated nodes with the most recent version asynchronously.",
      "It stops all writes to repair the node before reading.",
      "It deletes the data from the node with the highest latency."
    ],
    "answer": "During a read, if versions differ, the system updates the outdated nodes with the most recent version asynchronously.",
    "explanation": "In an Eventually Consistent system, replicas may diverge (Version Vectors). When a client reads, the coordinator node compares replicas, picks the latest one (based on timestamp/context), and sends that latest data back to the lagging replicas in the background to sync them up.",
    "difficulty": "Advanced"
  },
  {
    "id": 92,
    "question": "Why is 'Two-Phase Commit' (2PC) generally considered unsuitable for high-latency, geo-distributed transactions?",
    "options": [
      "It does not provide Atomicity.",
      "It is a blocking protocol where all nodes must hold locks and wait for the coordinator's commit decision, causing severe performance degradation under latency.",
      "It requires more than 3 replicas to function.",
      "It only works with MySQL databases."
    ],
    "answer": "It is a blocking protocol where all nodes must hold locks and wait for the coordinator's commit decision, causing severe performance degradation under latency.",
    "explanation": "2PC requires all participants to vote 'yes' and wait for the coordinator's final message before unlocking. In a high-latency network (e.g., cross-continent), the time spent holding locks drastically reduces concurrency and throughput, and a coordinator crash leaves nodes locked indefinitely.",
    "difficulty": "Advanced"
  },
  {
    "id": 93,
    "question": "In the context of an 'Event Sourcing' architecture, what are 'Projections'?",
    "options": [
      "The raw database schema used to store the events.",
      "Read-optimized views of the data generated by processing the stream of state-changing events.",
      "The mechanism used to encrypt the event stream.",
      "The API layer that exposes the events to the frontend."
    ],
    "answer": "Read-optimized views of the data generated by processing the stream of state-changing events.",
    "explanation": "Event Sourcing stores state as a sequence of events (the log). Querying this log directly is inefficient. Projections are separate, denormalized models (or CQRS read models) built by listening to the event stream to support specific query patterns.",
    "difficulty": "Advanced"
  },
  {
    "id": 94,
    "question": "What is the 'Thundering Herd' problem specifically regarding the expiration of a highly popular cache key?",
    "options": [
      "The database server crashes because it receives too many write requests.",
      "Thousands of concurrent requests detect a cache miss, and simultaneously query the database, overloading it.",
      "The network switch saturates due to multicast traffic.",
      "The load balancer fails because it runs out of connection slots."
    ],
    "answer": "Thousands of concurrent requests detect a cache miss, and simultaneously query the database, overloading it.",
    "explanation": "When a hot key expires, many threads/processes may simultaneously find the cache empty. Without locking or a mechanism like 'probabilistic early expiration', all these threads will hit the backend database at once, threatening its stability.",
    "difficulty": "Advanced"
  },
  {
    "id": 95,
    "question": "What is the 'Head-of-Line Blocking' issue in HTTP/1.1, and how does HTTP/2 mitigate it?",
    "options": [
      "In HTTP/1.1, a slow packet in the TCP queue blocks the delivery of subsequent packets; HTTP/2 fixes this by using UDP.",
      "In HTTP/1.1, only one request can be outstanding per TCP connection; a large response blocks subsequent requests; HTTP/2 uses multiplexing (streams) to interleave requests.",
      "It refers to browser rendering blocking; HTTP/2 fixes this with Server Push.",
      "It refers to the SYN flood attack; HTTP/2 uses SYN cookies to mitigate."
    ],
    "answer": "In HTTP/1.1, only one request can be outstanding per TCP connection; a large response blocks subsequent requests; HTTP/2 uses multiplexing (streams) to interleave requests.",
    "explanation": "HTTP/1.1 pipelining was rarely used due to HOL blocking (request 2 waits for request 1). HTTP/2 breaks requests into binary frames and multiplexes them over a single TCP connection, allowing a large response for request A to not delay the sending of frames for request B.",
    "difficulty": "Advanced"
  },
  {
    "id": 96,
    "question": "In a Vector Clock implementation for distributed systems, what does it imply if two events have vector clocks [1, 2, 0] and [1, 0, 1]?",
    "options": [
      "The events are identical.",
      "The events are concurrent (happened at the same time) with no causal relationship.",
      "The second event definitely happened after the first.",
      "The system has encountered a network partition."
    ],
    "answer": "The events are concurrent (happened at the same time) with no causal relationship.",
    "explanation": "Vector clocks capture causality. Event A happened before B if all elements of A are <= B and at least one is <. In this case, neither [1, 2, 0] <= [1, 0, 1] nor vice versa is true (2 > 0 but 0 < 1). This indicates the events are concurrent (conflicting versions).",
    "difficulty": "Advanced"
  },
  {
    "id": 97,
    "question": "What is the 'Zero Downtime Deployment' strategy known as 'Blue-Green Deployment', and what is its main infrastructure requirement?",
    "options": [
      "Deploying the new version to the same servers and restarting the process quickly.",
      "Running two identical production environments (Blue and Green) and switching the router (Load Balancer) to point to the new one.",
      "Deploying the new version to the cloud while keeping the old version on-premise.",
      "Updating the database schema before deploying the application code."
    ],
    "answer": "Running two identical production environments (Blue and Green) and switching the router (Load Balancer) to point to the new one.",
    "explanation": "Blue-Green creates a full duplicate environment. The switch is instant (usually via DNS or Load Balancer config). This requires nearly double the infrastructure capacity (compute and storage) to run both environments simultaneously during the deployment window.",
    "difficulty": "Advanced"
  },
  {
    "id": 98,
    "question": "Why is the 'N+1 Select Problem' a specific performance anti-pattern in ORMs (Object-Relational Mappers)?",
    "options": [
      "It executes N queries for the metadata and 1 query for the data.",
      "It fetches a list of N items with 1 query, but then performs an additional query for EACH of the N items to fetch related data, resulting in 1 + N queries.",
      "It creates N database connections for a single user request.",
      "It locks the database table N times, causing deadlocks."
    ],
    "answer": "It fetches a list of N items with 1 query, but then performs an additional query for EACH of the N items to fetch related data, resulting in 1 + N queries.",
    "explanation": "Lazy loading often triggers this: To load 100 users and their posts, the system runs 1 SQL for users, and then 100 separate SQL calls for posts. This massive increase in database round-trips kills performance. It should be replaced by Eager Loading (Joins).",
    "difficulty": "Advanced"
  },
  {
    "id": 99,
    "question": "In the context of Distributed Hash Tables (DHT) like Chord, how are nodes typically arranged logically?",
    "options": [
      "In a 2-dimensional grid based on datacenter location.",
      "In a ring where each node is responsible for a specific range of keys.",
      "In a strict hierarchy with a single root node.",
      "Randomly, with a central directory service tracking them."
    ],
    "answer": "In a ring where each node is responsible for a specific range of keys.",
    "explanation": "DHTs like Chord or Kademlia organize nodes in a circular identifier space (ring). Each node is assigned an ID (via hash), and it stores the keys that fall within its range of responsibility on the ring, enabling efficient lookups (O(log N)) without a central server.",
    "difficulty": "Advanced"
  }
]