[
  {
    "id": 1,
    "question": "Which primary data structure in pandas is used to represent two-dimensional, size-mutable, potentially heterogeneous tabular data?",
    "options": [
      "Series",
      "DataFrame",
      "Panel",
      "ndarray"
    ],
    "answer": "DataFrame",
    "explanation": "A DataFrame is the primary 2-dimensional data structure in pandas. Series is 1-dimensional, and Panel was deprecated.",
    "difficulty": "Beginner"
  },
  {
    "id": 2,
    "question": "What is the default delimiter used by the pandas `read_csv` function to separate columns?",
    "options": [
      "Tab (\\t)",
      "Semicolon (;)",
      "Comma (,)",
      "Pipe (|)"
    ],
    "answer": "Comma (,)",
    "explanation": "As implied by the name 'read_csv', the function defaults to the Comma Separated Values format.",
    "difficulty": "Beginner"
  },
  {
    "id": 3,
    "question": "Which attribute of a pandas DataFrame returns a tuple representing the dimensionality of the data (rows, columns)?",
    "options": [
      "size",
      "ndim",
      "shape",
      "dimensions"
    ],
    "answer": "shape",
    "explanation": "The `shape` attribute returns a tuple (rows, columns). `size` returns the total number of elements.",
    "difficulty": "Beginner"
  },
  {
    "id": 4,
    "question": "Which method is used to return the first 5 rows of a DataFrame by default for a quick preview?",
    "options": [
      "head()",
      "top()",
      "first()",
      "preview()"
    ],
    "answer": "head()",
    "explanation": "`head(n)` returns the first n rows, defaulting to 5. `tail()` is used for the last rows.",
    "difficulty": "Beginner"
  },
  {
    "id": 5,
    "question": "What does the `describe()` method generate when called on a DataFrame with numeric columns?",
    "options": [
      "A summary of the index",
      "Descriptive statistics (mean, std, min, max, quartiles)",
      "The count of missing values per column",
      "The data types of all columns"
    ],
    "answer": "Descriptive statistics (mean, std, min, max, quartiles)",
    "explanation": "`describe()` provides a summary of the central tendency, dispersion, and shape of the dataset's distribution.",
    "difficulty": "Beginner"
  },
  {
    "id": 6,
    "question": "Which attribute allows you to access the column labels (names) of a DataFrame?",
    "options": [
      "columns",
      "headers",
      "index",
      "keys"
    ],
    "answer": "columns",
    "explanation": "The `columns` attribute stores an Index object containing the column names of the DataFrame.",
    "difficulty": "Beginner"
  },
  {
    "id": 7,
    "question": "Which pandas object is a one-dimensional labeled array capable of holding any data type?",
    "options": [
      "DataFrame",
      "Panel",
      "Series",
      "Matrix"
    ],
    "answer": "Series",
    "explanation": "A Series is a 1D array with axis labels (index), whereas a DataFrame is 2D.",
    "difficulty": "Beginner"
  },
  {
    "id": 8,
    "question": "How do you select a single column named 'price' from a DataFrame `df`?",
    "options": [
      "df('price')",
      "df[['price']]",
      "df['price']",
      "df.price['price']"
    ],
    "answer": "df['price']",
    "explanation": "The standard way to select a single column is using dictionary-like notation `df['column_name']`. `df[['price']]` returns a DataFrame.",
    "difficulty": "Beginner"
  },
  {
    "id": 9,
    "question": "Which indexer is used for label-based selection (selecting by row/column names)?",
    "options": [
      "iloc",
      "loc",
      "at",
      "ix"
    ],
    "answer": "loc",
    "explanation": "`loc` is primarily label-based, whereas `iloc` is integer position-based. `ix` was deprecated and removed.",
    "difficulty": "Beginner"
  },
  {
    "id": 10,
    "question": "Which indexer is used for integer-location based selection (selecting by row/column numbers)?",
    "options": [
      "loc",
      "iloc",
      "pos",
      "get"
    ],
    "answer": "iloc",
    "explanation": "`iloc` stands for integer location and is used to select data based on its numerical position in the index.",
    "difficulty": "Beginner"
  },
  {
    "id": 11,
    "question": "What method is used to rename the labels of an axis (columns or index)?",
    "options": [
      "rename()",
      "change()",
      "set_labels()",
      "reindex()"
    ],
    "answer": "rename()",
    "explanation": "`rename()` allows you to alter axis labels based on a mapping (dictionary) or function.",
    "difficulty": "Beginner"
  },
  {
    "id": 12,
    "question": "Which parameter in the `read_csv` function allows you to treat specific values as NaN (missing data)?",
    "options": [
      "na_values",
      "missing_values",
      "nulls",
      "replace"
    ],
    "answer": "na_values",
    "explanation": "The `na_values` parameter accepts a list of strings that should be interpreted as missing data.",
    "difficulty": "Beginner"
  },
  {
    "id": 13,
    "question": "What is the recommended method to remove rows with missing values (NaN) from a DataFrame?",
    "options": [
      "remove()",
      "delete_na()",
      "dropna()",
      "clean()"
    ],
    "answer": "dropna()",
    "explanation": "`dropna()` removes missing values by default. You can specify axis=0 for rows or axis=1 for columns.",
    "difficulty": "Beginner"
  },
  {
    "id": 14,
    "question": "Which method fills missing values (NaN) with a specific value or method (e.g., forward fill)?",
    "options": [
      "fixna()",
      "fillna()",
      "replace_na()",
      "impute()"
    ],
    "answer": "fillna()",
    "explanation": "`fillna()` replaces NA/NaN values with a specified static value or using a propagation method like 'ffill'.",
    "difficulty": "Beginner"
  },
  {
    "id": 15,
    "question": "What does the `axis=1` parameter signify in a method like `df.drop()`?",
    "options": [
      "Drop rows",
      "Drop columns",
      "Drop the index",
      "Transpose the DataFrame"
    ],
    "answer": "Drop columns",
    "explanation": "In pandas, `axis=0` refers to rows (index), and `axis=1` refers to columns.",
    "difficulty": "Beginner"
  },
  {
    "id": 16,
    "question": "Which method is used to sort a DataFrame by the values in a specific column?",
    "options": [
      "sort_index()",
      "order_by()",
      "sort_values()",
      "arrange()"
    ],
    "answer": "sort_values()",
    "explanation": "`sort_values()` sorts by the values along either axis. `sort_index()` sorts by the index labels.",
    "difficulty": "Beginner"
  },
  {
    "id": 17,
    "question": "Which method is used to apply a function to every element of a Series or DataFrame?",
    "options": [
      "map()",
      "apply()",
      "applymap()",
      "loop()"
    ],
    "answer": "apply()",
    "explanation": "`apply()` applies a function along an axis of the DataFrame or on values of a Series. `map()` is for Series only.",
    "difficulty": "Beginner"
  },
  {
    "id": 18,
    "question": "What is the result of performing vectorized addition on two Series, `s1` and `s2`, if they have mismatched indices?",
    "options": [
      "An Error is raised",
      "The union of indices is introduced, with NaN for missing values",
      "Only the matching indices are summed",
      "The shorter series is recycled"
    ],
    "answer": "The union of indices is introduced, with NaN for missing values",
    "explanation": "Pandas aligns indices automatically; if a label exists in one but not the other, the result is NaN for that position.",
    "difficulty": "Beginner"
  },
  {
    "id": 19,
    "question": "Which method is used to combine two DataFrames by stacking them vertically (appending rows)?",
    "options": [
      "merge()",
      "join()",
      "concat()",
      "append()"
    ],
    "answer": "concat()",
    "explanation": "`pd.concat()` is used to concatenate pandas objects along a particular axis. `append()` was deprecated.",
    "difficulty": "Beginner"
  },
  {
    "id": 20,
    "question": "Which method is used for database-style joining of DataFrames based on columns or indices?",
    "options": [
      "concat()",
      "merge()",
      "attach()",
      "bind()"
    ],
    "answer": "merge()",
    "explanation": "`merge()` is similar to SQL JOINs, allowing you to merge DataFrames on keys (columns or indices).",
    "difficulty": "Beginner"
  },
  {
    "id": 21,
    "question": "Which operation groups data based on column values to perform split-apply-combine logic?",
    "options": [
      "groupby()",
      "cluster()",
      "aggregate()",
      "partition()"
    ],
    "answer": "groupby()",
    "explanation": "The `groupby()` operation involves grouping data, applying a function, and combining the results.",
    "difficulty": "Beginner"
  },
  {
    "id": 22,
    "question": "What is the primary purpose of using the `category` data type in pandas?",
    "options": [
      "To perform faster mathematical calculations",
      "To save memory when a column has repeated low-cardinality text values",
      "To automatically sort the data alphabetically",
      "To convert floats to integers"
    ],
    "answer": "To save memory when a column has repeated low-cardinality text values",
    "explanation": "The `category` dtype uses integer codes internally for repeated strings, significantly reducing memory usage.",
    "difficulty": "Beginner"
  },
  {
    "id": 23,
    "question": "How do you access a scalar value by label (single row, single column) efficiently?",
    "options": [
      "df.loc[row, col]",
      "df.at[row, col]",
      "df.iat[row, col]",
      "df.get(row, col)"
    ],
    "answer": "df.at[row, col]",
    "explanation": "`at` provides fast access to a single scalar value using labels. `loc` can also work but is optimized for slicing.",
    "difficulty": "Beginner"
  },
  {
    "id": 24,
    "question": "What warning is raised when trying to set a value on a slice of a DataFrame that is actually a view?",
    "options": [
      "RuntimeWarning",
      "SettingWithCopyWarning",
      "FutureWarning",
      "DeprecationWarning"
    ],
    "answer": "SettingWithCopyWarning",
    "explanation": "This warning alerts you to a chained assignment issue where you might be modifying a temporary view instead of the original DataFrame.",
    "difficulty": "Beginner"
  },
  {
    "id": 25,
    "question": "Which method is recommended to reset the index of a DataFrame to the default integer index?",
    "options": [
      "reset_index()",
      "reindex()",
      "clear_index()",
      "drop_index()"
    ],
    "answer": "reset_index()",
    "explanation": "`reset_index()` resets the index to a default range index and moves the old index into a column (if not dropped).",
    "difficulty": "Beginner"
  },
  {
    "id": 26,
    "question": "Which technique allows you to chain multiple DataFrame operations together in a single statement?",
    "options": [
      "Method Chaining",
      "Function Composition",
      "Pipe Lining",
      "Sequential Processing"
    ],
    "answer": "Method Chaining",
    "explanation": "Method chaining involves calling successive methods where the return value is the DataFrame itself.",
    "difficulty": "Beginner"
  },
  {
    "id": 27,
    "question": "What is the advantage of using vectorized string operations (e.g., `df['col'].str.upper()`) over Python loops?",
    "options": [
      "It automatically converts integers to strings",
      "It is faster because it uses optimized C/Cython loops under the hood",
      "It handles None values without errors",
      "It creates a copy of the data automatically"
    ],
    "answer": "It is faster because it uses optimized C/Cython loops under the hood",
    "explanation": "Vectorized operations avoid Python loops, executing batch operations in lower-level languages for performance.",
    "difficulty": "Beginner"
  },
  {
    "id": 28,
    "question": "Which method converts a DataFrame from a wide format to a long format (unpivoting)?",
    "options": [
      "pivot()",
      "melt()",
      "stack()",
      "transpose()"
    ],
    "answer": "melt()",
    "explanation": "`melt()` turns multiple columns into identifier variables and 'unpivots' the DataFrame into a long format.",
    "difficulty": "Beginner"
  },
  {
    "id": 29,
    "question": "Which attribute provides the number of elements in a DataFrame (rows x columns)?",
    "options": [
      "length",
      "size",
      "count",
      "total"
    ],
    "answer": "size",
    "explanation": "`size` returns an int representing the total number of elements. `shape` returns the dimensions (rows, cols).",
    "difficulty": "Beginner"
  },
  {
    "id": 30,
    "question": "What does the `pd.to_datetime()` function do?",
    "options": [
      "Calculates the time difference between two dates",
      "Converts an argument to datetime dtype",
      "Formats a datetime object to a string",
      "Extracts the current time"
    ],
    "answer": "Converts an argument to datetime dtype",
    "explanation": "This function converts various inputs (strings, epochs, etc.) into pandas datetime objects.",
    "difficulty": "Beginner"
  },
  {
    "id": 31,
    "question": "Which function is used to create a pivot table in pandas?",
    "options": [
      "pd.pivot()",
      "pd.pivot_table()",
      "pd.transform()",
      "pd.melt()"
    ],
    "answer": "pd.pivot_table()",
    "explanation": "`pivot_table()` allows you to create a spreadsheet-style pivot table as a DataFrame, optionally handling aggregation.",
    "difficulty": "Beginner"
  },
  {
    "id": 32,
    "question": "Which method is used to identify duplicate rows in a DataFrame?",
    "options": [
      "duplicated()",
      "is_duplicate()",
      "repeated()",
      "find_duplicates()"
    ],
    "answer": "duplicated()",
    "explanation": "`duplicated()` returns a boolean Series indicating whether each row is a duplicate of a previous row.",
    "difficulty": "Beginner"
  },
  {
    "id": 33,
    "question": "What is the correct syntax to filter a DataFrame `df` where column 'age' is greater than 30?",
    "options": [
      "df.filter(age > 30)",
      "df[df['age'] > 30]",
      "df.where('age' > 30)",
      "df.select(age > 30)"
    ],
    "answer": "df[df['age'] > 30]",
    "explanation": "Boolean indexing involves passing a boolean Series (created by the condition) into the square brackets `[]`.",
    "difficulty": "Beginner"
  },
  {
    "id": 34,
    "question": "Which method provides the highest performance when looking up a single value by integer position?",
    "options": [
      "df.iloc[row, col]",
      "df.iat[row, col]",
      "df.get_value(row, col)",
      "df.ix[row, col]"
    ],
    "answer": "df.iat[row, col]",
    "explanation": "`iat` is the fastest accessor for retrieving a single scalar value by integer position.",
    "difficulty": "Beginner"
  },
  {
    "id": 35,
    "question": "Which file format is recommended for efficiently saving pandas DataFrames while preserving data types (like categoricals)?",
    "options": [
      "CSV",
      "JSON",
      "Pickle",
      "TXT"
    ],
    "answer": "Pickle",
    "explanation": "Pickle serialization preserves Python object types, while CSV converts everything to strings/floats.",
    "difficulty": "Beginner"
  },
  {
    "id": 36,
    "question": "When optimizing a DataFrame containing a column with repetitive string values (e.g., 'US', 'UK', 'EU'), which dtype conversion yields the greatest memory reduction?",
    "options": [
      "Converting the column to 'object' dtype",
      "Converting the column to 'string' dtype",
      "Converting the column to 'category' dtype",
      "Converting the column to 'float' dtype"
    ],
    "answer": "Converting the column to 'category' dtype",
    "explanation": "The 'category' dtype stores each unique value only once and uses integer mapping for the rest, drastically reducing memory usage for low-cardinality columns compared to storing full string objects.",
    "difficulty": "Intermediate"
  },
  {
    "id": 37,
    "question": "What is the primary performance advantage of using vectorized string operations (e.g., `df['col'].str.upper()`) over `df['col'].apply(str.upper)`?",
    "options": [
      "Vectorized operations automatically handle missing values (NaN) without errors",
      "Vectorized operations utilize optimized C loops via NumPy/Pandas internals, avoiding Python loops",
      "Vectorized operations return a copy of the DataFrame, whereas apply modifies it in-place",
      "Vectorized operations allow for chaining multiple string methods simultaneously"
    ],
    "answer": "Vectorized operations utilize optimized C loops via NumPy/Pandas internals, avoiding Python loops",
    "explanation": "Pandas string methods (`.str`) are implemented in C and optimized to operate on the entire array at once, whereas `apply` generally iterates row-by-row in Python, which is significantly slower.",
    "difficulty": "Intermediate"
  },
  {
    "id": 38,
    "question": "In the context of a `merge` operation, how does setting `left_index=True` or `right_index=True` affect performance compared to merging on columns?",
    "options": [
      "It forces a Cartesian product which increases memory usage",
      "It disables sorting, making the result order unpredictable",
      "It is generally faster because pandas utilizes optimized hash-based lookups on sorted indices",
      "It has no performance impact but changes the column naming convention"
    ],
    "answer": "It is generally faster because pandas utilizes optimized hash-based lookups on sorted indices",
    "explanation": "Merging on indices allows pandas to leverage the pre-sorted order and optimized internal data structures of the Index, avoiding the need to hash and compare arbitrary column values.",
    "difficulty": "Intermediate"
  },
  {
    "id": 39,
    "question": "Which method is specifically designed to retrieve a single scalar value from a DataFrame by label with maximum performance?",
    "options": [
      "df.loc[row, col]",
      "df.at[row, col]",
      "df.iloc[row, col]",
      "df.get_value(row, col)"
    ],
    "answer": "df.at[row, col]",
    "explanation": "`.at[]` is optimized for accessing a single scalar value and is faster than `.loc[]` because it does not attempt to handle slices or arrays, returning the value directly.",
    "difficulty": "Intermediate"
  },
  {
    "id": 40,
    "question": "What is the consequence of using chained assignment (e.g., `df[df['A'] > 0]['B'] = 10`) in pandas?",
    "options": [
      "It modifies the original DataFrame safely",
      "It raises a syntax error immediately",
      "It often triggers a `SettingWithCopyWarning` and may fail to update the original DataFrame",
      "It is the recommended way to perform conditional updates for readability"
    ],
    "answer": "It often triggers a `SettingWithCopyWarning` and may fail to update the original DataFrame",
    "explanation": "Chained assignment creates a temporary object (a copy or a view) depending on memory layout, so the assignment might happen on a temporary object that is immediately discarded, leaving the original DataFrame unchanged.",
    "difficulty": "Intermediate"
  },
  {
    "id": 41,
    "question": "When using `pd.to_datetime` to parse a large column of dates, which argument provides the most significant speed boost?",
    "options": [
      "Setting `errors='coerce'`",
      "Providing a specific `format` string matching the data",
      "Setting `utc=True`",
      "Setting `infer_datetime_format=True`"
    ],
    "answer": "Providing a specific `format` string matching the data",
    "explanation": "When `format` is specified, pandas uses the faster C `strptime` implementation. Without it, pandas attempts to infer the format for each value, which is computationally expensive.",
    "difficulty": "Intermediate"
  },
  {
    "id": 42,
    "question": "How does the `.query()` method compare to standard boolean indexing for large DataFrames?",
    "options": [
      "It is purely syntactic sugar and offers no performance advantage",
      "It is slower because it evaluates strings as Python code",
      "It is often faster as it bypasses some Python overhead by using the numexpr engine internally",
      "It cannot handle complex logical operations"
    ],
    "answer": "It is often faster as it bypasses some Python overhead by using the numexpr engine internally",
    "explanation": "While `query()` is more readable, it is also optimized to use the `numexpr` library for large datasets, which can evaluate boolean expressions faster than standard Python evaluation.",
    "difficulty": "Intermediate"
  },
  {
    "id": 43,
    "question": "What is the function of the `.pipe()` method in pandas method chaining?",
    "options": [
      "To merge two DataFrames together in a chain",
      "To apply a function that expects the DataFrame as its first argument",
      "To serialize the DataFrame to JSON",
      "To calculate the skewness of the data"
    ],
    "answer": "To apply a function that expects the DataFrame as its first argument",
    "explanation": "`.pipe()` allows you to integrate custom functions or external library methods that take a DataFrame as the first argument into a method chain, promoting cleaner and more modular code.",
    "difficulty": "Intermediate"
  },
  {
    "id": 44,
    "question": "Which of the following operations preserves the shape of the original DataFrame when used with `groupby`?",
    "options": [
      "Aggregation (.agg / .mean)",
      "Filtration (.filter)",
      "Transformation (.transform)",
      "Counting (.count)"
    ],
    "answer": "Transformation (.transform)",
    "explanation": "Unlike aggregation which reduces the number of rows to unique groups, `.transform()` returns a Series having the same index as the original object, allowing for direct combination with the original data.",
    "difficulty": "Intermediate"
  },
  {
    "id": 45,
    "question": "Why is `pd.cut()` preferred over manual loop-based binning for discretizing continuous data?",
    "options": [
      "It automatically handles infinite bin ranges",
      "It returns a Categorical variable with optimized memory usage",
      "It is a legacy function and loops are faster",
      "It visualizes the bins automatically"
    ],
    "answer": "It returns a Categorical variable with optimized memory usage",
    "explanation": "`pd.cut()` is vectorized and returns a Categorical object, which is memory efficient and allows for efficient sorting and grouping, whereas manual loops are slow and do not automatically generate useful metadata.",
    "difficulty": "Intermediate"
  },
  {
    "id": 46,
    "question": "What is a key difference between `.stack()` and `.melt()` when reshaping data?",
    "options": [
      ".stack() pivots columns into rows creating a MultiIndex, while .melt() uses specified columns as identifiers",
      ".melt() works only on time-series data",
      ".stack() can only be used on hierarchical indices",
      ".melt() converts wide to long format, but .stack() converts long to wide"
    ],
    "answer": ".stack() pivots columns into rows creating a MultiIndex, while .melt() uses specified columns as identifiers",
    "explanation": "`.stack()` compresses a level in the DataFrame's columns to create a MultiIndex on the rows (long format), whereas `.melt()` is more flexible for unpivoting specific 'wide' columns based on ID variables.",
    "difficulty": "Intermediate"
  },
  {
    "id": 47,
    "question": "When reading a CSV file with `pd.read_csv`, which parameter is best suited to optimize memory usage before the DataFrame is fully loaded?",
    "options": [
      "usecols",
      "nrows",
      "dtype",
      "index_col"
    ],
    "answer": "dtype",
    "explanation": "While `usecols` reduces data volume, specifying `dtype` ensures that columns are stored in the most efficient format immediately (e.g., int32 instead of int64), preventing the need for later conversion.",
    "difficulty": "Intermediate"
  },
  {
    "id": 48,
    "question": "What is the effect of setting `copy_on_write=True` (available in newer pandas versions) during DataFrame operations?",
    "options": [
      "It doubles memory usage for every operation",
      "It makes all operations return views instead of copies",
      "It triggers a defensive copy only when data is modified, delaying the copy cost",
      "It disables SettingWithCopyWarning completely"
    ],
    "answer": "It triggers a defensive copy only when data is modified, delaying the copy cost",
    "explanation": "This mode makes pandas behave more predictably by allowing views during slicing but creating copies only at the moment values are updated (write), adhering to a copy-on-write semantics.",
    "difficulty": "Intermediate"
  },
  {
    "id": 49,
    "question": "Which method allows you to compare two DataFrames and summarize the differences (values that changed)?",
    "options": [
      "df1.diff(df2)",
      "df1.compare(df2)",
      "df1.equals(df2)",
      "pd.merge(df1, df2, how='outer')"
    ],
    "answer": "df1.compare(df2)",
    "explanation": "The `.compare()` method creates a DataFrame showing the values from the caller and the passed DataFrame side-by-side for any elements that differ.",
    "difficulty": "Intermediate"
  },
  {
    "id": 50,
    "question": "In a time-series analysis, what is the primary difference between `.shift()` and `.tshift()` (or `.shift(periods, freq=...)`)?",
    "options": [
      ".shift() moves data by filling with NaNs, while .tshift() modifies the index labels",
      ".shift() changes the frequency of the data",
      ".tshift() is used for resampling, while .shift() is for sorting",
      "There is no difference; they are aliases for the same method"
    ],
    "answer": ".shift() moves data by filling with NaNs, while .tshift() modifies the index labels",
    "explanation": "`shift()` moves the data values forward or backward, creating NaNs in the empty spaces. `tshift()` (now largely replaced by index manipulation) shifted the DateTime index itself.",
    "difficulty": "Intermediate"
  },
  {
    "id": 51,
    "question": "Which aggregation function is best used to determine the cardinality (number of unique values) of a column grouped by another category?",
    "options": [
      "df.groupby('col').count()",
      "df.groupby('col').size()",
      "df.groupby('col').nunique()",
      "df.groupby('col').sum()"
    ],
    "answer": "df.groupby('col').nunique()",
    "explanation": "`.count()` returns non-NaN counts, and `.size()` returns total rows. Only `.nunique()` calculates the number of distinct values within each group.",
    "difficulty": "Intermediate"
  },
  {
    "id": 52,
    "question": "Why might one prefer `.iterrows()` over `.itertuples()` when iterating over rows?",
    "options": [
      ".iterrows() is generally faster for numeric data",
      ".itertuples() returns data in a structure that is harder to access",
      ".iterrows() preserves the data types of the columns, while .itertuples() converts them",
      "There is rarely a reason; .itertuples() is faster and preserves types better"
    ],
    "answer": "There is rarely a reason; .itertuples() is faster and preserves types better",
    "explanation": "`.itertuples()` returns namedtuples, which are much faster to iterate and preserve data types, whereas `.iterrows()` returns a Series for each row (converting types) and is generally slow.",
    "difficulty": "Intermediate"
  },
  {
    "id": 53,
    "question": "What is the recommended pandas method to filter a DataFrame based on a complex string condition (e.g., \"A > 5 and B == 'text'\")?",
    "options": [
      "df[(df['A'] > 5) & (df['B'] == 'text')]",
      "df.query(\"A > 5 and B == 'text'\")",
      "df.filter(items=['A', 'B'])",
      "df.loc[\"A > 5 and B == 'text'\"]"
    ],
    "answer": "df.query(\"A > 5 and B == 'text'\")",
    "explanation": "While boolean indexing is standard, `.query()` allows for cleaner syntax and can be faster for large datasets as it offloads expression evaluation to the numexpr engine.",
    "difficulty": "Intermediate"
  },
  {
    "id": 54,
    "question": "How does `pd.factorize()` differ from `astype('category')`?",
    "options": [
      "pd.factorize() returns an array of integer codes and the unique values, while astype creates a mapping",
      "pd.factorize() automatically sorts the unique values",
      "astype('category') is faster but loses information",
      "There is no difference; they produce identical outputs"
    ],
    "answer": "pd.factorize() returns an array of integer codes and the unique values, while astype creates a mapping",
    "explanation": "`factorize` is a utility to encode values as integers (useful for ML inputs) and returns the unique values separately, whereas `astype('category')` transforms the column data type entirely.",
    "difficulty": "Intermediate"
  },
  {
    "id": 55,
    "question": "What is the purpose of the `sparse` parameter in pandas data structures?",
    "options": [
      "To compress DataFrames stored on disk",
      "To efficiently store data with mostly missing (NaN) values by only storing non-NaN data",
      "To limit the number of rows returned by a query",
      "To convert dense arrays into lists for easier processing"
    ],
    "answer": "To efficiently store data with mostly missing (NaN) values by only storing non-NaN data",
    "explanation": "Sparse dtypes are designed to save memory and computation for arrays containing mostly zeros or NaNs by storing only the non-empty values and their coordinates.",
    "difficulty": "Intermediate"
  },
  {
    "id": 56,
    "question": "Which method is used to handle duplicate indices that may arise during concatenation or merging?",
    "options": [
      "df.verify_integrity()",
      "pd.concat([df1, df2], verify_integrity=True)",
      "df.merge(df2, on='index', validate='one_to_one')",
      "Both B and C"
    ],
    "answer": "Both B and C",
    "explanation": "`verify_integrity=True` raises an error if duplicate indices are created in a concat, and `validate='one_to_one'` ensures a merge does not create duplicates in the key.",
    "difficulty": "Intermediate"
  },
  {
    "id": 57,
    "question": "When using `pd.get_dummies`, what is the impact of setting `drop_first=True`?",
    "options": [
      "It drops the last column of the DataFrame",
      "It creates dummy variables but removes the first level to avoid multicollinearity (dummy variable trap)",
      "It removes rows with missing values before creating dummies",
      "It only creates dummies for the first column in the list"
    ],
    "answer": "It creates dummy variables but removes the first level to avoid multicollinearity (dummy variable trap)",
    "explanation": "In linear regression models, including all categories creates perfect multicollinearity. `drop_first=True` removes one category so that the others are independent reference points.",
    "difficulty": "Intermediate"
  },
  {
    "id": 58,
    "question": "What is the main advantage of using the `eval()` method on a DataFrame?",
    "options": [
      "It allows the execution of arbitrary Python code strings",
      "It evaluates string expressions efficiently using large data libraries (like numexpr) and reduces intermediate memory usage",
      "It replaces the need for the .query() method",
      "It is faster than standard arithmetic for all types of operations"
    ],
    "answer": "It evaluates string expressions efficiently using large data libraries (like numexpr) and reduces intermediate memory usage",
    "explanation": "`eval()` parses string expressions and evaluates them using optimized C or numexpr engines, often avoiding the creation of temporary intermediate arrays that standard arithmetic would produce.",
    "difficulty": "Intermediate"
  },
  {
    "id": 59,
    "question": "Which method is preferred to apply a function to a DataFrame element-wise with support for parallel processing (if installed) via the `engine` parameter?",
    "options": [
      "df.applymap()",
      "df.apply()",
      "df.transform()",
      "df.aggregate()"
    ],
    "answer": "df.apply()",
    "explanation": "While `applymap` works element-wise, `apply` with `engine='numexpr'` or similar (in newer versions) allows for optimization. Note: `applymap` was deprecated in favor of `DataFrame.map` in newer versions, but `apply` remains the general workhorse.",
    "difficulty": "Intermediate"
  },
  {
    "id": 60,
    "question": "When resampling time-series data, what is the difference between `.asfreq()` and `.resample()`?",
    "options": [
      ".asfreq() is a simplified method for frequency conversion without aggregation, while .resample() implies a group-by operation",
      ".resample() only works on daily data",
      ".asfreq() fills NaN values automatically, while .resample() drops them",
      "There is no functional difference"
    ],
    "answer": ".asfreq() is a simplified method for frequency conversion without aggregation, while .resample() implies a group-by operation",
    "explanation": "`.asfreq()` simply converts the index to the specified frequency, generating NaNs or padding. `.resample()` is a powerful group-by-like operation that allows for aggregation (mean, sum, etc.) over time bins.",
    "difficulty": "Intermediate"
  },
  {
    "id": 61,
    "question": "What is the functionality of `df.explode()` on a column containing list-like objects?",
    "options": [
      "It converts lists into strings",
      "It flattens lists by transforming each list element into a separate row, replicating the index values",
      "It removes the list column from the DataFrame",
      "It calculates the variance of the numerical elements in the lists"
    ],
    "answer": "It flattens lists by transforming each list element into a separate row, replicating the index values",
    "explanation": "`.explode()` transforms each element of a list-like object (like a list or tuple) in a cell into a row, replicating the index values from the original row.",
    "difficulty": "Intermediate"
  },
  {
    "id": 62,
    "question": "Why is it generally recommended to use `df.values` sparingly or avoid it in favor of `df.to_numpy()`?",
    "options": [
      "df.values is a deprecated attribute",
      "df.values does not handle the ExtensionArray system well (e.g., Nullable Integers), whereas to_numpy() resolves them correctly",
      "df.values always returns a copy, wasting memory",
      "df.to_numpy() is strictly faster for all data types"
    ],
    "answer": "df.values does not handle the ExtensionArray system well (e.g., Nullable Integers), whereas to_numpy() resolves them correctly",
    "explanation": "The `.values` attribute is legacy and can sometimes return the underlying ExtensionArray object rather than a pure NumPy array, leading to compatibility issues. `.to_numpy()` is the explicit, recommended interface.",
    "difficulty": "Intermediate"
  },
  {
    "id": 63,
    "question": "Which technique allows you to perform a rolling window calculation that expands to include all past data points?",
    "options": [
      "df.rolling(window=n)",
      "df.expanding()",
      "df.ewm(alpha=0.5)",
      "df.shift(1)"
    ],
    "answer": "df.expanding()",
    "explanation": "While `.rolling()` uses a fixed window size, `.expanding()` creates a window that grows with each data point, including all data from the start of the series up to the current point.",
    "difficulty": "Intermediate"
  },
  {
    "id": 64,
    "question": "How does the `memory_usage(deep=True)` parameter differ from the default `memory_usage()` calculation?",
    "options": [
      "It includes memory used by the Index",
      "It includes the memory usage of object dtypes (strings) by introspecting the actual data",
      "It calculates the total system memory usage",
      "It forces garbage collection before calculating usage"
    ],
    "answer": "It includes the memory usage of object dtypes (strings) by introspecting the actual data",
    "explanation": "The default usage is an approximation calculated from the dtype. `deep=True` performs a system-level check of the actual objects in object columns, which is accurate but slower.",
    "difficulty": "Intermediate"
  },
  {
    "id": 65,
    "question": "What is the result of applying `df.groupby('col').transform('mean')`?",
    "options": [
      "A DataFrame with one row per unique group containing the mean",
      "A Series the same length as the original DataFrame, where each value is replaced by its group mean",
      "A scalar representing the grand mean",
      "An error because transform does not accept string arguments"
    ],
    "answer": "A Series the same length as the original DataFrame, where each value is replaced by its group mean",
    "explanation": "The `transform` method applies a function to each group and returns a result aligned with the original index, broadcasting the group's aggregated value (e.g., mean) back to every row in that group.",
    "difficulty": "Intermediate"
  },
  {
    "id": 66,
    "question": "When creating a pivot table using `pd.pivot_table`, what does the `margins` parameter do?",
    "options": [
      "It adds row and column subtotals (All)",
      "It defines the width of the table cells",
      "It removes NaN values from the result",
      "It flattens the hierarchical columns"
    ],
    "answer": "It adds row and column subtotals (All)",
    "explanation": "Setting `margins=True` adds a special 'All' row and column to the pivot table that computes the aggregate across all categories in that dimension.",
    "difficulty": "Intermediate"
  },
  {
    "id": 67,
    "question": "Which file format typically supports preserving pandas-specific data types (like DatetimeIndex and Categorical) most efficiently for reloading?",
    "options": [
      "CSV",
      "JSON",
      "Pickle (.pkl)",
      "Excel (.xlsx)"
    ],
    "answer": "Pickle (.pkl)",
    "explanation": "Pickle is a Python-specific serialization format that preserves object structure and arbitrary data types perfectly, whereas CSV, JSON, and Excel often convert types (e.g., datetime to string).",
    "difficulty": "Intermediate"
  },
  {
    "id": 68,
    "question": "In the context of method chaining, what does `.assign()` offer over direct column assignment (e.g., `df['new'] = ...`)?",
    "options": [
      "It allows you to create a new column based on the current state of the DataFrame within a chain",
      "It automatically downcasts the new column to save memory",
      "It prevents SettingWithCopyWarning always",
      "It is the only way to add columns to a MultiIndex DataFrame"
    ],
    "answer": "It allows you to create a new column based on the current state of the DataFrame within a chain",
    "explanation": "`.assign()` returns a new DataFrame with the new column added, enabling method chaining. Direct assignment (`df['col'] = ...`) is an in-place operation that breaks the chain.",
    "difficulty": "Intermediate"
  },
  {
    "id": 69,
    "question": "What is the primary risk of performing a `merge` operation without explicitly specifying the `on` parameter when column names overlap?",
    "options": [
      "The operation will fail and raise a KeyError",
      "Pandas defaults to an inner merge on all overlapping column names",
      "Pandas defaults to a Cartesian product (cross join)",
      "Pandas merges only on the index"
    ],
    "answer": "Pandas defaults to an inner merge on all overlapping column names",
    "explanation": "If `on` is not specified, pandas automatically uses the intersection of column names from both DataFrames as the join keys, which may not be the intended behavior if columns share names by coincidence.",
    "difficulty": "Intermediate"
  },
  {
    "id": 70,
    "question": "In the context of pandas internal memory management, how are DataFrames with homogeneous data types (e.g., all floats) stored compared to heterogeneous data types?",
    "options": [
      "Heterogeneous DataFrames store data column-wise in a single block, while homogeneous ones use a jagged array",
      "Homogeneous DataFrames utilize a single block manager, whereas heterogeneous DataFrames split data into multiple blocks based on dtype",
      "Both types utilize a row-major (C-order) single block, but homogeneous data types use automatic memory paging",
      "Heterogeneous DataFrames store data in a Dictionary of Lists, while homogeneous DataFrames use a NumPy ndarray"
    ],
    "answer": "Homogeneous DataFrames utilize a single block manager, whereas heterogeneous DataFrames split data into multiple blocks based on dtype",
    "explanation": "pandas uses the BlockManager to store data. If all columns have the same dtype, a single block is used (contiguous memory). Mixed dtypes are split into separate blocks, increasing overhead.",
    "difficulty": "Advanced"
  },
  {
    "id": 71,
    "question": "What is the primary computational advantage of using the `numexpr` engine via `df.eval()` for complex arithmetic operations?",
    "options": [
      "It automatically compiles the expression into C++ extensions",
      "It avoids allocating intermediate temporary arrays for each operation step in the expression",
      "It utilizes multiple CPU cores to parallelize the vectorized operation",
      "It converts the DataFrame into a sparse matrix format for calculation"
    ],
    "answer": "It avoids allocating intermediate temporary arrays for each operation step in the expression",
    "explanation": "Standard Python/pandas evaluation creates temporary arrays for each intermediate step (e.g., A+B in A+B+C). `numexpr` computes the entire expression at once, reducing memory bandwidth usage and overhead.",
    "difficulty": "Advanced"
  },
  {
    "id": 72,
    "question": "Why is `df.itertuples()` generally preferred over `df.iterrows()` for iterating through DataFrame rows?",
    "options": [
      "`itertuples` preserves the data types of the columns, while `iterrows` converts them to objects",
      "`itertuples` returns a view of the DataFrame, avoiding memory copies",
      "`itertuples` uses Cython-optimized row indexing, whereas `iterrows` uses pure Python",
      "`itertuples` is the only method that supports modifying the DataFrame in-place during iteration"
    ],
    "answer": "`itertuples` preserves the data types of the columns, while `iterrows` converts them to objects",
    "explanation": "`iterrows()` returns a Series for each row, which converts dtypes to object (causing overhead), whereas `itertuples()` returns a `namedtuple`, preserving the original C-level dtypes and being significantly faster.",
    "difficulty": "Advanced"
  },
  {
    "id": 73,
    "question": "When using `pd.merge()`, how does the `validate='one_to_one'` argument assist in data integrity?",
    "options": [
      "It enforces that the merge keys are unique in both the left and right DataFrames, raising a `MergeError` otherwise",
      "It automatically drops duplicate keys from the right DataFrame to ensure a one-to-one match",
      "It checks that the number of rows in the merged result equals the sum of rows in the inputs",
      "It converts the merge keys into a categorical index to speed up the join operation"
    ],
    "answer": "It enforces that the merge keys are unique in both the left and right DataFrames, raising a `MergeError` otherwise",
    "explanation": "The `validate` argument performs a cardinality check. `one_to_one` ensures the keys are unique in both DataFrames, preventing accidental many-to-many or one-to-many cartilage products that often indicate data quality issues.",
    "difficulty": "Advanced"
  },
  {
    "id": 74,
    "question": "What is the specific behavior of the `df.to_json()` method when dealing with a DataFrame containing a MultiIndex?",
    "options": [
      "It automatically flattens the MultiIndex into a single column of tuples",
      "It raises a `ValueError` unless the `orient` parameter is set to 'records' or 'index'",
      "It serializes the index levels as nested JSON objects rather than flattening them",
      "It drops the index and resets it to a default RangeIndex before serialization"
    ],
    "answer": "It automatically flattens the MultiIndex into a single column of tuples",
    "explanation": "By default, `to_json()` does not support hierarchical indices in standard JSON structures. It typically flattens the index into a tuple representation or requires specific `orient` strategies that don't perfectly map hierarchy.",
    "difficulty": "Advanced"
  },
  {
    "id": 75,
    "question": "How does the `copy-on-write` (CoW) mode, introduced in recent pandas versions, fundamentally change DataFrame mutation behavior?",
    "options": [
      "It makes all DataFrame operations lazy, evaluated only when `compute()` is called",
      "It ensures that updating a slice of a DataFrame (e.g., using `.loc`) never modifies the original parent object",
      "It automatically creates hard links to the underlying data on disk to save RAM",
      "It defers memory allocation until the DataFrame is explicitly written to a file"
    ],
    "answer": "It ensures that updating a slice of a DataFrame (e.g., using `.loc`) never modifies the original parent object",
    "explanation": "In CoW mode, any mutation triggers a copy of the underlying data. This prevents `SettingWithCopyWarning` issues and ensures that derived DataFrames (views) become independent copies upon modification.",
    "difficulty": "Advanced"
  },
  {
    "id": 76,
    "question": "When using `pd.read_csv()` for memory optimization, what is the technical reason for specifying `dtype={'column': 'category'}` for low-cardinality strings?",
    "options": [
      "It compresses the data using the Zlib algorithm",
      "It stores the data as a dictionary of unique values mapping to integer codes, reducing memory footprint",
      "It converts the string data into a C-style `char*` array",
      "It moves the column data to disk and uses a memory-mapped file handle"
    ],
    "answer": "It stores the data as a dictionary of unique values mapping to integer codes, reducing memory footprint",
    "explanation": "The `category` dtype stores data as an array of integers mapping to a small table of unique values. This drastically reduces memory usage compared to storing the full string for every row.",
    "difficulty": "Advanced"
  },
  {
    "id": 77,
    "question": "What is the functional difference between `df.groupby('col').transform('sum')` and `df.groupby('col').agg('sum')`?",
    "options": [
      "`transform` broadcasts the sum back to the original shape of the DataFrame, while `agg` returns a DataFrame with one row per group",
      "`transform` is optimized for string data types, while `agg` works only on numerics",
      "`agg` creates an index based on the group keys, while `transform` drops the group keys",
      "`transform` returns the result sorted by the group values, while `agg` returns the result unsorted"
    ],
    "answer": "`transform` broadcasts the sum back to the original shape of the DataFrame, while `agg` returns a DataFrame with one row per group",
    "explanation": "The `transform` method returns a Series or DataFrame having the same index as the original object, filled with the grouped calculation, allowing for easy column-wise combination.",
    "difficulty": "Advanced"
  },
  {
    "id": 78,
    "question": "Which `pandas.api.extensions.ExtensionDtype` behavior is required to implement a custom data type that supports pandas 2.0+ PyArrow interoperability?",
    "options": [
      "The dtype must implement the `__from_arrow__` method to construct the Array from an Arrow Array",
      "The dtype must inherit directly from `numpy.ndarray`",
      "The dtype must be immutable and hashable, but cannot interact with the buffer protocol",
      "The dtype must override the `reduce` function to support serialization"
    ],
    "answer": "The dtype must implement the `__from_arrow__` method to construct the Array from an Arrow Array",
    "explanation": "To support zero-copy conversions and interoperability with the PyArrow backend introduced in pandas 2.0, extension types must implement conversion methods like `__from_arrow__`.",
    "difficulty": "Advanced"
  },
  {
    "id": 79,
    "question": "What is the performance implication of using `df.sort_values()` before performing a `df.merge()` on non-index columns?",
    "options": [
      "Sorting allows pandas to use the 'merge' algorithm (linear scan) instead of 'hash', reducing memory overhead but requiring pre-sorted keys",
      "Sorting is deprecated in favor of hash joins and will trigger a `FutureWarning`",
      "Sorting converts the merge into an 'inner' join automatically, ignoring the `how` parameter",
      "Sorting has no impact on merge performance as hash tables are always constructed regardless of order"
    ],
    "answer": "Sorting allows pandas to use the 'merge' algorithm (linear scan) instead of 'hash', reducing memory overhead but requiring pre-sorted keys",
    "explanation": "If the DataFrames are already sorted by the merge keys, pandas can use the `merge` algorithm (linear complexity, lower memory) instead of the default `hash` algorithm (linear complexity, higher memory usage for hash tables).",
    "difficulty": "Advanced"
  },
  {
    "id": 80,
    "question": "In the context of `pd.Series.str` accessor methods, which method is most efficient for validating that a column conforms to a specific regex pattern without extracting substrings?",
    "options": [
      "Using `Series.str.extract()` and checking if the result is `None`",
      "Using `Series.str.match()` which returns a boolean array indicating if the pattern matches at the beginning of the string",
      "Using `Series.str.contains()` with `regex=True`",
      "Using `Series.str.fullmatch()` which checks if the whole string matches the regex pattern"
    ],
    "answer": "Using `Series.str.fullmatch()` which checks if the whole string matches the regex pattern",
    "explanation": "While `match` checks the start, `fullmatch` ensures the entire string conforms to the pattern. `contains` returns True if the pattern is found anywhere, which isn't a validation of format.",
    "difficulty": "Advanced"
  },
  {
    "id": 81,
    "question": "What is the purpose of the `memory_usage(deep=True)` method in pandas?",
    "options": [
      "It calculates the memory usage of the object including the memory of the object's container overhead only",
      "It includes the memory usage of the actual data contained in object dtypes (like strings) which is not accounted for in the shallow usage",
      "It forces the garbage collector to run before calculating the memory size",
      "It converts the DataFrame into a dense array format to calculate the theoretical minimum memory usage"
    ],
    "answer": "It includes the memory usage of the actual data contained in object dtypes (like strings) which is not accounted for in the shallow usage",
    "explanation": "The default `memory_usage` only accounts for the pointer size for object dtypes. `deep=True` introspects the actual Python objects being pointed to (e.g., the byte length of strings) to calculate real memory consumption.",
    "difficulty": "Advanced"
  },
  {
    "id": 82,
    "question": "How does `pd.factorize()` differ from `pd.astype('category')`?",
    "options": [
      "`pd.factorize()` returns integer codes and the unique values, but does not store the result as a categorical dtype",
      "`pd.factorize()` automatically sorts the unique values, whereas `astype('category')` preserves the order of appearance",
      "`pd.factorize()` only works on numeric data types",
      "`pd.factorize()` creates a categorical type with a fixed interval, treating data as quantitative"
    ],
    "answer": "`pd.factorize()` returns integer codes and the unique values, but does not store the result as a categorical dtype",
    "explanation": "`factorize` is a utility to encode data as integers (useful for indexing or machine learning), returning (codes, uniques). It does not inherently convert the column to the `category` dtype metadata.",
    "difficulty": "Advanced"
  },
  {
    "id": 83,
    "question": "When using `df.resample('ME').mean()` on a time-series with missing timestamps, how does pandas handle the periods with no data points?",
    "options": [
      "It interpolates the missing values based on surrounding periods",
      "It creates a row for the period with a `NaN` value for the aggregation",
      "It skips the period entirely, resulting in a non-regular time index",
      "It raises a `ValueError` requiring the user to `ffill()` the data first"
    ],
    "answer": "It creates a row for the period with a `NaN` value for the aggregation",
    "explanation": "By default, `resample` creates a regular index for the new frequency. Periods with no data appearing in the bin will result in `NaN` (or `NaT`) for the aggregation result.",
    "difficulty": "Advanced"
  },
  {
    "id": 84,
    "question": "What is the behavior of `df.loc[mask]` where `mask` is a list of boolean values compared to `mask` being a boolean Series?",
    "options": [
      "The boolean list is treated as column selection, whereas the Series is treated as row selection",
      "Pandas treats the boolean list as an index of positions (positional indexing) rather than a value-based filter",
      "The boolean list must be the same length as the DataFrame's columns, while the Series must match the index",
      "There is no difference; pandas automatically coerces the list to a Series"
    ],
    "answer": "Pandas treats the boolean list as an index of positions (positional indexing) rather than a value-based filter",
    "explanation": "A boolean list is interpreted as a list of integer labels (positional indexing by the boolean values themselves, which is confusing and rarely intended). A boolean Series with a matching index performs value-based filtering (alignment).",
    "difficulty": "Advanced"
  },
  {
    "id": 85,
    "question": "Which parameter in `df.to_parquet()` controls the trade-off between write speed and compression ratio?",
    "options": [
      "`compression_level`",
      "`engine`",
      "`compression`",
      "`row_group_size`"
    ],
    "answer": "`compression`",
    "explanation": "The `compression` parameter (e.g., 'snappy', 'gzip', 'brotli') selects the codec. Snappy is faster but compresses less; Gzip/Brotli are slower but smaller files.",
    "difficulty": "Advanced"
  },
  {
    "id": 86,
    "question": "What does the `pd.api.types.is_extension_array_dtype(arr)` function check?",
    "options": [
      "Whether the array is a subclass of `numpy.ndarray`",
      "Whether the array supports the buffer protocol",
      "Whether the dtype is registered in the pandas registry as a non-standard NumPy dtype (e.g., Categorical, DatetimeTZ, Sparse)",
      "Whether the array contains only scalar values"
    ],
    "answer": "Whether the dtype is registered in the pandas registry as a non-standard NumPy dtype (e.g., Categorical, DatetimeTZ, Sparse)",
    "explanation": "Extension arrays are types defined outside of standard NumPy. This function checks if the dtype is one of these custom types, which often have custom behaviors and memory layouts.",
    "difficulty": "Advanced"
  },
  {
    "id": 87,
    "question": "In the context of the `.pipe()` function, what is the primary advantage over standard method chaining?",
    "options": [
      "`.pipe()` automatically parallelizes the operation",
      "`.pipe()` allows passing custom functions or lambda expressions that accept the DataFrame as an argument, enabling reusability",
      "`.pipe()` modifies the DataFrame in-place to save memory",
      "`.pipe()` bypasses the index alignment check"
    ],
    "answer": "`.pipe()` allows passing custom functions or lambda expressions that accept the DataFrame as an argument, enabling reusability",
    "explanation": "`.pipe()` allows you to inject arbitrary functions into a method chain. It is particularly useful for applying custom, reusable transformations that don't exist as built-in DataFrame methods.",
    "difficulty": "Advanced"
  },
  {
    "id": 88,
    "question": "When using `df.stack()` on a DataFrame with MultiIndex columns, which parameter controls whether columns with all `NaN` values are dropped in the result?",
    "options": [
      "`dropna`",
      "`level`",
      "`sort`",
      "`fill_value`"
    ],
    "answer": "`dropna`",
    "explanation": "The `dropna` parameter (default=True) determines whether columns (or levels) that consist entirely of missing values in the result are removed from the hierarchy.",
    "difficulty": "Advanced"
  },
  {
    "id": 89,
    "question": "How does the `pd.Grouper` function facilitate complex grouping operations in `groupby`?",
    "options": [
      "It allows for grouping by a frequency (time-based) or a specific key simultaneously",
      "It forces the groupby operation to sort the data before grouping",
      "It converts the grouped object into a sparse matrix",
      "It automatically creates a hash of the group keys to speed up aggregation"
    ],
    "answer": "It allows for grouping by a frequency (time-based) or a specific key simultaneously",
    "explanation": "`pd.Grouper` (often seen as `key='col'` or `freq='D'`) is particularly powerful for grouping time-series data by specific frequencies (e.g., 'ME') or combining multiple keys/indexes in a groupby specification.",
    "difficulty": "Advanced"
  },
  {
    "id": 90,
    "question": "What is the `pd.NA` scalar value intended to replace in the pandas ecosystem?",
    "options": [
      "The use of `np.nan` for floating-point missing data",
      "The use of `None` for object-dtype missing data and `np.nan` for numerics, providing a unified missing value indicator",
      "The use of `pd.isna()` function",
      "The use of `NaN` strings in CSV files"
    ],
    "answer": "The use of `None` for object-dtype missing data and `np.nan` for numerics, providing a unified missing value indicator",
    "explanation": "`pd.NA` is a singleton designed to act as a universal missing indicator for all dtypes (integer, boolean, string), unlike `np.nan` which forces float conversion.",
    "difficulty": "Advanced"
  },
  {
    "id": 91,
    "question": "What is the effect of setting `df.flags.allows_duplicate_labels = False`?",
    "options": [
      "It prevents any column from having the same value as another column in the same row",
      "It raises an error when setting an index or columns that have duplicate labels",
      "It automatically renames duplicate labels by appending a suffix",
      "It converts all labels to strings to facilitate comparison"
    ],
    "answer": "It raises an error when setting an index or columns that have duplicate labels",
    "explanation": "This setting enforces label uniqueness (like a primary key in SQL). Any operation resulting in non-unique axes will raise a `DuplicateLabelError`.",
    "difficulty": "Advanced"
  }
]