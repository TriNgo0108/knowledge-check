[
  {
    "id": 1,
    "question": "Which primary data structure in pandas is used to represent two-dimensional, size-mutable, potentially heterogeneous tabular data?",
    "options": [
      "Series",
      "DataFrame",
      "Panel",
      "Matrix"
    ],
    "answer": "DataFrame",
    "explanation": "A DataFrame is a 2D labeled data structure with columns of potentially different types. Series is 1D, and Panel is deprecated (removed in modern pandas).",
    "difficulty": "Beginner"
  },
  {
    "id": 2,
    "question": "What is the default return type of a single column selection from a pandas DataFrame?",
    "options": [
      "List",
      "NumPy array",
      "pandas Series",
      "Dictionary"
    ],
    "answer": "pandas Series",
    "explanation": "Selecting a single column returns a Series, which is a 1D labeled array. Returning a NumPy array requires accessing the `.values` attribute or using `.to_numpy()`.",
    "difficulty": "Beginner"
  },
  {
    "id": 3,
    "question": "Which attribute of a DataFrame returns a tuple representing the dimensionality (rows, columns)?",
    "options": [
      "ndim",
      "size",
      "shape",
      "dimensions"
    ],
    "answer": "shape",
    "explanation": "The `shape` attribute returns a tuple (rows, columns). `ndim` returns the number of dimensions (2 for a DataFrame), and `size` returns the total number of elements.",
    "difficulty": "Beginner"
  },
  {
    "id": 4,
    "question": "Which function is used to read a comma-separated values (CSV) file into a DataFrame?",
    "options": [
      "pd.read_csv()",
      "pd.load_csv()",
      "pd.open_csv()",
      "pd.import_csv()"
    ],
    "answer": "pd.read_csv()",
    "explanation": "`read_csv` is the primary parser function for CSV files. `load_csv`, `open_csv`, and `import_csv` are not standard pandas functions.",
    "difficulty": "Beginner"
  },
  {
    "id": 5,
    "question": "How are missing values represented in a pandas DataFrame or Series by default?",
    "options": [
      "None",
      "NaN",
      "Null",
      "NA"
    ],
    "answer": "NaN",
    "explanation": "Pandas uses `NaN` (Not a Number), specifically the IEEE 754 floating-point representation, to denote missing data. While Python's `None` is accepted, it is converted to `NaN` in float or object arrays.",
    "difficulty": "Beginner"
  },
  {
    "id": 6,
    "question": "What is the primary advantage of using vectorized operations in pandas instead of Python loops?",
    "options": [
      "They use less memory on disk",
      "They are implemented in C and are much faster",
      "They automatically handle missing data without errors",
      "They make the code easier to read"
    ],
    "answer": "They are implemented in C and are much faster",
    "explanation": "Vectorized operations leverage low-level optimized C and NumPy implementations, avoiding the overhead of the Python interpreter loop.",
    "difficulty": "Beginner"
  },
  {
    "id": 7,
    "question": "Which method is used to remove duplicate rows from a DataFrame?",
    "options": [
      "df.drop_duplicates()",
      "df.remove_duplicates()",
      "df.delete_duplicates()",
      "df.clean_duplicates()"
    ],
    "answer": "df.drop_duplicates()",
    "explanation": "`drop_duplicates()` identifies and removes rows where all specified column values are identical. The other methods do not exist in the standard API.",
    "difficulty": "Beginner"
  },
  {
    "id": 8,
    "question": "Which parameter in `pd.read_csv` allows you to process a large file in smaller, manageable pieces?",
    "options": [
      "batch_size",
      "chunksize",
      "iterator",
      "split_rows"
    ],
    "answer": "chunksize",
    "explanation": "The `chunksize` parameter returns a TextFileReader object yielding an iterable of DataFrames. `batch_size` is common in deep learning libraries, not standard `read_csv`.",
    "difficulty": "Beginner"
  },
  {
    "id": 9,
    "question": "Which data type is best optimized for memory usage when a column contains a limited set of repetitive string values?",
    "options": [
      "object",
      "str",
      "category",
      "factor"
    ],
    "answer": "category",
    "explanation": "The `category` dtype uses integer-based representation internally, drastically reducing memory usage for repetitive string data compared to the generic `object` type.",
    "difficulty": "Beginner"
  },
  {
    "id": 10,
    "question": "What does the inplace=True argument do in methods like df.dropna()?",
    "options": [
      "Returns a new copy of the DataFrame",
      "Modifies the original DataFrame object directly",
      "Stores the DataFrame in a temporary file",
      "Checks if the operation is valid before applying it"
    ],
    "answer": "Modifies the original DataFrame object directly",
    "explanation": "When `inplace=True`, the operation modifies the object in place and returns `None`. By default (False), pandas returns a new copy with the changes applied.",
    "difficulty": "Beginner"
  },
  {
    "id": 11,
    "question": "Which accessor is used to select rows and columns by their label (name)?",
    "options": [
      "df.iloc[]",
      "df.loc[]",
      "df.pos[]",
      "df.at[]"
    ],
    "answer": "df.loc[]",
    "explanation": "`.loc[]` is label-based, accepting index or column names. `.iloc[]` is integer-position based. `.at[]` is label-based but for accessing single scalar values only.",
    "difficulty": "Beginner"
  },
  {
    "id": 12,
    "question": "Which accessor is used to select rows and columns by their integer position?",
    "options": [
      "df.loc[]",
      "df.iloc[]",
      "df.ix[]",
      "df.int[]"
    ],
    "answer": "df.iloc[]",
    "explanation": "`.iloc[]` is strictly integer-location based (0 to length-1). `.ix[]` was deprecated and removed in pandas 1.0.0.",
    "difficulty": "Beginner"
  },
  {
    "id": 13,
    "question": "How do you select the first 5 rows of a DataFrame named df?",
    "options": [
      "df.head(5)",
      "df.top(5)",
      "df.first(5)",
      "df.start(5)"
    ],
    "answer": "df.head(5)",
    "explanation": "The `head(n)` method returns the first n rows. `top`, `first`, and `start` are not standard DataFrame methods for this purpose.",
    "difficulty": "Beginner"
  },
  {
    "id": 14,
    "question": "Which method provides a concise summary of a DataFrame, including the index dtype, non-null values, and memory usage?",
    "options": [
      "df.summary()",
      "df.describe()",
      "df.info()",
      "df.structure()"
    ],
    "answer": "df.info()",
    "explanation": "`df.info()` prints metadata about the DataFrame (types, non-nulls, memory). `describe()` provides statistical summaries of numeric columns.",
    "difficulty": "Beginner"
  },
  {
    "id": 15,
    "question": "What is the result of the expression df['A'] > 5?",
    "options": [
      "A filtered DataFrame",
      "A Series of boolean values",
      "A Series of integers 0 and 1",
      "An integer count of True values"
    ],
    "answer": "A Series of boolean values",
    "explanation": "Comparisons on a Series return a boolean Series of the same length. This mask is often used for filtering (passed into `df[...]`).",
    "difficulty": "Beginner"
  },
  {
    "id": 16,
    "question": "Which method is used to fill missing values (NaN) with a specific value or calculated statistic?",
    "options": [
      "df.replace()",
      "df.fillna()",
      "df.impute()",
      "df.clean()"
    ],
    "answer": "df.fillna()",
    "explanation": "`fillna()` is specifically designed to handle NA/NaN data by filling it with a scalar or dictionary. `replace()` is for general value replacement, not just NAs.",
    "difficulty": "Beginner"
  },
  {
    "id": 17,
    "question": "Which method is used to apply a function along an axis of the DataFrame (row-wise or column-wise)?",
    "options": [
      "df.map()",
      "df.apply()",
      "df.transform()",
      "df.aggregate()"
    ],
    "answer": "df.apply()",
    "explanation": "`apply()` applies a function along the specified axis (0 for columns, 1 for rows). `map` is generally for Series element-wise operations.",
    "difficulty": "Beginner"
  },
  {
    "id": 18,
    "question": "What is the default axis parameter value in methods like df.sum() or df.drop()?",
    "options": [
      "1",
      "0",
      "'rows'",
      "'columns'"
    ],
    "answer": "0",
    "explanation": "Axis 0 refers to the rows (index), meaning operations go downwards (e.g., summing all rows to get a column total). Axis 1 refers to columns.",
    "difficulty": "Beginner"
  },
  {
    "id": 19,
    "question": "Which pandas function combines multiple DataFrame objects by stacking them vertically or horizontally?",
    "options": [
      "pd.merge()",
      "pd.join()",
      "pd.concat()",
      "pd.combine()"
    ],
    "answer": "pd.concat()",
    "explanation": "`pd.concat()` concatenates pandas objects along a particular axis. `merge` and `join` are database-style join operations based on keys/indexes.",
    "difficulty": "Beginner"
  },
  {
    "id": 20,
    "question": "Which method allows you to filter a DataFrame using a string expression, often improving readability?",
    "options": [
      "df.filter()",
      "df.mask()",
      "df.query()",
      "df.where()"
    ],
    "answer": "df.query()",
    "explanation": "`df.query('col > 5')` allows filtering using a string expression. `where()` retains values where the condition is True but replaces others with NaN (or fill value), and `filter` selects by labels.",
    "difficulty": "Beginner"
  },
  {
    "id": 21,
    "question": "Which method is used to rename the labels (columns or index) of a DataFrame?",
    "options": [
      "df.rename()",
      "df.relabel()",
      "df.set_labels()",
      "df.change_header()"
    ],
    "answer": "df.rename()",
    "explanation": "`rename()` takes a dictionary or function to map old names to new names for columns or index labels.",
    "difficulty": "Beginner"
  },
  {
    "id": 22,
    "question": "What is the purpose of the groupby() method?",
    "options": [
      "To sort the DataFrame by a specific column",
      "To split the data into groups based on some criteria",
      "To delete rows based on column values",
      "To merge two DataFrames"
    ],
    "answer": "To split the data into groups based on some criteria",
    "explanation": "The 'split-apply-combine' pattern starts with `groupby`, which separates data into groups for subsequent aggregation or transformation.",
    "difficulty": "Beginner"
  },
  {
    "id": 23,
    "question": "Which data type conversion reduces memory usage by defaulting from float64 to float32?",
    "options": [
      "Downcasting",
      "Upcasting",
      "Type casting",
      "Object casting"
    ],
    "answer": "Downcasting",
    "explanation": "Downcasting involves converting data to a smaller data type (e.g., float64 to float32, int64 to int8) to reduce memory usage, provided the data fits in the smaller range.",
    "difficulty": "Beginner"
  },
  {
    "id": 24,
    "question": "Which operation joins two DataFrames based on a common column, similar to SQL JOINs?",
    "options": [
      "pd.concat()",
      "pd.append()",
      "pd.merge()",
      "pd.stack()"
    ],
    "answer": "pd.merge()",
    "explanation": "`pd.merge()` is a database-style join operation that connects DataFrames on columns or indices. `concat` simply stacks them.",
    "difficulty": "Beginner"
  },
  {
    "id": 25,
    "question": "What does the isnull() method return when applied to a DataFrame?",
    "options": [
      "Rows that have been deleted",
      "A boolean DataFrame indicating missing values",
      "The count of null values",
      "Rows sorted by nullity"
    ],
    "answer": "A boolean DataFrame indicating missing values",
    "explanation": "`isnull()` (or `isna()`) returns a boolean mask of the same shape as the original, where True indicates a missing value (NaN/None).",
    "difficulty": "Beginner"
  },
  {
    "id": 26,
    "question": "Which attribute returns the index (row labels) of a DataFrame?",
    "options": [
      "df.columns",
      "df.rows",
      "df.index",
      "df.axes"
    ],
    "answer": "df.index",
    "explanation": "`df.index` provides the row labels. `df.columns` provides column labels. `df.axes` returns a list containing both the row and column axis.",
    "difficulty": "Beginner"
  },
  {
    "id": 27,
    "question": "How can you set a specific column to be the index of the DataFrame?",
    "options": [
      "df.reindex(column)",
      "df.set_index(column_name)",
      "df.change_index(column_name)",
      "df.index_to(column_name)"
    ],
    "answer": "df.set_index(column_name)",
    "explanation": "`set_index()` sets an existing column as the new index. `reindex` is used to conform a DataFrame to a new index, filling missing data where needed.",
    "difficulty": "Beginner"
  },
  {
    "id": 28,
    "question": "Which method resets the index of a DataFrame to the default integer index (0, 1, 2...)?",
    "options": [
      "df.default_index()",
      "df.reindex()",
      "df.reset_index()",
      "df.clear_index()"
    ],
    "answer": "df.reset_index()",
    "explanation": "`reset_index()` moves the current index into columns and creates a new default integer index. `reindex()` is for conforming data to a new set of labels.",
    "difficulty": "Beginner"
  },
  {
    "id": 29,
    "question": "When using df.drop(), what must you specify to delete a column instead of a row?",
    "options": [
      "columns=True",
      "axis=1",
      "orient='columns'",
      "dim=1"
    ],
    "answer": "axis=1",
    "explanation": "The `axis` parameter dictates the direction; `axis=1` (or 'columns') targets columns. `axis=0` targets rows.",
    "difficulty": "Beginner"
  },
  {
    "id": 30,
    "question": "Which method calculates the statistical correlation between columns in a DataFrame?",
    "options": [
      "df.cov()",
      "df.correlation()",
      "df.corr()",
      "df.relation()"
    ],
    "answer": "df.corr()",
    "explanation": "`df.corr()` computes pairwise correlation of columns, excluding NA/null values. `df.cov()` computes covariance.",
    "difficulty": "Beginner"
  },
  {
    "id": 31,
    "question": "What does the sort_values() method do?",
    "options": [
      "Sorts the DataFrame by the index labels",
      "Sorts the DataFrame by the values in one or more columns",
      "Sorts the column names alphabetically",
      "Sorts the data types of the columns"
    ],
    "answer": "Sorts the DataFrame by the values in one or more columns",
    "explanation": "`sort_values()` sorts by data content (column values). `sort_index()` is used to sort by the row index labels.",
    "difficulty": "Beginner"
  },
  {
    "id": 32,
    "question": "Which method is used to identify unique values in a Series?",
    "options": [
      "df.unique()",
      "df.nunique()",
      "df.value_counts()",
      "df.distinct()"
    ],
    "answer": "df.unique()",
    "explanation": "`unique()` returns the unique values as a NumPy array. `nunique()` returns the count of unique values, and `value_counts()` returns the frequency of each.",
    "difficulty": "Beginner"
  },
  {
    "id": 33,
    "question": "Why is using .apply() generally discouraged in favor of vectorized operations for large datasets?",
    "options": [
      ".apply() is not compatible with lambda functions",
      ".apply() iterates rows in Python, which is slow",
      ".apply() modifies the original DataFrame unintentionally",
      ".apply() only works on numeric data"
    ],
    "answer": ".apply() iterates rows in Python, which is slow",
    "explanation": "`apply()` often loops in Python space, incurring interpreter overhead. Vectorized operations use C-level optimizations, making them significantly faster.",
    "difficulty": "Beginner"
  },
  {
    "id": 34,
    "question": "Which method returns the frequency count of each unique value in a Series?",
    "options": [
      "df.count()",
      "df.mode()",
      "df.value_counts()",
      "df.frequency()"
    ],
    "answer": "df.value_counts()",
    "explanation": "`value_counts()` returns a Series containing counts of unique values. `count()` returns the number of non-NA observations, and `mode()` returns the most frequent value.",
    "difficulty": "Beginner"
  },
  {
    "id": 35,
    "question": "Which property allows you to access a single scalar value using the fastest lookup method?",
    "options": [
      ".at[]",
      ".loc[]",
      ".iloc[]",
      ".get()"
    ],
    "answer": ".at[]",
    "explanation": "`.at[]` provides label-based scalar access and is very fast for looking up a single value. `.loc[]` and `.iloc[]` are designed for slicing and selecting ranges.",
    "difficulty": "Beginner"
  },
  {
    "id": 36,
    "question": "Why are vectorized string operations (accessed via the `.str` accessor) generally preferred over standard Python string methods within a loop?",
    "options": [
      "They automatically convert all strings to lowercase for consistency",
      "They avoid Python loops by executing optimized C-level routines on the entire array",
      "They allow for regex parsing which standard Python methods do not support",
      "They automatically handle missing values by converting them to empty strings"
    ],
    "answer": "They avoid Python loops by executing optimized C-level routines on the entire array",
    "explanation": "Using the `.str` accessor leverages vectorized operations implemented in C, which are significantly faster than iterating through a Series with a Python loop. This reduces interpreter overhead and optimizes memory access patterns.",
    "difficulty": "Intermediate"
  },
  {
    "id": 37,
    "question": "What is the primary memory benefit of converting a column containing repeated string values (e.g., 'US', 'EU', 'ASIA') to the `category` dtype?",
    "options": [
      "It compresses the data by storing strings as integer codes and maintaining a single lookup table of unique values",
      "It converts the strings into their hashed integer representations, allowing for faster mathematical operations",
      "It removes all null values from the column to save space",
      "It stores the data on disk instead of in RAM"
    ],
    "answer": "It compresses the data by storing strings as integer codes and maintaining a single lookup table of unique values",
    "explanation": "The `category` dtype uses a memory-efficient approach where the actual data is stored as integers (codes) pointing to a separate array of unique categories. This drastically reduces memory usage when the cardinality (number of unique values) is low compared to the row count.",
    "difficulty": "Intermediate"
  },
  {
    "id": 38,
    "question": "When using `pd.merge()`, what is the result of `how='outer'` compared to `how='inner'` on two DataFrames with non-overlapping keys?",
    "options": [
      "Outer merge returns only rows where keys match in both; inner merge returns all rows from both",
      "Outer merge returns all rows from both frames, filling NaNs where there are no matches; inner merge returns only rows with matching keys in both",
      "Outer merge returns only keys from the left dataframe; inner merge returns only keys from the right dataframe",
      "Outer merge drops duplicate columns; inner merge keeps all columns"
    ],
    "answer": "Outer merge returns all rows from both frames, filling NaNs where there are no matches; inner merge returns only rows with matching keys in both",
    "explanation": "An outer join preserves all keys from both the left and right DataFrames, introducing `NaN` for missing matches. An inner join acts as an intersection, retaining only the keys found in both DataFrames.",
    "difficulty": "Intermediate"
  },
  {
    "id": 39,
    "question": "What is the specific function of the `pd.cut()` function compared to `pd.qcut()`?",
    "options": [
      "`pd.cut()` bins data into equal-width bins based on value ranges, while `pd.qcut()` bins data into quantiles based on sample quantiles",
      "`pd.cut()` bins data based on quantiles, while `pd.qcut()` bins data based on value ranges",
      "`pd.cut()` is used exclusively for time-series data, while `pd.qcut()` is for numerical data",
      "`pd.cut()` sorts the data, while `pd.qcut()` aggregates it"
    ],
    "answer": "`pd.cut()` bins data into equal-width bins based on value ranges, while `pd.qcut()` bins data into quantiles based on sample quantiles",
    "explanation": "`pd.cut()` divides the data into bins of equal size (range), meaning the distance between bin edges is constant. `pd.qcut()` divides the data such that each bin has approximately the same number of observations (frequency).",
    "difficulty": "Intermediate"
  },
  {
    "id": 40,
    "question": "In the context of a `groupby()` operation, what is the difference between the `transform` and `agg` (aggregation) methods?",
    "options": [
      "`transform` returns a scalar value for each group, while `agg` returns a Series of the same length as the input",
      "`transform` returns a Series/DataFrame of the same shape as the input, while `agg` returns a scalar or Series reduced to one value per group",
      "`transform` is only valid for numerical columns, whereas `agg` works on strings",
      "`transform` creates a MultiIndex, whereas `agg` flattens the index"
    ],
    "answer": "`transform` returns a Series/DataFrame of the same shape as the input, while `agg` returns a scalar or Series reduced to one value per group",
    "explanation": "The `transform` method is designed to broadcast values back to the original index shape, allowing for operations like standardization or filling missing values per group. The `agg` method reduces the data dimensionality, returning one summary row per group.",
    "difficulty": "Intermediate"
  },
  {
    "id": 41,
    "question": "What is the default behavior of pandas when performing an arithmetic operation (e.g., addition) on two DataFrames with different indices and non-matching labels?",
    "options": [
      "It raises a `ValueError` due to index misalignment",
      "It performs the operation based on row position, ignoring the index",
      "It aligns data by index labels and introduces `NaN` for labels that do not overlap",
      "It defaults to an inner join on the columns and an outer join on the rows"
    ],
    "answer": "It aligns data by index labels and introduces `NaN` for labels that do not overlap",
    "explanation": "Pandas follows the principle of data alignment. When performing arithmetic, it automatically aligns indices; if a label exists in one DataFrame but not the other, pandas introduces `NaN` for the missing data in the result.",
    "difficulty": "Intermediate"
  },
  {
    "id": 42,
    "question": "Why might you use the `query()` method instead of standard boolean indexing for filtering large DataFrames?",
    "options": [
      "`query()` automatically converts data types to integers",
      "`query()` is generally more readable and can be faster as it utilizes the `numexpr` engine behind the scenes",
      "Standard boolean indexing does not support logical operators like 'and'/'or'",
      "`query()` is the only way to filter based on multiple columns"
    ],
    "answer": "`query()` is generally more readable and can be faster as it utilizes the `numexpr` engine behind the scenes",
    "explanation": "`query()` allows for a more concise, SQL-like syntax and can optimize the evaluation of complex expressions by offloading them to the `numexpr` library, which is often faster for large boolean masks.",
    "difficulty": "Intermediate"
  },
  {
    "id": 43,
    "question": "What is the consequence of passing `inplace=True` to a DataFrame method?",
    "options": [
      "The method returns a new DataFrame, and the original remains unchanged",
      "The method modifies the original DataFrame directly and returns `None`",
      "The operation is performed immediately without going through the pandas internal pipeline",
      "The original DataFrame is deleted from memory"
    ],
    "answer": "The method modifies the original DataFrame directly and returns `None`",
    "explanation": "When `inplace=True` is used, the operation modifies the object in place and returns `None`. This prevents the accidental assignment of the result (which would be `None`) to a variable overwriting the original data.",
    "difficulty": "Intermediate"
  },
  {
    "id": 44,
    "question": "When reading a very large CSV file using `pd.read_csv()`, which parameter allows you to process the file in chunks to avoid running out of RAM?",
    "options": [
      "`memory_map=True`",
      "`chunksize`",
      "`nrows`",
      "`iterator=True`"
    ],
    "answer": "`chunksize`",
    "explanation": "Setting `chunksize` causes `read_csv` to return a `TextFileReader` object that yields an iterator of DataFrames. This allows you to process a specific number of rows at a time, keeping memory usage stable.",
    "difficulty": "Intermediate"
  },
  {
    "id": 45,
    "question": "What is the technical reason behind the `SettingWithCopyWarning` in pandas?",
    "options": [
      "The user is trying to modify a view of a DataFrame returned by another operation, potentially affecting the parent DataFrame unintentionally",
      "The DataFrame contains immutable data types that cannot be modified",
      "The operation being performed is deprecated and will be removed in a future version",
      "The index of the DataFrame is not unique"
    ],
    "answer": "The user is trying to modify a view of a DataFrame returned by another operation, potentially affecting the parent DataFrame unintentionally",
    "explanation": "The warning appears when chained indexing (e.g., `df[df['A'] > 0]['B'] = 0`) creates a temporary copy or view, and pandas cannot guarantee if the modification will apply to the original or the copy. It signals ambiguous assignment logic.",
    "difficulty": "Intermediate"
  },
  {
    "id": 46,
    "question": "What does the `axis` parameter specifically control in the context of `df.apply(func)`?",
    "options": [
      "The data type of the returned value",
      "Whether the function is applied to each column (`axis=0`) or each row (`axis=1`)",
      "The index of the DataFrame",
      "The number of parallel threads to use"
    ],
    "answer": "Whether the function is applied to each column (`axis=0`) or each row (`axis=1`)",
    "explanation": "In pandas, `axis=0` refers to the index (vertical), meaning the function is applied column-wise. `axis=1` refers to the columns (horizontal), meaning the function is applied row-wise.",
    "difficulty": "Intermediate"
  },
  {
    "id": 47,
    "question": "How does the `memory_usage(deep=True)` method differ from `memory_usage(deep=False)`?",
    "options": [
      "`deep=True` includes the memory usage of the object types, specifically calculating the full memory footprint of strings",
      "`deep=True` scans the system RAM to find duplicates",
      "`deep=False` compresses the DataFrame before calculation",
      "`deep=True` returns the memory usage in Megabytes, whereas `deep=False` uses Bytes"
    ],
    "answer": "`deep=True` includes the memory usage of the object types, specifically calculating the full memory footprint of strings",
    "explanation": "By default (`deep=False`), pandas reports the memory used by the object references (pointers). `deep=True` introspects the actual objects (like strings) contained in `object` dtype columns to provide the total memory consumption.",
    "difficulty": "Intermediate"
  },
  {
    "id": 48,
    "question": "What is the purpose of the `stack()` method on a MultiIndex DataFrame?",
    "options": [
      "It converts column labels into index labels, creating a 'long' format Series/DataFrame",
      "It converts a long format DataFrame into a wide format",
      "It removes the innermost level of the index",
      "It sorts the DataFrame by the index values"
    ],
    "answer": "It converts column labels into index labels, creating a 'long' format Series/DataFrame",
    "explanation": "The `stack()` method pivots the columns of a DataFrame (or the innermost column level of a MultiIndex DataFrame) into the index, effectively turning the wide table into a long format (Series with MultiIndex).",
    "difficulty": "Intermediate"
  },
  {
    "id": 49,
    "question": "What is the difference between `pd.concat([df1, df2])` and `df1.merge(df2)`?",
    "options": [
      "`concat` stacks data along an axis (index or columns) based on position or keys, while `merge` combines data based on column values (SQL-like joins)",
      "`concat` is used for inner joins only, while `merge` handles outer joins",
      "`concat` works only on Series, while `merge` works only on DataFrames",
      "There is no difference; they are aliases for the same function"
    ],
    "answer": "`concat` stacks data along an axis (index or columns) based on position or keys, while `merge` combines data based on column values (SQL-like joins)",
    "explanation": "`concat` is used to glue pandas objects together along a specific axis (appending rows or columns). `merge` is used for database-style join operations that link data based on common columns or indices.",
    "difficulty": "Intermediate"
  },
  {
    "id": 50,
    "question": "When working with time-series data, what does `asfreq('D')` do compared to `resample('D')`?",
    "options": [
      "`asfreq` converts the data to a daily frequency without changing the index, while `resample` aggregates data (requires an aggregation function)",
      "`asfreq` changes the frequency and fills NaN with interpolation, while `resample` drops rows",
      "`resample` changes the index frequency, while `asfreq` calculates the moving average",
      "`asfreq` is only used for upsampling, while `resample` is for downsampling"
    ],
    "answer": "`asfreq` converts the data to a daily frequency without changing the index, while `resample` aggregates data (requires an aggregation function)",
    "explanation": "`asfreq()` strictly converts the index to the specified frequency, generating a new index with NaNs for missing dates (unless a `fill_method` is specified). `resample()` creates groups of time intervals to which an aggregation function (like mean, sum) must be applied.",
    "difficulty": "Intermediate"
  },
  {
    "id": 51,
    "question": "Which operation is likely to be the most computationally expensive and slowest on a large DataFrame?",
    "options": [
      "Vectorized addition of two columns",
      "Iterating over rows using `df.iterrows()`",
      "Filtering rows using boolean indexing",
      "Calculating the correlation matrix"
    ],
    "answer": "Iterating over rows using `df.iterrows()`",
    "explanation": "`iterrows()` yields Pandas Series objects for each row, incurring significant overhead from dtype construction and index lookups. Vectorized operations are implemented in C and are orders of magnitude faster.",
    "difficulty": "Intermediate"
  },
  {
    "id": 52,
    "question": "What is the primary use case for `df.get_dummies()`?",
    "options": [
      "Filling missing values with dummy data",
      "Converting categorical variables into binary (0 or 1) indicator columns",
      "Creating dummy indices for testing",
      "Extracting time-based features"
    ],
    "answer": "Converting categorical variables into binary (0 or 1) indicator columns",
    "explanation": "`get_dummies()` performs One-Hot Encoding. It transforms a categorical column into multiple columns of 0s and 1s, where a new column is created for each unique category value.",
    "difficulty": "Intermediate"
  },
  {
    "id": 53,
    "question": "How does `df.dropna()` behave by default regarding the `axis` and `how` parameters?",
    "options": [
      "It drops columns if any value is missing (`axis=1`, `how='any'`)",
      "It drops rows if all values are missing (`axis=0`, `how='all'`)",
      "It drops rows if any value is missing (`axis=0`, `how='any'`)",
      "It drops columns if all values are missing (`axis=1`, `how='any'`)"
    ],
    "answer": "It drops rows if any value is missing (`axis=0`, `how='any'`)",
    "explanation": "The default behavior is `axis=0` (drop rows) and `how='any'` (drop the row if at least one value is missing). This removes any row containing `NaN` values.",
    "difficulty": "Intermediate"
  },
  {
    "id": 54,
    "question": "What is the function of the `pd.to_numeric()` function when `errors='coerce'` is passed?",
    "options": [
      "It forces all non-numeric columns to be deleted",
      "It converts the argument to a numeric type, converting invalid parsing to `NaN`",
      "It raises an error if the column contains strings",
      "It rounds all floating-point numbers to the nearest integer"
    ],
    "answer": "It converts the argument to a numeric type, converting invalid parsing to `NaN`",
    "explanation": "When `errors='coerce'` is used, pandas attempts to convert the input to numeric types. If a value cannot be parsed (e.g., a string 'hello'), it converts that specific value to `NaN` instead of raising an exception.",
    "difficulty": "Intermediate"
  },
  {
    "id": 55,
    "question": "In the context of MultiIndexing, what is the difference between `df.loc[('A', 'x')]` and `df.xs(('A', 'x'))`?",
    "options": [
      "They are identical, but `xs` is the preferred modern syntax",
      "`loc` requires the full tuple for a cross-section, while `xs` allows selecting a specific level without specifying the others if `level` is set",
      "`xs` can only be used on the index level, whereas `loc` can be used on columns",
      "`xs` returns a copy, while `loc` returns a view"
    ],
    "answer": "`loc` requires the full tuple for a cross-section, while `xs` allows selecting a specific level without specifying the others if `level` is set",
    "explanation": "While both can select data, `xs` (cross-section) is designed to simplify selection by allowing you to specify a `level` argument to slice a MultiIndex without explicitly passing keys for higher levels.",
    "difficulty": "Intermediate"
  },
  {
    "id": 56,
    "question": "What does the `method='ffill'` argument do in `df.reindex()`?",
    "options": [
      "It forward-fills missing values by propagating the last valid observation forward to meet the new index requirements",
      "It fills missing values with the mean of the column",
      "It drops any rows in the new index that do not exist in the old DataFrame",
      "It raises an error if the index is not sorted"
    ],
    "answer": "It forward-fills missing values by propagating the last valid observation forward to meet the new index requirements",
    "explanation": "When reindexing (conforming data to a new set of labels), `NaN` may appear. `method='ffill'` (forward fill) handles these gaps by carrying forward the previous valid value.",
    "difficulty": "Intermediate"
  },
  {
    "id": 57,
    "question": "Why is `df.itertuples()` generally faster and more memory-efficient than `df.iterrows()`?",
    "options": [
      "`itertuples` returns the data as standard Python tuples, which do not carry the overhead of pandas Series metadata",
      "`itertuples` uses multi-threading to process rows in parallel",
      "`iterrows` creates a deep copy of the DataFrame for every iteration",
      "`itertuples` skips the index and only iterates over values"
    ],
    "answer": "`itertuples` returns the data as standard Python tuples, which do not carry the overhead of pandas Series metadata",
    "explanation": "`itertuples()` yields namedtuples representing the row, which are lightweight Python objects. `iterrows()` constructs a full Series for every row, which involves heavy dtype and index overhead.",
    "difficulty": "Intermediate"
  },
  {
    "id": 58,
    "question": "What is the effect of setting `dtype_backend='pyarrow'` (using Pandas 2.0+ with PyArrow installed)?",
    "options": [
      "It enables parallel processing for all groupby operations",
      "It uses PyArrow backed data types which are generally more memory efficient and faster for string operations",
      "It converts all data types to 64-bit floating point numbers",
      "It disables the Global Interpreter Lock (GIL)"
    ],
    "answer": "It uses PyArrow backed data types which are generally more memory efficient and faster for string operations",
    "explanation": "PyArrow-backed types offer distinct advantages, particularly for strings (reducing memory overhead significantly) and null handling, while maintaining high performance for data manipulation.",
    "difficulty": "Intermediate"
  },
  {
    "id": 59,
    "question": "When using `df.rolling(window=3)`, what is the 'center' argument used for?",
    "options": [
      "To shift the rolling window calculation to the center of the window, ensuring symmetry",
      "To center the data values by subtracting the mean",
      "To set the window size to the middle of the DataFrame",
      "To calculate the median instead of the mean"
    ],
    "answer": "To shift the rolling window calculation to the center of the window, ensuring symmetry",
    "explanation": "By default, rolling windows are right-aligned (the label is at the right edge). Setting `center=True` places the label at the center of the window, so the calculation uses previous and subsequent values relative to the index.",
    "difficulty": "Intermediate"
  },
  {
    "id": 60,
    "question": "What is the `na_values` parameter in `pd.read_csv()` used for?",
    "options": [
      "To fill empty columns with the mean of the row",
      "To specify additional strings (beyond defaults) that should be recognized as missing values (NaN)",
      "To automatically remove rows containing NaN",
      "To convert NaN values to the string 'NA'"
    ],
    "answer": "To specify additional strings (beyond defaults) that should be recognized as missing values (NaN)",
    "explanation": "Different datasets use different tokens for missing data (e.g., 'N/A', 'null', '-'). The `na_values` parameter allows you to define a list of strings that pandas should convert to `NaN` upon reading the file.",
    "difficulty": "Intermediate"
  },
  {
    "id": 61,
    "question": "What is the result of `df.groupby('col').size()` compared to `df.groupby('col').count()`?",
    "options": [
      "`size` returns the count of non-null values, while `count` returns the total number of rows including NaN",
      "`size` returns the total number of rows (including NaN), while `count` returns the count of non-null values",
      "`size` aggregates by summing the values, while `count` averages them",
      "`size` only works on numeric columns"
    ],
    "answer": "`size` returns the total number of rows (including NaN), while `count` returns the count of non-null values",
    "explanation": "The `size` method simply counts the number of rows in each group, regardless of content. The `count` method excludes `NaN` values from the count for each column.",
    "difficulty": "Intermediate"
  },
  {
    "id": 62,
    "question": "What is the `pd.eval()` function primarily designed to optimize?",
    "options": [
      "The evaluation of complex string expressions involving large DataFrame operations by reducing intermediate object allocation",
      "The parsing of JSON strings into DataFrames",
      "The evaluation of Python code stored in a CSV column",
      "The type conversion of object columns to integers"
    ],
    "answer": "The evaluation of complex string expressions involving large DataFrame operations by reducing intermediate object allocation",
    "explanation": "`eval` parses an expression string and performs the operation efficiently, often using `numexpr` behind the scenes. This avoids the creation of temporary intermediate DataFrames that would normally occur during standard chained operations.",
    "difficulty": "Intermediate"
  },
  {
    "id": 63,
    "question": "How does `df.explode('col')` transform a DataFrame?",
    "options": [
      "It splits the string in the column into multiple columns",
      "It flattens list-like elements in the specified column, replicating the index values for each list item",
      "It removes outliers from the numerical column",
      "It sorts the DataFrame by the specified column"
    ],
    "answer": "It flattens list-like elements in the specified column, replicating the index values for each list item",
    "explanation": "If a column contains lists or tuples, `explode` transforms each element in the list into a separate row. It copies the index (and other columns) to align with these new rows.",
    "difficulty": "Intermediate"
  },
  {
    "id": 64,
    "question": "What is the function of `df.melt()`?",
    "options": [
      "It converts a wide-format DataFrame into a long-format DataFrame by unpivoting specified columns",
      "It converts a long-format DataFrame into a wide-format DataFrame",
      "It drops rows with missing values in a specific column",
      "It creates a pivot table summary"
    ],
    "answer": "It converts a wide-format DataFrame into a long-format DataFrame by unpivoting specified columns",
    "explanation": "`melt` is used to reshape data from a wide format (many columns) to a long format (identifier variables and variable-value pairs), which is often required for visualization or statistical modeling.",
    "difficulty": "Intermediate"
  },
  {
    "id": 65,
    "question": "What does the `sort_index()` parameter `na_position='last'` do?",
    "options": [
      "It deletes missing values before sorting",
      "It places missing index values at the end of the sorted result instead of the beginning",
      "It interpolates missing values based on the sorted index",
      "It keeps missing values in their original position"
    ],
    "answer": "It places missing index values at the end of the sorted result instead of the beginning",
    "explanation": "By default, pandas sorts missing values (`NaN`) at the beginning (first) of the index. Setting `na_position='last'` moves these entries to the end of the sorted DataFrame.",
    "difficulty": "Intermediate"
  },
  {
    "id": 66,
    "question": "In a MultiIndex DataFrame, what is the purpose of `df.swaplevel()`?",
    "options": [
      "To delete a level from the index",
      "To swap the order of two specific levels in the index (e.g., from (Outer, Inner) to (Inner, Outer))",
      "To sort the index by a specific level",
      "To convert the index to columns"
    ],
    "answer": "To swap the order of two specific levels in the index (e.g., from (Outer, Inner) to (Inner, Outer))",
    "explanation": "This function reorders the levels of the MultiIndex. It does not change the data itself, only the hierarchy and arrangement of the index labels, which affects how the data is accessed or sorted.",
    "difficulty": "Intermediate"
  },
  {
    "id": 67,
    "question": "What is the behavior of `df.to_numeric(..., downcast='integer')`?",
    "options": [
      "It rounds the numbers to the nearest whole number",
      "It attempts to cast the data to the smallest possible integer subtype (e.g., int8, int16) that can hold the values",
      "It converts the column to the generic object type",
      "It converts the data to unsigned integers only"
    ],
    "answer": "It attempts to cast the data to the smallest possible integer subtype (e.g., int8, int16) that can hold the values",
    "explanation": "The `downcast` parameter tries to find the smallest dtype that can represent the data. 'integer' attempts to use signed integer types (int8, int16, int32, int64) based on the minimum and maximum values present.",
    "difficulty": "Intermediate"
  },
  {
    "id": 68,
    "question": "What does the `sample()` function do?",
    "options": [
      "It splits the DataFrame into training and testing sets",
      "It returns a random sample of items from an axis of the object, with or without replacement",
      "It aggregates data by taking the first value of each group",
      "It smooths the time-series data using a moving average"
    ],
    "answer": "It returns a random sample of items from an axis of the object, with or without replacement",
    "explanation": "`df.sample()` is used for random sampling. You can specify `frac` (fraction of axis) or `n` (number of items), and `replace=True` to allow the same row to be selected multiple times.",
    "difficulty": "Intermediate"
  },
  {
    "id": 69,
    "question": "What is `df.where()` used for?",
    "options": [
      "To filter rows similar to SQL's WHERE clause, returning only matching rows",
      "To replace values where the condition is False, while keeping values where the condition is True",
      "To locate the index of the first occurrence of a value",
      "To query the database for the DataFrame schema"
    ],
    "answer": "To replace values where the condition is False, while keeping values where the condition is True",
    "explanation": "Unlike boolean indexing which filters out data, `where` preserves the shape of the DataFrame. Values where the condition is False are replaced with `NaN` (or a value specified by the `other` parameter), while True values remain unchanged.",
    "difficulty": "Intermediate"
  },
  {
    "id": 70,
    "question": "What is the significance of the `copy=False` argument in methods like `df.astype()` or `df.replace()`?",
    "options": [
      "It deletes the original DataFrame to save memory",
      "It returns the original DataFrame unchanged to enforce immutability",
      "It modifies the data in place where possible to avoid copying data and save memory",
      "It creates a shallow copy of the DataFrame"
    ],
    "answer": "It modifies the data in place where possible to avoid copying data and save memory",
    "explanation": "Setting `copy=False` suggests to pandas that it should modify the object in place if the dtype is compatible, rather than creating a new object. This is a performance optimization, though pandas may ignore it if the copy is unavoidable.",
    "difficulty": "Intermediate"
  },
  {
    "id": 71,
    "question": "Which memory optimization technique is most effective for a column containing a low cardinality string variable (e.g., 'US', 'UK', 'EU') repeated over 1 million rows?",
    "options": [
      "Converting the column to 'object' dtype and applying run-length encoding",
      "Converting the column to 'category' dtype to store data as integers and a lookup table",
      "Downcasting the column to 'int8' after mapping strings to numeric codes manually",
      "Using `SparseDtype` to replace zeros with a sparse representation"
    ],
    "answer": "Converting the column to 'category' dtype to store data as integers and a lookup table",
    "explanation": "The 'category' dtype stores unique values once and uses integer mapping for the rest, drastically reducing memory usage for repetitive string data compared to the default 'object' dtype which stores full strings.",
    "difficulty": "Advanced"
  },
  {
    "id": 72,
    "question": "In the context of `pandas.eval()`, what is the primary benefit of using the `engine='numexpr'` argument for large DataFrames?",
    "options": [
      "It allows for the execution of arbitrary Python code strings within the expression",
      "It utilizes memory optimization and multi-threading to compute arithmetic operations faster than standard Python",
      "It automatically checks for NaN values and handles them without raising warnings",
      "It converts the DataFrame operations into SQL queries for database execution"
    ],
    "answer": "It utilizes memory optimization and multi-threading to compute arithmetic operations faster than standard Python",
    "explanation": "The 'numexpr' engine optimizes arithmetic operations by reducing memory usage and utilizing CPU cache and multi-threading, bypassing Python's interpreter overhead for large array operations.",
    "difficulty": "Advanced"
  },
  {
    "id": 73,
    "question": "What is the specific purpose of the `pd.Grouper` function when used inside a `groupby` operation?",
    "options": [
      "To specify a key function to apply to the index before grouping",
      "To aggregate data based on a specific time frequency (resampling) while grouping by other columns",
      "To create a hierarchical index based on the unique values of the specified column",
      "To filter out groups that do not meet a specified variance threshold"
    ],
    "answer": "To aggregate data based on a specific time frequency (resampling) while grouping by other columns",
    "explanation": "`pd.Grouper(freq='...')` enables time-based grouping (resampling) within a `groupby`, often used in conjunction with other column grouping to create time-series aggregations.",
    "difficulty": "Advanced"
  },
  {
    "id": 74,
    "question": "When using `df.merge()` with `how='outer'`, what distinguishes the `indicator` parameter when set to `True`?",
    "options": [
      "It adds a column '_merge' indicating the source of each row (left_only, right_only, or both)",
      "It displays a progress bar indicating the merge status",
      "It optimizes memory usage by dropping columns not present in both DataFrames",
      "It creates a MultiIndex on the merged dataframe to preserve the original indices"
    ],
    "answer": "It adds a column '_merge' indicating the source of each row (left_only, right_only, or both)",
    "explanation": "The `indicator=True` parameter appends a column named `_merge` that takes categorical values ('left_only', 'right_only', 'both') to show which DataFrame(s) the row originated from.",
    "difficulty": "Advanced"
  },
  {
    "id": 75,
    "question": "How does the `transform()` method differ from `apply()` when used with a GroupBy object?",
    "options": [
      "`transform` returns a DataFrame aggregated by groups, while `apply` returns a scalar",
      "`transform` must return a result that is the same shape as the input group chunk, broadcasting the result back to the original index",
      "`apply` allows vectorized operations while `transform` requires an explicit loop",
      "`transform` is significantly slower and deprecated in favor of `apply`"
    ],
    "answer": "`transform` must return a result that is the same shape as the input group chunk, broadcasting the result back to the original index",
    "explanation": "Unlike `apply` which can return arbitrary shapes (often scalar or reduced), `transform` enforces that the function returns a Series aligned with the input group, allowing the result to be merged back into the original DataFrame structure.",
    "difficulty": "Advanced"
  },
  {
    "id": 76,
    "question": "What is the primary difference between `df.stack()` and `df.melt()`?",
    "options": [
      "`stack` converts wide format to long format using column names, while `melt` pivots the index into columns",
      "`stack` pivots columns into the index (creating a MultiIndex), while `melt` unpivots a DataFrame from wide to long format based on identifier variables",
      "`stack` is used for aggregation, while `melt` is used for sorting",
      "`stack` works only on numeric columns, while `melt` works only on string columns"
    ],
    "answer": "`stack` pivots columns into the index (creating a MultiIndex), while `melt` unpivots a DataFrame from wide to long format based on identifier variables",
    "explanation": "`stack()` compresses a level of column labels into the index (hierarchical), whereas `melt()` converts a wide DataFrame into a long format by specifying ID and value variables, often resulting in row repetition.",
    "difficulty": "Advanced"
  },
  {
    "id": 77,
    "question": "Which parameter in `pd.read_csv()` allows you to parse specific columns as dates while also combining multiple columns into a single DateTime index?",
    "options": [
      "`parse_dates` with a list of lists (e.g., `[['year', 'month', 'day']]`)",
      "`index_col` set to the string 'datetime'",
      "`date_parser` applied to the `usecols` argument",
      "`infer_datetime_format` set to True alongside `usecols`"
    ],
    "answer": "`parse_dates` with a list of lists (e.g., `[['year', 'month', 'day']]`)",
    "explanation": "Passing a nested list (e.g., `[['col1', 'col2']]`) to `parse_dates` instructs pandas to concatenate those columns into a single DateTime column, effectively creating a composite date key.",
    "difficulty": "Advanced"
  },
  {
    "id": 78,
    "question": "In the context of `df.pivot_table()`, what does the argument `margins=True` calculate?",
    "options": [
      "The standard error of the mean for all aggregate functions",
      "Subtotals (row and column 'All') for each group and the grand total",
      "The padding required to align the table to the screen width",
      "The difference between the maximum and minimum values in each bin"
    ],
    "answer": "Subtotals (row and column 'All') for each group and the grand total",
    "explanation": "The `margins=True` parameter adds extra rows and columns labeled 'All' that display the aggregate of the entire dataset across the specified dimensions.",
    "difficulty": "Advanced"
  },
  {
    "id": 79,
    "question": "When dealing with MultiIndex sorting, what is the specific function of `level` in `df.sort_index(level=...)`?",
    "options": [
      "It specifies the depth of recursion for the sort algorithm",
      "It indicates which levels of the index should be sorted, allowing for partial sorting of a MultiIndex",
      "It flattens the MultiIndex into a single level before sorting",
      "It determines the number of CPU cores to use for the sorting operation"
    ],
    "answer": "It indicates which levels of the index should be sorted, allowing for partial sorting of a MultiIndex",
    "explanation": "The `level` argument allows you to target specific levels of the MultiIndex for sorting without requiring the entire index hierarchy to be sorted, enabling granular control.",
    "difficulty": "Advanced"
  },
  {
    "id": 80,
    "question": "Which method is best suited to convert a DataFrame with MultiIndex columns into a standard flat index by concatenating the levels?",
    "options": [
      "`df.reset_index()` with `drop=True`",
      "`df.stack().unstack()`",
      "`df.to_flat()`",
      "`df.columns = df.columns.map('_'.join)`"
    ],
    "answer": "`df.columns = df.columns.map('_'.join)`",
    "explanation": "While not a built-in method like 'flatten', accessing `df.columns` and mapping a join function over the MultiIndex tuples is the standard vectorized way to flatten multi-level column names into single strings.",
    "difficulty": "Advanced"
  },
  {
    "id": 81,
    "question": "What is the behavior of the `as_index=False` parameter in `df.groupby()`?",
    "options": [
      "It prevents the groupby keys from being set as the index of the resulting aggregation, returning a standard range index",
      "It disables index-based lookup performance optimizations",
      "It forces the resulting DataFrame to be sorted by the group keys",
      "It drops the group keys entirely from the aggregated DataFrame"
    ],
    "answer": "It prevents the groupby keys from being set as the index of the resulting aggregation, returning a standard range index",
    "explanation": "By default, `groupby` uses group keys as the index. `as_index=False` overrides this to return a DataFrame with a default numeric index and the group keys as regular columns.",
    "difficulty": "Advanced"
  },
  {
    "id": 82,
    "question": "How does `df.query('column > @variable')` interact with the environment compared to standard boolean indexing?",
    "options": [
      "It is slower because it compiles SQL code",
      "It uses the `numexpr` engine to filter large datasets faster than standard Python boolean indexing",
      "It requires the column to be indexed beforehand",
      "It automatically handles missing values by dropping them"
    ],
    "answer": "It uses the `numexpr` engine to filter large datasets faster than standard Python boolean indexing",
    "explanation": "`query()` is an alternative to standard indexing that offloads expression evaluation to `numexpr` (if installed), reducing memory overhead and using optimized C loops for speed.",
    "difficulty": "Advanced"
  },
  {
    "id": 83,
    "question": "What is the specific role of the `pd.to_numeric()` function's `errors='coerce'` parameter?",
    "options": [
      "It raises an exception if any string cannot be converted to a number",
      "It converts invalid parsing values to NaN and forces valid numeric strings to float/int",
      "It ignores rows with conversion errors and returns the rest unchanged",
      "It automatically converts integers to floats to prevent overflow"
    ],
    "answer": "It converts invalid parsing values to NaN and forces valid numeric strings to float/int",
    "explanation": "While `errors='raise'` is default, `errors='coerce'` ensures that conversion failures result in `NaN` (Not a Number) rather than stopping the operation, allowing for clean numeric data.",
    "difficulty": "Advanced"
  },
  {
    "id": 84,
    "question": "What differentiates `df.replace()` from `df.fillna()` when handling missing data?",
    "options": [
      "`replace` can target specific values or regex patterns, while `fillna` strictly targets NaN/None values",
      "`fillna` works on strings, while `replace` only works on numeric types",
      "`replace` is faster because it uses in-place modification by default",
      "`fillna` can interpolate data, but `replace` cannot"
    ],
    "answer": "`replace` can target specific values or regex patterns, while `fillna` strictly targets NaN/None values",
    "explanation": "`fillna` is specialized for `np.nan`/`None` handling (forward/backward fill), whereas `replace` is a general value substitution tool capable of dictionary mappings and regex replacements.",
    "difficulty": "Advanced"
  },
  {
    "id": 85,
    "question": "In the context of Time Series, what is the primary effect of `df.asfreq('D')` on a DateTimeIndex DataFrame?",
    "options": [
      "It resamples the data by averaging values to a daily frequency",
      "It changes the frequency of the index to daily and fills missing new dates with NaN",
      "It groups the data by day and applies a sum aggregation",
      "It filters the dataframe to show only rows that are exactly 24 hours apart"
    ],
    "answer": "It changes the frequency of the index to daily and fills missing new dates with NaN",
    "explanation": "`asfreq()` converts the TimeSeries to a specified frequency (e.g., 'D' for Daily). Unlike `resample()`, it does not perform aggregation; it simply conforms the index to the frequency, inserting NaN where data is missing.",
    "difficulty": "Advanced"
  },
  {
    "id": 86,
    "question": "What is the purpose of `df.explode('column')`?",
    "options": [
      "It expands a list-like column (containing lists, tuples, or Series) into separate rows, duplicating the index",
      "It creates a box plot for the specified numeric column",
      "It expands categorical strings into dummy variables (one-hot encoding)",
      "It calculates the variance of a time-series column"
    ],
    "answer": "It expands a list-like column (containing lists, tuples, or Series) into separate rows, duplicating the index",
    "explanation": "The `explode` method transforms each element in a list-like column into a separate row, repeating the values of other columns, which is essential for normalizing nested list data.",
    "difficulty": "Advanced"
  },
  {
    "id": 87,
    "question": "What characterizes `df.rank()` with `method='dense'`?",
    "options": [
      "Ranks are assigned based on insertion order, leaving gaps in the ranking sequence",
      "Tied values get the same rank, and the next rank is incremented by 1 (no gaps)",
      "All tied values receive the maximum rank in the group",
      "It ranks purely based on the index location, not the value"
    ],
    "answer": "Tied values get the same rank, and the next rank is incremented by 1 (no gaps)",
    "explanation": "The 'dense' method assigns ranks such that ties get the same rank, but unlike 'min' or 'max', the next rank increases strictly by 1, ensuring no gaps in the ranking numbers.",
    "difficulty": "Advanced"
  },
  {
    "id": 88,
    "question": "Which `merge` parameter ensures that the merge keys are unique in the right DataFrame, raising an error otherwise?",
    "options": [
      "`how='right'`",
      "`validate='one_to_one'`",
      "`on='key'`",
      "`indicator=True`"
    ],
    "answer": "`validate='one_to_one'`",
    "explanation": "The `validate` parameter checks key uniqueness. 'one_to_one' ensures a 1:1 match in keys, whereas 'one_to_many' or 'many_to_one' validates specific cardinality constraints, helping prevent unintended duplication.",
    "difficulty": "Advanced"
  },
  {
    "id": 89,
    "question": "When using `pd.cut()` to bin continuous data, what does the `right=False` parameter do?",
    "options": [
      "It excludes the right edge of the bin, making the interval `[a, b)` instead of `(a, b]`",
      "It aligns the bins to the right side of the x-axis in a plot",
      "It removes the rightmost bin entirely",
      "It sorts the resulting categories in descending order"
    ],
    "answer": "It excludes the right edge of the bin, making the interval `[a, b)` instead of `(a, b]`",
    "explanation": "By default, bins are half-open intervals `(a, b]`. Setting `right=False` reverses this to `[a, b)`, meaning the left side is inclusive and the right side is exclusive.",
    "difficulty": "Advanced"
  },
  {
    "id": 90,
    "question": "What is the utility of the `df.where(cond, other)` method compared to boolean indexing `df[mask]`?",
    "options": [
      "`where` preserves the shape of the DataFrame and replaces values where the condition is False with `other` (or NaN)",
      "`where` drops rows that do not meet the condition, effectively filtering the DataFrame",
      "`where` evaluates the condition row-by-row in Python, making it slower",
      "`where` can only be applied to Series objects, not DataFrames"
    ],
    "answer": "`where` preserves the shape of the DataFrame and replaces values where the condition is False with `other` (or NaN)",
    "explanation": "Unlike boolean indexing which filters (selects) data, `where` retains the full DataFrame shape but masks/updates values that do not meet the condition, similar to `numpy.where`.",
    "difficulty": "Advanced"
  },
  {
    "id": 91,
    "question": "What does the `observed=False` parameter in `groupby` do when grouping by a Categorical column?",
    "options": [
      "It returns unobserved categories in the result with a count of 0",
      "It drops categories that are not present in the data (performing a 'observed' groupby)",
      "It calculates statistics only on the first occurrence of each category",
      "It converts the categorical type back to object before grouping"
    ],
    "answer": "It returns unobserved categories in the result with a count of 0",
    "explanation": "When `observed=False` (default prior to some versions), the groupby result includes all possible category levels, even those not appearing in the data, often filled with NaN or 0.",
    "difficulty": "Advanced"
  },
  {
    "id": 92,
    "question": "Which operation creates a boolean Series indicating if each element is found in the passed sequence?",
    "options": [
      "`df.isin(values)`",
      "`df.contains(values)`",
      "`df.query(values)`",
      "`df.match(values)`"
    ],
    "answer": "`df.isin(values)`",
    "explanation": "`isin()` is the vectorized method to filter DataFrames by checking if element values are contained in a specific iterable (list, set, Series), returning a boolean mask.",
    "difficulty": "Advanced"
  },
  {
    "id": 93,
    "question": "What is the correct way to chain method calls while preserving a reference to the temporary object in assignment?",
    "options": [
      "Using `df.assign(new_col = df['col'] + 1).dropna()`",
      "Using `df.pipe(lambda x: x.assign(new_col=x['col']+1)).dropna()`",
      "Using `df.apply(lambda x: x['col'] + 1).dropna()`",
      "Using `df.eval('new_col = col + 1').dropna()`"
    ],
    "answer": "Using `df.assign(new_col = df['col'] + 1).dropna()`",
    "explanation": "`assign` returns a new object with the new column added, allowing subsequent method calls (like `dropna`) to be chained safely. This is preferred over direct assignment for method chaining workflows.",
    "difficulty": "Advanced"
  },
  {
    "id": 94,
    "question": "How does `df.corr()` handle non-numeric data by default?",
    "options": [
      "It converts categorical data to strings and calculates distance correlation",
      "It automatically calculates Pearson correlation only for numeric columns, ignoring non-numeric",
      "It raises a `TypeError` unless `numeric_only=True` is specified in recent versions",
      "It performs One-Hot Encoding on all columns before calculating correlation"
    ],
    "answer": "It automatically calculates Pearson correlation only for numeric columns, ignoring non-numeric",
    "explanation": "Pandas intelligently computes correlation on numeric matrices, silently excluding columns of dtype object or category without requiring manual filtering (though newer versions warn and suggest `numeric_only`).",
    "difficulty": "Advanced"
  },
  {
    "id": 95,
    "question": "What is the primary function of `df.iloc` compared to `df.loc`?",
    "options": [
      "`iloc` selects rows and columns by integer location or position, while `loc` selects by label",
      "`iloc` is faster because it does not use the index, while `loc` uses the index exclusively",
      "`iloc` can accept boolean arrays, but `loc` cannot",
      "`iloc` is used for label-based slicing, while `loc` is for integer-based slicing"
    ],
    "answer": "`iloc` selects rows and columns by integer location or position, while `loc` selects by label",
    "explanation": "The fundamental distinction is that `iloc` is integer-position based (0 to len-1), whereas `loc` is label-based, utilizing the actual index values or column names.",
    "difficulty": "Advanced"
  },
  {
    "id": 96,
    "question": "When iterating over a DataFrame with `itertuples()`, what data structure is yielded for each row?",
    "options": [
      "A standard Python tuple",
      "A pandas Series",
      "A `namedtuple` object (or `pandas.NamedTuple`) allowing attribute access",
      "A dictionary of column-value pairs"
    ],
    "answer": "A `namedtuple` object (or `pandas.NamedTuple`) allowing attribute access",
    "explanation": "`itertuples()` yields namedtuples (e.g., `Pandas(Index=1, colA='val')`), which is significantly faster and more memory-efficient than `iterrows()`, which constructs Series objects.",
    "difficulty": "Advanced"
  },
  {
    "id": 97,
    "question": "What is the function of `df.reindex()` compared to `df.set_index()`?",
    "options": [
      "`reindex` conforms a DataFrame to a new index, introducing NaN for missing labels, while `set_index` uses an existing column as the index",
      "`reindex` resets the index to a default range, while `set_index` sorts the DataFrame",
      "`reindex` changes the column labels, while `set_index` changes the row labels",
      "`reindex` modifies the data in place, while `set_index` creates a copy"
    ],
    "answer": "`reindex` conforms a DataFrame to a new index, introducing NaN for missing labels, while `set_index` uses an existing column as the index",
    "explanation": "`set_index` moves data into the index structure. `reindex` alters the alignment to a completely new set of labels (often used for time-series alignment), filling gaps with NaN.",
    "difficulty": "Advanced"
  },
  {
    "id": 98,
    "question": "Which `pd.merge` suffix argument is used to customize the default suffixes (`_x`, `_y`) added to overlapping column names?",
    "options": [
      "`on_suffixes`",
      "`suffixes`",
      "`lsuffix` and `rsuffix` (used in `join`, not merge)`",
      "`merge_suffixes`"
    ],
    "answer": "`suffixes`",
    "explanation": "The `suffixes` parameter (default `('_x', '_y')`) allows you to specify custom strings to append to column names that are identical in both DataFrames involved in the merge.",
    "difficulty": "Advanced"
  },
  {
    "id": 99,
    "question": "How does `pd.to_datetime(..., utc=True)` affect timezone-naive timestamps?",
    "options": [
      "It converts the time to the local system timezone",
      "It converts the timezone-naive timestamps to UTC-aware timestamps",
      "It drops the time component, keeping only the date",
      "It raises an error if the timestamp is not already timezone-aware"
    ],
    "answer": "It converts the timezone-naive timestamps to UTC-aware timestamps",
    "explanation": "Setting `utc=True` localizes the input datetime to the UTC timezone, converting it from a naive datetime (no timezone info) to an aware one (UTC+00:00).",
    "difficulty": "Advanced"
  },
  {
    "id": 100,
    "question": "What is the effect of the `dropna` parameter in `df.value_counts()`?",
    "options": [
      "It removes rows with NaN from the original DataFrame",
      "It excludes NaN values from the count of unique values (default is True in older versions)",
      "It includes NaN in the unique value counts if set to False",
      "It converts NaN counts to 0"
    ],
    "answer": "It includes NaN in the unique value counts if set to False",
    "explanation": "By default, `value_counts` drops NaNs. Setting `dropna=False` forces pandas to include `NaN` as a distinct category in the output, counting its occurrences.",
    "difficulty": "Advanced"
  }
]