[
  {
    "id": 1,
    "question": "What is the worst-case time complexity of the Bubble Sort algorithm?",
    "options": [
      "O(log n)",
      "O(n)",
      "O(n log n)",
      "O(n²)"
    ],
    "answer": "O(n²)",
    "explanation": "Bubble Sort uses nested loops where the outer loop runs n times and the inner loop runs up to n times in the worst case (reversed array), resulting in O(n²) complexity.",
    "difficulty": "Beginner"
  },
  {
    "id": 2,
    "question": "Which data structure operates on the Last-In, First-Out (LIFO) principle?",
    "options": [
      "Queue",
      "Stack",
      "Linked List",
      "Priority Queue"
    ],
    "answer": "Stack",
    "explanation": "A Stack removes the most recently added element first. Queues use FIFO, while Linked Lists and Priority Queues have different access patterns.",
    "difficulty": "Beginner"
  },
  {
    "id": 3,
    "question": "What is the primary requirement for Binary Search to function correctly?",
    "options": [
      "The array must be sorted",
      "The array must contain only integers",
      "The array size must be a power of two",
      "The array must be implemented as a linked list"
    ],
    "answer": "The array must be sorted",
    "explanation": "Binary search relies on the order of elements to eliminate half of the remaining search space each step. It fails on unsorted data.",
    "difficulty": "Beginner"
  },
  {
    "id": 4,
    "question": "In Big O notation, how is the time complexity of accessing an element in an array by its index classified?",
    "options": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n²)"
    ],
    "answer": "O(1)",
    "explanation": "Arrays allow random access, meaning memory can be calculated directly from the index using pointer arithmetic, taking constant time.",
    "difficulty": "Beginner"
  },
  {
    "id": 5,
    "question": "Which sorting algorithm has a best-case time complexity of O(n log n)?",
    "options": [
      "Bubble Sort",
      "Selection Sort",
      "Merge Sort",
      "Insertion Sort"
    ],
    "answer": "Merge Sort",
    "explanation": "Merge Sort always divides the array in half, leading to O(n log n) time regardless of initial order. Bubble and Selection are O(n²), while Insertion is O(n) best case.",
    "difficulty": "Beginner"
  },
  {
    "id": 6,
    "question": "What is the primary disadvantage of using an Adjacency Matrix to represent a graph?",
    "options": [
      "High space complexity for sparse graphs",
      "Slow edge lookup time",
      "Difficulty in adding vertices",
      "Inability to represent weighted edges"
    ],
    "answer": "High space complexity for sparse graphs",
    "explanation": "An Adjacency Matrix requires V² space regardless of the number of edges, making it inefficient for graphs with few edges (sparse graphs).",
    "difficulty": "Beginner"
  },
  {
    "id": 7,
    "question": "Which algorithm design technique involves breaking a problem into subproblems, solving them recursively, and combining their solutions?",
    "options": [
      "Greedy approach",
      "Divide and Conquer",
      "Dynamic Programming",
      "Backtracking"
    ],
    "answer": "Divide and Conquer",
    "explanation": "Divide and Conquer (e.g., Merge Sort) splits the problem, solves independent subproblems, and merges results. Dynamic programming requires overlapping subproblems.",
    "difficulty": "Beginner"
  },
  {
    "id": 8,
    "question": "What condition defines a 'stable' sorting algorithm?",
    "options": [
      "It always executes in O(n log n) time",
      "It preserves the relative order of equal elements",
      "It requires no additional memory space",
      "It works on both linked lists and arrays"
    ],
    "answer": "It preserves the relative order of equal elements",
    "explanation": "Stability means that if two elements are equal, their order in the sorted output matches their order in the input.",
    "difficulty": "Beginner"
  },
  {
    "id": 9,
    "question": "What is the time complexity of a linear search algorithm?",
    "options": [
      "O(1)",
      "O(log n)",
      "O(n)",
      "O(n²)"
    ],
    "answer": "O(n)",
    "explanation": "In the worst case, a linear search checks every element in the input of size n exactly once, resulting in linear time complexity.",
    "difficulty": "Beginner"
  },
  {
    "id": 10,
    "question": "Which property distinguishes a Min-Heap from a Max-Heap?",
    "options": [
      "Shape of the tree structure",
      "Parent nodes are smaller than children in a Min-Heap",
      "Min-Heaps are balanced while Max-Heaps are not",
      "Implementation using arrays vs linked lists"
    ],
    "answer": "Parent nodes are smaller than children in a Min-Heap",
    "explanation": "A Min-Heap ensures the root is the minimum element and every parent is less than or equal to its children. Max-Heaps have parents larger than children.",
    "difficulty": "Beginner"
  },
  {
    "id": 11,
    "question": "In Recursion, what is the 'base case'?",
    "options": [
      "The recursive call that processes the data",
      "The condition that stops the recursion",
      "The initial argument passed to the function",
      "The variable holding the return value"
    ],
    "answer": "The condition that stops the recursion",
    "explanation": "The base case returns a value without making a further recursive call, preventing infinite loops and stack overflow errors.",
    "difficulty": "Beginner"
  },
  {
    "id": 12,
    "question": "What is the space complexity of Merge Sort in a standard implementation?",
    "options": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n log n)"
    ],
    "answer": "O(n)",
    "explanation": "Standard Merge Sort requires an auxiliary array to merge the sorted subarrays, resulting in linear space usage.",
    "difficulty": "Beginner"
  },
  {
    "id": 13,
    "question": "Which traversal method visits nodes in a Binary Search Tree (BST) in sorted order?",
    "options": [
      "Pre-order traversal",
      "Post-order traversal",
      "In-order traversal",
      "Level-order traversal"
    ],
    "answer": "In-order traversal",
    "explanation": "In-order traversal visits Left Child -> Root -> Right Child. In a BST, this property visits elements in ascending order.",
    "difficulty": "Beginner"
  },
  {
    "id": 14,
    "question": "What is the primary purpose of a Hash Table's collision resolution strategy?",
    "options": [
      "To reduce the time complexity to O(1)",
      "To handle two keys hashing to the same index",
      "To increase the load factor",
      "To sort the data as it is inserted"
    ],
    "answer": "To handle two keys hashing to the same index",
    "explanation": "A collision occurs when two keys map to the same bucket. Resolution strategies like chaining or open addressing manage these conflicts.",
    "difficulty": "Beginner"
  },
  {
    "id": 15,
    "question": "Which algorithm uses a greedy approach to find the shortest path from a single source in a graph with non-negative weights?",
    "options": [
      "Floyd-Warshall algorithm",
      "Bellman-Ford algorithm",
      "Dijkstra's algorithm",
      "Kruskal's algorithm"
    ],
    "answer": "Dijkstra's algorithm",
    "explanation": "Dijkstra's algorithm greedily selects the nearest unvisited node to update distances, assuming non-negative edge weights.",
    "difficulty": "Beginner"
  },
  {
    "id": 16,
    "question": "What is the role of a 'pivot' in the Quick Sort algorithm?",
    "options": [
      "To merge two sorted subarrays",
      "To partition the array into elements less than and greater than itself",
      "To swap adjacent elements",
      "To store the minimum value found"
    ],
    "answer": "To partition the array into elements less than and greater than itself",
    "explanation": "The pivot is chosen to divide the array; elements smaller than the pivot go left, and larger go right, enabling the recursive sort.",
    "difficulty": "Beginner"
  },
  {
    "id": 17,
    "question": "Which data structure is typically used to implement a Breadth-First Search (BFS)?",
    "options": [
      "Stack",
      "Queue",
      "Priority Queue",
      "Array"
    ],
    "answer": "Queue",
    "explanation": "BFS explores neighbors level by level. A Queue is ideal because it processes elements in the order they were discovered (FIFO).",
    "difficulty": "Beginner"
  },
  {
    "id": 18,
    "question": "What defines a 'complete' binary tree?",
    "options": [
      "All nodes have either 0 or 2 children",
      "Every level, except possibly the last, is completely filled, and nodes are as far left as possible",
      "The tree is perfectly balanced with equal numbers of left and right nodes",
      "Leaves exist only at the deepest level"
    ],
    "answer": "Every level, except possibly the last, is completely filled, and nodes are as far left as possible",
    "explanation": "Completeness requires all levels to be full except possibly the last, which must fill from left to right without gaps.",
    "difficulty": "Beginner"
  },
  {
    "id": 19,
    "question": "Which complexity class describes an algorithm whose runtime doubles for every additional element in the input?",
    "options": [
      "O(n)",
      "O(n²)",
      "O(2ⁿ)",
      "O(log n)"
    ],
    "answer": "O(2ⁿ)",
    "explanation": "Exponential complexity (O(2ⁿ)) means the time grows exponentially with input size n, typical of brute-force recursive solutions.",
    "difficulty": "Beginner"
  },
  {
    "id": 20,
    "question": "In the context of algorithmic analysis, what does the term 'Auxiliary Space' refer to?",
    "options": [
      "Total space taken by the input and the algorithm",
      "Extra or temporary space used by the algorithm excluding input size",
      "Space required for the compiler",
      "Space occupied by the source code"
    ],
    "answer": "Extra or temporary space used by the algorithm excluding input size",
    "explanation": "Auxiliary Space measures the temporary memory allocated during execution (like temporary arrays or stack space), distinct from total Space Complexity.",
    "difficulty": "Beginner"
  },
  {
    "id": 21,
    "question": "Which search algorithm requires the data to be uniformly distributed to perform optimally?",
    "options": [
      "Binary Search",
      "Linear Search",
      "Interpolation Search",
      "Jump Search"
    ],
    "answer": "Interpolation Search",
    "explanation": "Interpolation Search estimates the position of the key based on value distribution. It performs poorly (O(n)) if data is not uniform.",
    "difficulty": "Beginner"
  },
  {
    "id": 22,
    "question": "What is the result of `3 % 2` in most programming languages?",
    "options": [
      "1",
      "1.5",
      "0",
      "2"
    ],
    "answer": "1",
    "explanation": "The modulo operator `%` returns the remainder of the division. 3 divided by 2 is 1 with a remainder of 1.",
    "difficulty": "Beginner"
  },
  {
    "id": 23,
    "question": "Which of the following is NOT a typical method for handling collisions in Hash Tables?",
    "options": [
      "Chaining",
      "Open Addressing",
      "Linear Probing",
      "Binary Search"
    ],
    "answer": "Binary Search",
    "explanation": "Binary Search is a search algorithm. Chaining, Open Addressing, and Linear Probing are techniques to resolve hash collisions.",
    "difficulty": "Beginner"
  },
  {
    "id": 24,
    "question": "Which sorting algorithm is considered 'adaptive'?",
    "options": [
      "Selection Sort",
      "Merge Sort",
      "Insertion Sort",
      "Heap Sort"
    ],
    "answer": "Insertion Sort",
    "explanation": "An adaptive algorithm takes advantage of existing order in the input. Insertion Sort runs faster (O(n)) on partially sorted data.",
    "difficulty": "Beginner"
  },
  {
    "id": 25,
    "question": "In a Linked List, what is the time complexity to insert a node at the beginning of the list?",
    "options": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n²)"
    ],
    "answer": "O(1)",
    "explanation": "Inserting at the head requires only updating the `head` pointer and the new node's `next` pointer, which are constant time operations.",
    "difficulty": "Beginner"
  },
  {
    "id": 26,
    "question": "What does the term 'Greedy Algorithm' imply?",
    "options": [
      "It tries all possible solutions to find the best one",
      "It makes the locally optimal choice at each step with the hope of finding a global optimum",
      "It divides the problem into smaller subproblems",
      "It uses randomness to improve performance"
    ],
    "answer": "It makes the locally optimal choice at each step with the hope of finding a global optimum",
    "explanation": "Greedy algorithms prioritize immediate benefit without backtracking. This does not always guarantee the best global solution.",
    "difficulty": "Beginner"
  },
  {
    "id": 27,
    "question": "Which data structure is best suited to implement a recursive function iteratively?",
    "options": [
      "Queue",
      "Heap",
      "Stack",
      "Set"
    ],
    "answer": "Stack",
    "explanation": "Recursion relies on the call stack to track return addresses and local variables. Explicitly using a Stack data structure mimics this behavior.",
    "difficulty": "Beginner"
  },
  {
    "id": 28,
    "question": "What is the output of the following operation: `5 << 1` (bitwise left shift)?",
    "options": [
      "2",
      "5",
      "10",
      "25"
    ],
    "answer": "10",
    "explanation": "Left shifting an integer by 1 bit multiplies it by 2. The binary for 5 (101) becomes 1010, which is 10.",
    "difficulty": "Beginner"
  },
  {
    "id": 29,
    "question": "Which algorithm is used to find the Minimum Spanning Tree of a graph?",
    "options": [
      "Floyd-Warshall",
      "Dijkstra",
      "Kruskal's",
      "Bellman-Ford"
    ],
    "answer": "Kruskal's",
    "explanation": "Kruskal's algorithm (and Prim's) is designed to connect all vertices in a graph with the minimum total edge weight without cycles.",
    "difficulty": "Beginner"
  },
  {
    "id": 30,
    "question": "What is a 'Watchdog Timer' in the context of algorithmic reliability systems?",
    "options": [
      "A sorting algorithm",
      "A hardware timer used to detect and recover from computer malfunctions",
      "A function to measure execution time",
      "A loop counter variable"
    ],
    "answer": "A hardware timer used to detect and recover from computer malfunctions",
    "explanation": "A Watchdog Timer is a fail-safe mechanism that resets the system if the main program hangs or fails to 'kick' the timer within a set period.",
    "difficulty": "Beginner"
  },
  {
    "id": 31,
    "question": "In Big O notation, why are constants typically dropped (e.g., O(2n) becomes O(n))?",
    "options": [
      "They are difficult to calculate",
      "As n grows large, the constant becomes irrelevant relative to the growth rate",
      "Computers are too fast to notice constants",
      "Constants only affect space complexity"
    ],
    "answer": "As n grows large, the constant becomes irrelevant relative to the growth rate",
    "explanation": "Big O focuses on asymptotic growth as input size approaches infinity. Lower-order terms and constants become negligible.",
    "difficulty": "Beginner"
  },
  {
    "id": 32,
    "question": "Which term describes a situation where an algorithm makes repeated calls to the same inputs?",
    "options": [
      "Overlapping Subproblems",
      "Optimal Substructure",
      "Greedy Choice Property",
      "Dynamic Constraint"
    ],
    "answer": "Overlapping Subproblems",
    "explanation": "Overlapping subproblems occur when a recursive algorithm revisits the same subproblem multiple times, which Dynamic Programming aims to optimize.",
    "difficulty": "Beginner"
  },
  {
    "id": 33,
    "question": "What distinguishes a 'Queue' from a 'Priority Queue'?",
    "options": [
      "FIFO vs priority-based element removal",
      "Stack-based vs array-based implementation",
      "Push vs pop operations",
      "Linear vs constant time insertion"
    ],
    "answer": "FIFO vs priority-based element removal",
    "explanation": "A standard Queue removes the oldest element (FIFO). A Priority Queue removes the element with the highest (or lowest) priority.",
    "difficulty": "Beginner"
  },
  {
    "id": 34,
    "question": "What is the worst-case time complexity for finding an element in a Binary Search Tree (BST)?",
    "options": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n log n)"
    ],
    "answer": "O(n)",
    "explanation": "In the worst case (a skewed tree that resembles a linked list), the BST degenerates, requiring traversal of all n nodes.",
    "difficulty": "Beginner"
  },
  {
    "id": 35,
    "question": "Which of the following is NOT a property of an algorithm?",
    "options": [
      "Finiteness",
      "Ambiguity",
      "Input",
      "Effectiveness"
    ],
    "answer": "Ambiguity",
    "explanation": "Algorithms must be unambiguous; each step must be clear and lead to a single, definite interpretation. Finiteness, Input, and Effectiveness are core properties.",
    "difficulty": "Beginner"
  },
  {
    "id": 36,
    "question": "Which condition is strictly required for the Sieve of Eratosthenes to correctly identify all prime numbers up to a limit N?",
    "options": [
      "The input array must be initialized with -1",
      "The limit N must be an even number",
      "The limit N must be sufficiently large to overflow the stack",
      "The algorithm must iterate only up to the square root of N to mark composites"
    ],
    "answer": "The algorithm must iterate only up to the square root of N to mark composites",
    "explanation": "Any composite number $x$ must have a prime factor $p \\le \\sqrt{x}$. Therefore, iterating past $\\sqrt{N}$ to mark multiples is redundant for finding primes up to $N$.",
    "difficulty": "Intermediate"
  },
  {
    "id": 37,
    "question": "Why does Dijkstra's algorithm fail to calculate the shortest path when a graph contains negative edge weights?",
    "options": [
      "It uses a greedy approach that assumes a once-selected node has the shortest final path",
      "Negative weights cause the priority queue to overflow",
      "The algorithm requires the graph to be a directed acyclic graph (DAG)",
      "It cannot detect negative cycles, leading to infinite loops"
    ],
    "answer": "It uses a greedy approach that assumes a once-selected node has the shortest final path",
    "explanation": "Dijkstra's greedy strategy extracts the node with the minimum distance and marks it as 'visited'. Negative edges could later provide a shorter path to a node already processed, violating this core assumption.",
    "difficulty": "Intermediate"
  },
  {
    "id": 38,
    "question": "What is the primary time complexity advantage of using a Fenwick Tree (Binary Indexed Tree) over a Segment Tree for point updates and prefix sum queries?",
    "options": [
      "Fenwick Trees have a lower constant factor and bitwise operations",
      "Segment Trees require $O(N)$ space whereas Fenwick Trees require $O(\\log N)$",
      "Fenwick Trees perform queries in $O(1)$ time",
      "Segment Trees cannot handle point updates efficiently"
    ],
    "answer": "Fenwick Trees have a lower constant factor and bitwise operations",
    "explanation": "While both structures support operations in $O(\\log N)$, Fenwick Trees are typically faster in practice and have a smaller memory footprint ($O(N)$ vs $4N$) due to simpler pointer arithmetic using bitwise operations.",
    "difficulty": "Intermediate"
  },
  {
    "id": 39,
    "question": "Which property of the modulo operation is utilized in the 'Modular Multiplicative Inverse' to solve linear congruence equations?",
    "options": [
      "If $\\gcd(a, m) = 1$, then there exists an integer $x$ such that $a \\cdot x \\equiv 1 \\pmod m$",
      "If $a \\equiv b \\pmod m$, then $a^k \\equiv b^k \\pmod{m^k}$",
      "The modulo operator distributes over subtraction and division",
      "The inverse exists only if $m$ is a prime number"
    ],
    "answer": "If $\\gcd(a, m) = 1$, then there exists an integer $x$ such that $a \\cdot x \\equiv 1 \\pmod m$",
    "explanation": "The modular inverse of $a$ modulo $m$ exists if and only if $a$ and $m$ are coprime (gcd is 1). This property allows dividing by $a$ under modulo by multiplying by its inverse.",
    "difficulty": "Intermediate"
  },
  {
    "id": 40,
    "question": "What is the specific role of the 'Union-Find' (Disjoint Set Union) data structure in Kruskal's algorithm for finding a Minimum Spanning Tree (MST)?",
    "options": [
      "To sort the edges by weight in ascending order",
      "To efficiently detect and prevent cycles when adding edges",
      "To store the adjacency list representation of the graph",
      "To calculate the shortest path between the root nodes"
    ],
    "answer": "To efficiently detect and prevent cycles when adding edges",
    "explanation": "Kruskal's algorithm processes edges in order of weight. Union-Find checks if two vertices of an edge are already in the same connected component; if so, adding the edge would create a cycle.",
    "difficulty": "Intermediate"
  },
  {
    "id": 41,
    "question": "In the Fast Fourier Transform (FFT), what is the significance of the 'bit-reversal' permutation step?",
    "options": [
      "It reverses the bits of the input coefficients to enable in-place computation",
      "It converts the signal from frequency domain to time domain",
      "It ensures the result fits within the 64-bit integer limit",
      "It optimizes the cache locality of the recursive calls"
    ],
    "answer": "It reverses the bits of the input coefficients to enable in-place computation",
    "explanation": "The Cooley-Tukey FFT algorithm divides the input by even and odd indices. Bit-reversal reorders the input array so that the recursive stages can be performed iteratively and in-place, accessing elements in the correct sequence.",
    "difficulty": "Intermediate"
  },
  {
    "id": 42,
    "question": "When implementing the 'Two Pointers' technique on an array, what pre-condition must typically be satisfied to guarantee correctness?",
    "options": [
      "The array must contain only positive integers",
      "The array size must be a power of two",
      "The array must be sorted (or monotonic) regarding the target search criteria",
      "The pointers must start at the maximum distance apart"
    ],
    "answer": "The array must be sorted (or monotonic) regarding the target search criteria",
    "explanation": "The Two Pointers technique relies on moving a pointer left or right based on a comparison with the current sum or value. This logic assumes that moving one pointer in a specific direction consistently increases or decreases the relevant metric, which requires sorting.",
    "difficulty": "Intermediate"
  },
  {
    "id": 43,
    "question": "Which algorithm is specifically designed to find the shortest path from a single source to all other nodes in a graph that contains negative edge weights but no negative cycles?",
    "options": [
      "Dijkstra's Algorithm",
      "Prim's Algorithm",
      "Bellman-Ford Algorithm",
      "Floyd-Warshall Algorithm"
    ],
    "answer": "Bellman-Ford Algorithm",
    "explanation": "The Bellman-Ford algorithm relaxes all edges $|V|-1$ times, allowing it to propagate the effects of negative weights. Dijkstra's fails with negative weights, and Floyd-Warshall is for all-pairs paths.",
    "difficulty": "Intermediate"
  },
  {
    "id": 44,
    "question": "What is the primary advantage of using a 'Sparse Table' over a 'Segment Tree' for Range Minimum Queries (RMQ)?",
    "options": [
      "Sparse Tables support updates in $O(1)$ time",
      "Sparse Tables provide $O(1)$ query time with no log factor",
      "Sparse Tables use less memory than Segment Trees",
      "Sparse Tables can handle range updates efficiently"
    ],
    "answer": "Sparse Tables provide $O(1)$ query time with no log factor",
    "explanation": "Sparse Tables use $O(N \\log N)$ pre-processing to answer static RMQs in $O(1)$ using the properties of overlapping intervals (Power of Two lengths). Segment Trees require $O(\\log N)$ per query.",
    "difficulty": "Intermediate"
  },
  {
    "id": 45,
    "question": "In the 'Rabin-Karp' string matching algorithm, what is the purpose of the 'rolling hash' function?",
    "options": [
      "To encrypt the pattern before searching",
      "To allow updating the hash value of a sliding window in constant time",
      "To minimize the number of collisions using a lookup table",
      "To sort the characters of the pattern lexicographically"
    ],
    "answer": "To allow updating the hash value of a sliding window in constant time",
    "explanation": "When the window slides by one character, the rolling hash removes the contribution of the outgoing character and adds the incoming one using arithmetic, avoiding recalculating the hash for the entire substring.",
    "difficulty": "Intermediate"
  },
  {
    "id": 46,
    "question": "Which of the following statements correctly describes the 'Meet in the Middle' algorithmic technique?",
    "options": [
      "It splits the input into two halves, solves both recursively, and merges results",
      "It divides the problem into two independent subsets, brute-forces both, and combines results",
      "It performs a binary search between the left and right boundaries of the answer space",
      "It is a variation of binary search applied to graph problems"
    ],
    "answer": "It divides the problem into two independent subsets, brute-forces both, and combines results",
    "explanation": "Meet in the Middle reduces time complexity from $O(2^N)$ to $O(2^{N/2})$ by splitting the set in half, enumerating all subset sums/possibilities for each half, sorting/searching one half against the other.",
    "difficulty": "Intermediate"
  },
  {
    "id": 47,
    "question": "In Dynamic Programming, what is the distinguishing characteristic of the 'Space Optimization' technique?",
    "options": [
      "Using a hash map to store states instead of an array",
      "Reducing the time complexity from $O(N^2)$ to $O(N \\log N)$",
      "Storing only the previously computed states required for the next step, rather than the full DP table",
      "Precomputing all factorials modulo $10^9 + 7$"
    ],
    "answer": "Storing only the previously computed states required for the next step, rather than the full DP table",
    "explanation": "Many DP recurrences (like Fibonacci or Knapsack) only depend on the previous row or two values. Space optimization exploits this to reduce memory from $O(N \\times M)$ to $O(M)$ or $O(1)$.",
    "difficulty": "Intermediate"
  },
  {
    "id": 48,
    "question": "What is the necessary condition for the 'Mo's Algorithm' to be applicable to a problem?",
    "options": [
      "The queries must be processed offline and the data structure must support add/remove operations",
      "The data must be a tree structure",
      "The query complexity must be strictly less than $O(N)$",
      "The input must be small enough to fit in L1 cache"
    ],
    "answer": "The queries must be processed offline and the data structure must support add/remove operations",
    "explanation": "Mo's Algorithm rearranges the order of queries (offline processing) to minimize the movement of pointers ($L, R$). It requires the underlying problem to support modifying the answer by adding or removing an element from the current range.",
    "difficulty": "Intermediate"
  },
  {
    "id": 49,
    "question": "In a 'Max Flow' problem, what constitutes the 'Residual Capacity' of an edge?",
    "options": [
      "The original capacity of the edge minus the flow already pushed through it",
      "The maximum flow that can pass through the source node",
      "The total capacity of all edges entering a node",
      "The capacity of the reverse edge in the residual graph"
    ],
    "answer": "The original capacity of the edge minus the flow already pushed through it",
    "explanation": "Residual capacity represents how much more flow can be pushed through an edge. Algorithms like Ford-Fulkerson construct a residual graph where these capacities determine augmenting paths.",
    "difficulty": "Intermediate"
  },
  {
    "id": 50,
    "question": "Why is the 'Tarjan's Strongly Connected Components' algorithm generally preferred over Kosaraju's algorithm for large graphs?",
    "options": [
      "Tarjan's algorithm uses only one DFS pass, whereas Kosaraju's uses two",
      "Tarjan's algorithm does not require a stack",
      "Tarjan's algorithm works on directed graphs only",
      "Tarjan's algorithm has a time complexity of $O(V)$"
    ],
    "answer": "Tarjan's algorithm uses only one DFS pass, whereas Kosaraju's uses two",
    "explanation": "While both are $O(V+E)$, Tarjan's performs a single DFS traversal to identify SCCs, whereas Kosaraju's requires a DFS to compute order, a transpose of the graph, and a second DFS.",
    "difficulty": "Intermediate"
  },
  {
    "id": 51,
    "question": "What is the time complexity of finding the Longest Common Subsequence (LCS) of two strings of lengths $M$ and $N$ using standard Dynamic Programming?",
    "options": [
      "$O((M + N) \\log(M + N))$",
      "$O(M \\cdot N)$",
      "$O(2^{\\min(M, N)})$",
      "$O(M + N)$"
    ],
    "answer": "$O(M \\cdot N)$",
    "explanation": "The standard DP approach builds a table of size $(M+1) \\times (N+1)$. Filling each cell takes constant time based on the previous cell's value, resulting in a quadratic time complexity relative to the string lengths.",
    "difficulty": "Intermediate"
  },
  {
    "id": 52,
    "question": "In the context of computational geometry, the 'Graham Scan' algorithm is used to construct what structure?",
    "options": [
      "Voronoi Diagram",
      "Convex Hull",
      "Delaunay Triangulation",
      "Minimum Spanning Tree"
    ],
    "answer": "Convex Hull",
    "explanation": "Graham Scan is an efficient $O(N \\log N)$ algorithm to find the convex hull (the smallest convex polygon containing all points) of a set of 2D points by sorting them and maintaining a stack of boundary points.",
    "difficulty": "Intermediate"
  },
  {
    "id": 53,
    "question": "Which property differentiates a 'Binary Heap' used for Priority Queues from a Binary Search Tree (BST)?",
    "options": [
      "Heaps allow duplicate values while BSTs do not",
      "Heaps are complete binary trees, whereas BSTs are not guaranteed to be complete",
      "Heaps guarantee $O(N)$ search time while BSTs guarantee $O(\\log N)$",
      "Heaps store larger values at the root while BSTs store smaller values"
    ],
    "answer": "Heaps are complete binary trees, whereas BSTs are not guaranteed to be complete",
    "explanation": "The shape property of a binary heap requires it to be a complete tree (filled left to right). This allows implementation in a simple array. A BST prioritizes the order property (Left < Root < Right) and can have arbitrary structure.",
    "difficulty": "Intermediate"
  },
  {
    "id": 54,
    "question": "What is the worst-case time complexity of the 'QuickSort' algorithm when the pivot selection consistently results in the most unbalanced partitions?",
    "options": [
      "$O(N \\log N)$",
      "$O(N)$",
      "$O(N^2)$",
      "$O(2^N)$"
    ],
    "answer": "$O(N^2)$",
    "explanation": "If the pivot is always the smallest or largest element (e.g., sorted input with bad pivot choice), the recursion depth becomes $N$, and the total operations degrade to a quadratic sum.",
    "difficulty": "Intermediate"
  },
  {
    "id": 55,
    "question": "Which data structure is most efficient for implementing a 'Least Recently Used' (LRU) Cache?",
    "options": [
      "Hash Map with Doubly Linked List",
      "Circular Buffer",
      "Binary Search Tree",
      "Dynamic Array"
    ],
    "answer": "Hash Map with Doubly Linked List",
    "explanation": "A Hash Map provides $O(1)$ access to cache entries. A Doubly Linked List allows $O(1)$ removal of a node and reinsertion at the head (most recently used), or removal of the tail (least recently used) for eviction.",
    "difficulty": "Intermediate"
  },
  {
    "id": 56,
    "question": "In the context of Bit Manipulation, what is the result of the operation `(x & -x)`?",
    "options": [
      "It clears the least significant bit (LSB)",
      "It isolates the rightmost 1-bit (obtains the lowest set bit)",
      "It negates the integer x",
      "It checks if x is a power of 2"
    ],
    "answer": "It isolates the rightmost 1-bit (obtains the lowest set bit)",
    "explanation": "In two's complement representation, `-x` is `~x + 1`. The expression `x & -x` results in a binary number with only the least significant bit of `x` set, useful in Fenwick Trees and lowbit calculations.",
    "difficulty": "Intermediate"
  },
  {
    "id": 57,
    "question": "What is the defining characteristic of a 'Greedy Algorithm' that makes it suitable for the 'Activity Selection Problem'?",
    "options": [
      "It evaluates all possible subsets of activities to find the maximum size",
      "It makes locally optimal choices at each step hoping to find a global optimum",
      "It divides the problem into two halves and solves them recursively",
      "It requires a backtracking step to verify the solution"
    ],
    "answer": "It makes locally optimal choices at each step hoping to find a global optimum",
    "explanation": "For the Activity Selection problem, always picking the activity with the earliest finish time (local optimum) leaves the maximum remaining time for other activities, proving to be the global optimum.",
    "difficulty": "Intermediate"
  },
  {
    "id": 58,
    "question": "Why is the 'A*' (A-Star) search algorithm preferred over Dijkstra for pathfinding in grid-based maps or games?",
    "options": [
      "A* uses a heuristic to estimate the cost to the goal, guiding the search direction",
      "A* has a better worst-case time complexity than Dijkstra",
      "A* does not require a priority queue",
      "A* can handle negative edge weights where Dijkstra cannot"
    ],
    "answer": "A* uses a heuristic to estimate the cost to the goal, guiding the search direction",
    "explanation": "A* evaluates nodes using $f(n) = g(n) + h(n)$, where $h(n)$ is a heuristic estimate of the distance to the goal. This directs the search toward the target, often exploring fewer nodes than Dijkstra's blind uniform cost search.",
    "difficulty": "Intermediate"
  },
  {
    "id": 59,
    "question": "What is the primary function of the 'Bloom Filter' data structure?",
    "options": [
      "To store a set of integers in sorted order",
      "To efficiently test whether an element is a member of a set with a probability of false positives",
      "To compress strings using lossless compression",
      "To find the shortest path in a weighted graph"
    ],
    "answer": "To efficiently test whether an element is a member of a set with a probability of false positives",
    "explanation": "A Bloom Filter is a space-efficient probabilistic data structure. It can tell you if an item is definitely *not* in the set or *probably* in the set, allowing for false positives but never false negatives.",
    "difficulty": "Intermediate"
  },
  {
    "id": 60,
    "question": "In the 'Kadane's Algorithm' for finding the maximum subarray sum, why does resetting the current sum to 0 upon encountering a negative number work?",
    "options": [
      "Negative numbers are not part of the input",
      "A negative prefix sum decreases the total sum of any subsequent subarray",
      "The algorithm requires the subarray to start at an odd index",
      "It minimizes the space complexity by clearing the accumulator"
    ],
    "answer": "A negative prefix sum decreases the total sum of any subsequent subarray",
    "explanation": "If the running sum becomes negative, it is counter-productive to keep it for the next element, as adding a negative to any future number reduces the result. Thus, the optimal subarray starts fresh after the negative dip.",
    "difficulty": "Intermediate"
  },
  {
    "id": 61,
    "question": "Which of the following is a consequence of the 'Pigeonhole Principle' in algorithm analysis?",
    "options": [
      "If $n$ items are put into $m$ containers, with $n > m$, then at least one container must contain more than one item",
      "The worst-case complexity of any sorting algorithm is $O(N \\log N)$",
      "A hash table will never have collisions",
      "Any recursive algorithm can be converted to an iterative one"
    ],
    "answer": "If $n$ items are put into $m$ containers, with $n > m$, then at least one container must contain more than one item",
    "explanation": "This principle is fundamental for proving the existence of collisions in hash functions (pigeons) with a limited range of output values (pigeonholes) and proving lower bounds in sorting.",
    "difficulty": "Intermediate"
  },
  {
    "id": 62,
    "question": "What does the 'Master Theorem' provide a method for calculating?",
    "options": [
      "The time complexity of divide-and-conquer recurrences of the form $T(n) = aT(n/b) + f(n)$",
      "The space complexity of recursive depth-first search",
      "The optimal branching factor for a B-Tree",
      "The greatest common divisor of two numbers"
    ],
    "answer": "The time complexity of divide-and-conquer recurrences of the form $T(n) = aT(n/b) + f(n)$",
    "explanation": "The Master Theorem is a direct way to solve recurrence relations for algorithms that break a problem of size $n$ into $a$ subproblems of size $n/b$ with a work function $f(n)$.",
    "difficulty": "Intermediate"
  },
  {
    "id": 63,
    "question": "In the 'Traveling Salesperson Problem' (TSP), what is the role of 'Held-Karp' Dynamic Programming?",
    "options": [
      "It provides a greedy approximation solution",
      "It solves TSP exactly in $O(n^2 2^n)$ time rather than $O(n!)$",
      "It converts the problem into a Max-Flow instance",
      "It restricts the graph to a planar embedding"
    ],
    "answer": "It solves TSP exactly in $O(n^2 2^n)$ time rather than $O(n!)$",
    "explanation": "The Held-Karp algorithm uses DP with bitmask states to keep track of visited sets of nodes, reducing the complexity from brute force factorial to exponential, which is feasible for $N$ up to roughly 20-25.",
    "difficulty": "Intermediate"
  },
  {
    "id": 64,
    "question": "What distinguishes a 'Red-Black Tree' from an AVL Tree?",
    "options": [
      "Red-Black trees allow for faster lookups but slower insertions",
      "Red-Black trees ensure the tree is perfectly balanced, while AVL trees are approximately balanced",
      "Red-Black trees provide faster insertion and deletion at the cost of slightly less strict balancing",
      "Red-Black trees do not use rotations"
    ],
    "answer": "Red-Black trees provide faster insertion and deletion at the cost of slightly less strict balancing",
    "explanation": "AVL trees are strictly balanced (height difference of at most 1), leading to faster lookups but potentially more rotations during updates. Red-Black trees are more relaxed, offering better performance for write-heavy operations.",
    "difficulty": "Intermediate"
  },
  {
    "id": 65,
    "question": "Which algorithmic technique is used to compute the determinant of a matrix or solve a system of linear equations in $O(N^3)$?",
    "options": [
      "Gaussian Elimination",
      "Breadth-First Search",
      "Prim's Algorithm",
      "Fast Fourier Transform"
    ],
    "answer": "Gaussian Elimination",
    "explanation": "Gaussian Elimination transforms a matrix into row echelon form using elementary row operations. This process allows for finding solutions to linear systems or calculating determinants in cubic time.",
    "difficulty": "Intermediate"
  },
  {
    "id": 66,
    "question": "What is the worst-case time complexity of the 'Edmonds-Karp' implementation of the Ford-Fulkerson method for Max Flow?",
    "options": [
      "$O(V E^2)$",
      "$O(V^2 E)$",
      "$O(E \\log V)$",
      "$O(V^3)$"
    ],
    "answer": "$O(V E^2)$",
    "explanation": "Edmonds-Karp implements Ford-Fulkerson using BFS to find augmenting paths. It runs in $O(V E^2)$ because each augmenting path increases the shortest path length from source to sink, which can happen at most $V$ times, costing $O(E)$ per BFS.",
    "difficulty": "Intermediate"
  },
  {
    "id": 67,
    "question": "In the 'KMP' (Knuth-Morris-Pratt) algorithm, what is the purpose of the 'LPS' (Longest Prefix Suffix) array?",
    "options": [
      "To store the length of the longest proper prefix which is also a suffix for every sub-pattern",
      "To index the characters of the text string",
      "To reverse the string for matching",
      "To calculate the hash of the pattern"
    ],
    "answer": "To store the length of the longest proper prefix which is also a suffix for every sub-pattern",
    "explanation": "The LPS array (also known as the 'prefix function') tells the algorithm how many characters can be skipped when a mismatch occurs. By avoiding re-checking characters that are known to match, KMP achieves $O(N+M)$ time.",
    "difficulty": "Intermediate"
  },
  {
    "id": 68,
    "question": "What is the key prerequisite for applying the 'Heavy Light Decomposition' (HLD) technique on a tree?",
    "options": [
      "The tree must be a binary tree",
      "The tree must be rooted (to define parent-child relationships and subtree sizes)",
      "All edge weights must be equal",
      "The tree must have exactly $2^N$ nodes"
    ],
    "answer": "The tree must be rooted (to define parent-child relationships and subtree sizes)",
    "explanation": "HLD relies on decomposing the tree into chains based on subtree sizes (heavy vs. light children). This requires the tree to be rooted so that 'subtree size' and 'parent' direction are well-defined.",
    "difficulty": "Intermediate"
  },
  {
    "id": 69,
    "question": "Which of the following is the most efficient method to compute the number of trailing zeroes in the factorial of a number $N$ ($N!$)?",
    "options": [
      "Calculate $N!$ and divide by 10 until the remainder is non-zero",
      "Count the number of pairs of prime factors (2, 5) in the prime factorization of $N!$",
      "Sum the digits of $N$ modulo 5",
      "Use the Lucas Theorem"
    ],
    "answer": "Count the number of pairs of prime factors (2, 5) in the prime factorization of $N!$",
    "explanation": "A trailing zero is produced by a factor of 10, or $2 \\times 5$. Since there are always more factors of 2 than 5 in $N!$, the number of trailing zeroes is determined by the sum of $\\lfloor N/5 \\rfloor + \\lfloor N/25 \\rfloor + \\dots$.",
    "difficulty": "Intermediate"
  },
  {
    "id": 70,
    "question": "In the context of network flow algorithms, what is the primary theoretical justification for the correctness of the Ford-Fulkerson method when using rational capacities?",
    "options": [
      "Integrality Theorem",
      "Max-Flow Min-Cut Theorem",
      "Hall's Marriage Theorem",
      "Dilworth's Decomposition Theorem"
    ],
    "answer": "Max-Flow Min-Cut Theorem",
    "explanation": "The Max-Flow Min-Cut Theorem states that the maximum amount of flow passing from the source to the sink is equal to the total weight of the edges in the minimum cut, providing the foundation for the algorithm's termination and correctness.",
    "difficulty": "Advanced"
  },
  {
    "id": 71,
    "question": "Which algorithmic technique is utilized by the Z-algorithm to efficiently compute the Z-array (longest common prefix substring match) in linear time?",
    "options": [
      "Two-pointer approach maintaining an interval [L, R]",
      "Rolling hash with binary search",
      "Suffix automaton state traversal",
      "KMP failure function prefix aggregation"
    ],
    "answer": "Two-pointer approach maintaining an interval [L, R]",
    "explanation": "The Z-algorithm maintains an interval [L, R] which is the interval with maximum R such that the substring S[L...R] is a prefix of S. It uses previously computed values to avoid re-comparing characters, ensuring O(N) complexity.",
    "difficulty": "Advanced"
  },
  {
    "id": 72,
    "question": "What is the time complexity of finding the Longest Common Subsequence (LCS) of two sequences of lengths N and M using the standard Dynamic Programming approach with space optimization?",
    "options": [
      "O(N * M) time and O(min(N, M)) space",
      "O((N + M) * log(N + M)) time and O(N + M) space",
      "O(N + M) time and O(N * M) space",
      "O(N * M) time and O(N * M) space"
    ],
    "answer": "O(N * M) time and O(min(N, M)) space",
    "explanation": "Standard DP requires computing N+1 by M+1 states, taking O(N*M) time. Space can be optimized to O(min(N, M)) because calculating the current row only requires the previous row and the current cell's left neighbor.",
    "difficulty": "Advanced"
  },
  {
    "id": 73,
    "question": "In the implementation of a Treap (Cartesian Tree), what property ensures that the tree structure is balanced with high probability?",
    "options": [
      "Heap property on randomly assigned priorities",
      "Strict enforcement of AVL balance factors",
      "Color-based flipping as in Red-Black trees",
      "Splitting and merging based on rank"
    ],
    "answer": "Heap property on randomly assigned priorities",
    "explanation": "A Treap maintains the Binary Search Tree property on keys and the Heap property on randomly assigned priorities. The random priorities ensure the expected height is logarithmic, making the tree probabilistically balanced.",
    "difficulty": "Advanced"
  },
  {
    "id": 74,
    "question": "What specific problem arises when implementing Dijkstra's algorithm using a standard binary heap versus a Fibonacci heap on dense graphs?",
    "options": [
      "The decrease-key operation is slower in a binary heap, increasing total complexity",
      "The binary heap cannot handle negative edge weights",
      "The Fibonacci heap requires O(V) space for adjacency lists",
      "The binary heap implementation cannot support adjacency matrix representation"
    ],
    "answer": "The decrease-key operation is slower in a binary heap, increasing total complexity",
    "explanation": "Dijkstra's complexity with a binary heap is O((V + E) log V), largely driven by O(E log V) decrease-key calls. A Fibonacci heap reduces decrease-key to O(1) amortized, resulting in O(E + V log V) total time.",
    "difficulty": "Advanced"
  },
  {
    "id": 75,
    "question": "Which theorem guarantees that in a bipartite graph, the size of the maximum matching is equal to the size of the minimum vertex cover?",
    "options": [
      "Kőnig's Theorem",
      "Menger's Theorem",
      "Kuratowski's Theorem",
      "Vizing's Theorem"
    ],
    "answer": "Kőnig's Theorem",
    "explanation": "Kőnig's theorem states that in any bipartite graph, the number of edges in a maximum matching equals the number of vertices in a minimum vertex cover, providing a powerful duality relationship for combinatorial optimization.",
    "difficulty": "Advanced"
  },
  {
    "id": 76,
    "question": "How does the 'Block Decomposition' or 'Square Root Decomposition' technique improve query performance on static arrays?",
    "options": [
      "Divides the array into blocks of size √N to answer range queries in O(√N) time",
      "Partitions the array into log N segments to enable binary search",
      "Transforms the array into a Segment Tree for O(log N) updates",
      "Creates a hash table of all possible sub-arrays for O(1) lookup"
    ],
    "answer": "Divides the array into blocks of size √N to answer range queries in O(√N) time",
    "explanation": "By dividing the array into √N blocks, precomputing block aggregates allows range queries to be answered by combining at most √N block sums and at most √N individual elements, resulting in O(√N) time complexity.",
    "difficulty": "Advanced"
  },
  {
    "id": 77,
    "question": "What is the primary advantage of using Heavy-Light Decomposition (HLD) on a tree?",
    "options": [
      "It reduces path queries to a sequence of O(log N) range queries on a linear data structure",
      "It transforms any tree into a Binary Search Tree for efficient searching",
      "It automatically balances the tree to ensure logarithmic height",
      "It compresses the tree into a Trie for pattern matching"
    ],
    "answer": "It reduces path queries to a sequence of O(log N) range queries on a linear data structure",
    "explanation": "HLD splits the tree into disjoint chains such that the path from any node to the root passes through O(log N) chains. This allows complex path queries (sum, max) to be mapped to range queries on a Segment Tree or BIT.",
    "difficulty": "Advanced"
  },
  {
    "id": 78,
    "question": "In the context of Bitmask Dynamic Programming, how is the state typically represented for solving the Traveling Salesman Problem (TSP) for N cities?",
    "options": [
      "dp[mask][i] representing the minimum cost to reach set 'mask' ending at city 'i'",
      "dp[i][j] representing the cost of the shortest path between city 'i' and 'j'",
      "dp[mask] representing the total cost of visiting all cities in 'mask'",
      "dp[i][mask] representing the number of ways to visit 'mask' starting from city 'i'"
    ],
    "answer": "dp[mask][i] representing the minimum cost to reach set 'mask' ending at city 'i'",
    "explanation": "The TSP DP state tracks the subset of visited cities via a bitmask 'mask' and the current city 'i'. This allows optimal substructure where the solution depends on smaller subsets and the previous city.",
    "difficulty": "Advanced"
  },
  {
    "id": 79,
    "question": "What is the worst-case time complexity of the Edmonds-Karp algorithm for Maximum Flow?",
    "options": [
      "O(V * E^2)",
      "O(E * log V)",
      "O(V^2 * E)",
      "O(V^3)"
    ],
    "answer": "O(V * E^2)",
    "explanation": "Edmonds-Karp is a specific implementation of Ford-Fulkerson that uses BFS to find augmenting paths. It runs in O(V * E^2) because each augmenting path increases the shortest path length from source to sink, and this happens at most V/2 times.",
    "difficulty": "Advanced"
  },
  {
    "id": 80,
    "question": "When using a Segment Tree with Lazy Propagation, what is the time complexity for a range update followed by a range query?",
    "options": [
      "O(log N) for both operations",
      "O(N) for update and O(log N) for query",
      "O(log N) for update and O(N) for query",
      "O(N) for both operations"
    ],
    "answer": "O(log N) for both operations",
    "explanation": "Lazy propagation defers updates to child nodes until strictly necessary. This ensures that only the nodes on the path to the segment range are visited, maintaining O(log N) complexity for both updates and queries.",
    "difficulty": "Advanced"
  },
  {
    "id": 81,
    "question": "Which algorithm is specifically designed to find the Maximum Matching in a general (not necessarily bipartite) graph?",
    "options": [
      "Edmonds' Blossom Algorithm",
      "Hopcroft-Karp Algorithm",
      "Hungarian Algorithm",
      "Prim's Algorithm"
    ],
    "answer": "Edmonds' Blossom Algorithm",
    "explanation": "The Blossom Algorithm handles general graphs by contracting odd-length cycles (blossoms) into single vertices, solving the matching problem where bipartite-specific algorithms like Hopcroft-Karp fail.",
    "difficulty": "Advanced"
  },
  {
    "id": 82,
    "question": "What is the time complexity of Gaussian Elimination for solving a system of N linear equations with N variables?",
    "options": [
      "O(N^3)",
      "O(N^2)",
      "O(N * log N)",
      "O(2^N)"
    ],
    "answer": "O(N^3)",
    "explanation": "The standard Gaussian Elimination algorithm involves three nested loops: iterating over rows, then pivots, and then updating the subsequent rows, resulting in a cubic time complexity of O(N^3).",
    "difficulty": "Advanced"
  },
  {
    "id": 83,
    "question": "In the context of string algorithms, what does the 'Failure Function' (or 'Pi' function) in the Knuth-Morris-Pratt (KMP) algorithm compute?",
    "options": [
      "The length of the longest proper prefix of the substring which is also a suffix",
      "The index of the next character to compare in the text",
      "The number of occurrences of the pattern in the text",
      "The hash value of the current sliding window"
    ],
    "answer": "The length of the longest proper prefix of the substring which is also a suffix",
    "explanation": "The failure function optimizes the search by storing the length of the longest proper prefix that is also a suffix for every prefix of the pattern, allowing the algorithm to skip redundant comparisons without backtracking.",
    "difficulty": "Advanced"
  },
  {
    "id": 84,
    "question": "What distinguishes a 'Persistent' data structure from an ephemeral one?",
    "options": [
      "It preserves all previous versions of itself upon modification",
      "It is stored on non-volatile storage media",
      "It is thread-safe by design",
      "It uses a hash map for O(1) access times"
    ],
    "answer": "It preserves all previous versions of itself upon modification",
    "explanation": "Persistent data structures maintain access to all previous states. When an update occurs, instead of modifying the existing structure in-place, new nodes are created, sharing unchanged parts with the old version (path copying).",
    "difficulty": "Advanced"
  },
  {
    "id": 85,
    "question": "In the Fast Fourier Transform (FFT) based polynomial multiplication, what is the primary operation used to transform coefficients to point-value representations efficiently?",
    "options": [
      "Divide and Conquer using the roots of unity",
      "Dynamic Programming on the coefficient matrix",
      "Greedy selection of coefficients",
      "Modular arithmetic with prime fields"
    ],
    "answer": "Divide and Conquer using the roots of unity",
    "explanation": "FFT exploits the symmetry of the roots of unity to recursively break down the polynomial evaluation into smaller sub-problems (DFT), reducing the complexity from O(N^2) naive multiplication to O(N log N).",
    "difficulty": "Advanced"
  },
  {
    "id": 86,
    "question": "Which technique reduces the problem of finding the Strongly Connected Components (SCC) in a directed graph to finding Connected Components in an undirected graph?",
    "options": [
      "Kosaraju's Algorithm",
      "Prim's Algorithm",
      "Kruskal's Algorithm",
      "BFS Coloring"
    ],
    "answer": "Kosaraju's Algorithm",
    "explanation": "Kosaraju's algorithm runs DFS to order vertices, then transposes the graph (reverses edges), and processes nodes in reverse order of finishing times. In the transposed graph, DFS trees correspond to SCCs.",
    "difficulty": "Advanced"
  },
  {
    "id": 87,
    "question": "What is the worst-case time complexity of the QuickSort algorithm using the 'median-of-three' pivot selection strategy?",
    "options": [
      "O(N^2)",
      "O(N log N)",
      "O(N)",
      "O(N^1.5)"
    ],
    "answer": "O(N^2)",
    "explanation": "While median-of-three improves average performance and protects against sorted input, it does not strictly guarantee O(N log N) worst-case complexity. An adversary can still force O(N^2) behavior (e.g., by forcing the median to be the smallest element).",
    "difficulty": "Advanced"
  },
  {
    "id": 88,
    "question": "Which algorithm is most efficient for finding the shortest path from a single source to all other nodes in a graph containing negative edge weights but no negative cycles?",
    "options": [
      "Bellman-Ford Algorithm",
      "Dijkstra's Algorithm",
      "A* Search",
      "Floyd-Warshall Algorithm"
    ],
    "answer": "Bellman-Ford Algorithm",
    "explanation": "Dijkstra fails with negative weights. The Bellman-Ford algorithm relaxes all edges V-1 times, ensuring the shortest path is found even with negative weights, and can also detect the presence of negative cycles.",
    "difficulty": "Advanced"
  },
  {
    "id": 89,
    "question": "In computational geometry, what is the primary application of the 'Monotone Chain' algorithm?",
    "options": [
      "Constructing the Convex Hull of a set of 2D points",
      "Triangulating a polygon",
      "Finding the closest pair of points",
      "Calculating the intersection of two line segments"
    ],
    "answer": "Constructing the Convex Hull of a set of 2D points",
    "explanation": "The Monotone Chain algorithm (or Andrew's algorithm) sorts points by x-coordinate and then builds the upper and lower hulls. It is a simple and robust O(N log N) method for computing the convex hull.",
    "difficulty": "Advanced"
  },
  {
    "id": 90,
    "question": "What is the amortized time complexity for a sequence of operations on a disjoint-set (Union-Find) data structure using Union by Rank and Path Compression?",
    "options": [
      "Inverse Ackermann function α(N)",
      "O(log N)",
      "O(1)",
      "O(log* N)"
    ],
    "answer": "Inverse Ackermann function α(N)",
    "explanation": "The combination of Union by Rank and Path Compression results in an amortized time complexity of O(α(N)), where α is the inverse Ackermann function. For all practical values of N, α(N) is less than 5, effectively constant.",
    "difficulty": "Advanced"
  },
  {
    "id": 91,
    "question": "Which property of the modulo operator is critical for the correctness of Modular Exponentiation (calculating a^b % m)?",
    "options": [
      "(a * b) % m = [(a % m) * (b % m)] % m",
      "(a + b) % m = (a % m) + (b % m)",
      "(a - b) % m = (a % m) - (b % m)",
      "(a / b) % m = (a % m) / (b % m)"
    ],
    "answer": "(a * b) % m = [(a % m) * (b % m)] % m",
    "explanation": "Modular multiplication distributes over modulo. This property allows the algorithm to compute powers (which are repeated multiplications) without overflow by taking the modulus at every intermediate step.",
    "difficulty": "Advanced"
  },
  {
    "id": 92,
    "question": "In the context of 'Centroid Decomposition', what ensures that the depth of the recursion tree is O(log N)?",
    "options": [
      "The centroid splits the tree into subtrees of at most N/2 size",
      "The centroid is always a leaf node",
      "The centroid is the node with the maximum degree",
      "The centroid connects all nodes in the shortest path"
    ],
    "answer": "The centroid splits the tree into subtrees of at most N/2 size",
    "explanation": "By definition, removing a centroid breaks the tree into connected components (subtrees) where each component has at most N/2 nodes. This size reduction guarantees a logarithmic recursion depth.",
    "difficulty": "Advanced"
  },
  {
    "id": 93,
    "question": "What is the primary application of the 'Hungarian Algorithm'?",
    "options": [
      "Solving the Assignment Problem in weighted bipartite graphs",
      "Finding the shortest path in a graph with negative weights",
      "Determining the Maximum Flow in a network",
      "Computing the Minimum Spanning Tree"
    ],
    "answer": "Solving the Assignment Problem in weighted bipartite graphs",
    "explanation": "The Hungarian Algorithm (or Kuhn's algorithm for unweighted) solves the assignment problem: finding the maximum weight matching in a weighted bipartite graph in polynomial time (O(N^3)).",
    "difficulty": "Advanced"
  },
  {
    "id": 94,
    "question": "Which of the following statements best describes the 'Meet-in-the-Middle' optimization technique?",
    "options": [
      "It splits the input into two halves, computes all possibilities for each, and combines the results",
      "It uses a hash table to store intermediate results of overlapping subproblems",
      "It divides the problem size by 2 at every step like merge sort",
      "It meets the search pointers in the middle of a sorted array"
    ],
    "answer": "It splits the input into two halves, computes all possibilities for each, and combines the results",
    "explanation": "Meet-in-the-Middle is a technique for problems where input size is too small for exponential 2^N but too large for direct brute force. It splits N into N/2 and N/2, computing 2^(N/2) states for each and combining them, typically for subset sum or encryption problems.",
    "difficulty": "Advanced"
  },
  {
    "id": 95,
    "question": "What is the function of the 'Residual Graph' in Max-Flow algorithms?",
    "options": [
      "It represents the remaining capacity of edges and allows flow to be undone",
      "It stores the shortest path distances from source to sink",
      "It maintains the set of nodes currently saturated by flow",
      "It visualizes the cut where the flow is maximum"
    ],
    "answer": "It represents the remaining capacity of edges and allows flow to be undone",
    "explanation": "The residual graph maintains capacities for how much more flow can be pushed (forward edges) and how much flow can be cancelled (backward edges), enabling algorithms to find augmenting paths and optimize the total flow.",
    "difficulty": "Advanced"
  },
  {
    "id": 96,
    "question": "What is the output of 'Topological Sorting' applied to a Directed Acyclic Graph (DAG)?",
    "options": [
      "A linear ordering of vertices such that for every directed edge u -> v, u comes before v",
      "A grouping of vertices by their indegree count",
      "A list of vertices sorted by their distance from the source node",
      "A hierarchical tree structure representing parent-child relationships"
    ],
    "answer": "A linear ordering of vertices such that for every directed edge u -> v, u comes before v",
    "explanation": "Topological sort produces a linear sequence where every directed edge goes from an earlier node to a later node. This is only possible if the graph has no directed cycles.",
    "difficulty": "Advanced"
  },
  {
    "id": 97,
    "question": "Which data structure is theoretically optimal for implementing a Priority Queue where arbitrary element deletions are frequent?",
    "options": [
      "Fibonacci Heap",
      "Binary Heap",
      "Unsorted Array",
      "Balanced BST (e.g., AVL Tree)"
    ],
    "answer": "Fibonacci Heap",
    "explanation": "While a Binary Heap is common, a Fibonacci Heap provides amortized O(1) insert and decrease-key, and O(log N) delete-min. Crucially, deleting an arbitrary node is O(log N), making it efficient for complex operations.",
    "difficulty": "Advanced"
  },
  {
    "id": 98,
    "question": "In the Rabin-Karp string matching algorithm, what technique is used to compute the hash of the next window in O(1) time?",
    "options": [
      "Rolling Hash",
      "Double Hashing",
      "Universal Hashing",
      "Perfect Hashing"
    ],
    "answer": "Rolling Hash",
    "explanation": "The rolling hash technique utilizes the hash value of the previous window to compute the hash of the next window. It subtracts the contribution of the outgoing character and adds the contribution of the incoming character using modulo arithmetic.",
    "difficulty": "Advanced"
  },
  {
    "id": 99,
    "question": "What is the worst-case time complexity of the 'Sieve of Eratosthenes' algorithm to find all primes up to N?",
    "options": [
      "O(N log log N)",
      "O(N log N)",
      "O(N)",
      "O(N^2)"
    ],
    "answer": "O(N log log N)",
    "explanation": "The complexity is derived from the harmonic series sum of 1/p for primes p up to N. While the nested loops suggest O(N^2), the marking operations decrease rapidly, converging to N * log log N.",
    "difficulty": "Advanced"
  }
]