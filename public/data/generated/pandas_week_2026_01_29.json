[
  {
    "id": 1,
    "question": "Which Python library is Pandas tightly integrated with for numerical operations?",
    "options": [
      "Matplotlib",
      "NumPy",
      "Scikit-Learn",
      "TensorFlow"
    ],
    "answer": "NumPy",
    "explanation": "Pandas is built on top of NumPy and uses it for high-performance numerical computing and array operations. Matplotlib is for visualization, and Scikit-Learn is for machine learning.",
    "difficulty": "Beginner"
  },
  {
    "id": 2,
    "question": "What is the primary purpose of using Matplotlib in conjunction with Pandas?",
    "options": [
      "To perform complex numerical calculations",
      "To create visualizations and charts",
      "To clean missing data",
      "To train machine learning models"
    ],
    "answer": "To create visualizations and charts",
    "explanation": "Matplotlib is the standard library in Python for creating static, interactive, and animated visualizations. Pandas integrates with it to allow users to plot data directly from DataFrames.",
    "difficulty": "Beginner"
  },
  {
    "id": 3,
    "question": "Which of the following is NOT listed as a typical type of data cleaning in Pandas?",
    "options": [
      "Replace values",
      "Fill in missing data",
      "Create new visualizations",
      "Fix column names"
    ],
    "answer": "Create new visualizations",
    "explanation": "Creating visualizations is part of data analysis or presentation, not data cleaning. The text lists replacing values, filling missing data, changing formats, fixing column names, and handling outliers as cleaning tasks.",
    "difficulty": "Beginner"
  },
  {
    "id": 4,
    "question": "According to the text, Pandas sits at the intersection of what two concepts?",
    "options": [
      "Database management and web development",
      "The convenience of spreadsheets and complex programming languages",
      "Hardware acceleration and cloud storage",
      "Natural language processing and image recognition"
    ],
    "answer": "The convenience of spreadsheets and complex programming languages",
    "explanation": "Pandas bridges the gap between the ease of use found in spreadsheet applications (like Excel) and the power and complexity of programming languages like R or Python.",
    "difficulty": "Beginner"
  },
  {
    "id": 5,
    "question": "What is a key benefit of 'Nullable dtypes' introduced in modern Pandas?",
    "options": [
      "They make code run slower",
      "They prevent the use of integers",
      "They provide a cleaner and safer way to handle missing data",
      "They are only used for text data"
    ],
    "answer": "They provide a cleaner and safer way to handle missing data",
    "explanation": "Nullable dtypes allow for missing values (NA) in data types that previously couldn't hold them natively (like integers), leading to safer and more consistent data handling.",
    "difficulty": "Beginner"
  },
  {
    "id": 6,
    "question": "Which industry is explicitly mentioned as a principal instrument for Pandas usage?",
    "options": [
      "Video game development",
      "Fashion design",
      "Finance",
      "Civil engineering"
    ],
    "answer": "Finance",
    "explanation": "The text specifically lists finance, along with healthcare, marketing analytics, and research, as industries where Pandas is a principal instrument.",
    "difficulty": "Beginner"
  },
  {
    "id": 7,
    "question": "What does the 'filter' operation allow you to do in data transformation?",
    "options": [
      "Combine two different tables",
      "Select specific rows based on criteria",
      "Change the data type of a column",
      "Fill in missing values"
    ],
    "answer": "Select specific rows based on criteria",
    "explanation": "Filtering is the process of viewing only the rows that meet specific conditions, allowing you to narrow down a dataset to the most relevant information.",
    "difficulty": "Beginner"
  },
  {
    "id": 8,
    "question": "In the context of time-series analysis, what does 'resampling' refer to?",
    "options": [
      "Changing the frequency of your time-series data (e.g., daily to monthly)",
      "Removing all time data from the dataset",
      "Renaming the columns of the dataset",
      "Converting strings to integers"
    ],
    "answer": "Changing the frequency of your time-series data (e.g., daily to monthly)",
    "explanation": "Resampling is a powerful technique in time-series analysis that allows you to convert data from one time frequency to another, such as aggregating daily data into weekly averages.",
    "difficulty": "Beginner"
  },
  {
    "id": 9,
    "question": "Which operation is used to combine two tables together in Pandas?",
    "options": [
      "Filtering",
      "Merging",
      "Pivoting",
      "Grouping"
    ],
    "answer": "Merging",
    "explanation": "Merging allows you to join two distinct DataFrames (tables) based on shared columns or indices, similar to SQL JOINs.",
    "difficulty": "Beginner"
  },
  {
    "id": 10,
    "question": "What is the primary purpose of fixing column names during data cleaning?",
    "options": [
      "To increase the file size",
      "To make them consistent and easier to reference in code",
      "To convert all data to numbers",
      "To delete the rows"
    ],
    "answer": "To make them consistent and easier to reference in code",
    "explanation": "Consistent column names are essential for writing clean, error-free code. Fixing them removes spaces, special characters, or inconsistencies that might cause errors during analysis.",
    "difficulty": "Beginner"
  },
  {
    "id": 11,
    "question": "How does Pandas support machine learning workflows?",
    "options": [
      "By directly creating neural network architectures",
      "By providing ready datasets that can be fed into ML pipelines",
      "By replacing the need for training data",
      "By writing the final software application"
    ],
    "answer": "By providing ready datasets that can be fed into ML pipelines",
    "explanation": "Pandas is used to clean and shape data so that it is in the correct format to be directly used by machine learning libraries like Scikit-Learn and TensorFlow.",
    "difficulty": "Beginner"
  },
  {
    "id": 12,
    "question": "What is the function of a 'rolling window' in time-series analysis?",
    "options": [
      "It calculates a statistic over a sliding window of time",
      "It creates a static chart",
      "It deletes old data points",
      "It converts time zones"
    ],
    "answer": "It calculates a statistic over a sliding window of time",
    "explanation": "Rolling windows are used to smooth out data or calculate moving averages by performing calculations (like mean or sum) over a specific subset of data that moves through the time series.",
    "difficulty": "Beginner"
  },
  {
    "id": 13,
    "question": "Why is cleaning data considered a prerequisite for analysis?",
    "options": [
      "Because unclean data is too small",
      "Because it ensures you form reliable datasets that won't cause errors",
      "Because it creates more columns",
      "Because it makes the data look colorful"
    ],
    "answer": "Because it ensures you form reliable datasets that won't cause errors",
    "explanation": "Unclean data can lead to incorrect results or cause code to crash. Cleaning ensures the dataset is reliable and robust enough for subsequent analysis.",
    "difficulty": "Beginner"
  },
  {
    "id": 14,
    "question": "What is the main advantage of the 'modern string dtype' in Pandas?",
    "options": [
      "It is slower but uses less memory",
      "It is incompatible with other libraries",
      "It is faster, safer, and Arrow-friendly",
      "It automatically translates languages"
    ],
    "answer": "It is faster, safer, and Arrow-friendly",
    "explanation": "The modern string dtype provides performance improvements and safety features, leveraging the Apache Arrow format for better efficiency compared to older object-dtype strings.",
    "difficulty": "Beginner"
  },
  {
    "id": 15,
    "question": "What does 'shifting' dates in a time-series allow you to do?",
    "options": [
      "Change the calendar year",
      "Move data points forward or backward in time to compare past and present",
      "Delete the date column entirely",
      "Randomize the order of rows"
    ],
    "answer": "Move data points forward or backward in time to compare past and present",
    "explanation": "Shifting moves data points by a specified number of time periods, which is useful for calculating changes over time or comparing current values with previous ones (lag).",
    "difficulty": "Beginner"
  },
  {
    "id": 16,
    "question": "Which data cleaning task involves changing a column's data type from text to a number?",
    "options": [
      "Handling outliers",
      "Changing the format of columns",
      "Merging tables",
      "Pivoting data"
    ],
    "answer": "Changing the format of columns",
    "explanation": "Data often comes in as text (strings) even if it represents numbers. Changing the format (type conversion) allows you to perform mathematical operations on that data.",
    "difficulty": "Beginner"
  },
  {
    "id": 17,
    "question": "What is the result of using the 'pivot' transformation?",
    "options": [
      "Rows become columns and unique values are reshaped into a new format",
      "The dataset is deleted",
      "All data is sorted by date",
      "Missing values are ignored"
    ],
    "answer": "Rows become columns and unique values are reshaped into a new format",
    "explanation": "Pivoting reshapes data by turning unique values from one column into multiple new columns, allowing you to reorganize data for better comparison or reporting.",
    "difficulty": "Beginner"
  },
  {
    "id": 18,
    "question": "What is one way Pandas helps with effective decision making?",
    "options": [
      "By guessing the results without data",
      "By shaping and cleaning data in a reliable manner",
      "By printing physical reports",
      "By removing the need for human input"
    ],
    "answer": "By shaping and cleaning data in a reliable manner",
    "explanation": "Reliable data cleaning and shaping are prerequisites for analysis. Pandas ensures that the data fed into decision-making processes is accurate and consistent.",
    "difficulty": "Beginner"
  },
  {
    "id": 19,
    "question": "Which library works seamlessly with Pandas to provide machine learning algorithms?",
    "options": [
      "NumPy",
      "Matplotlib",
      "Scikit-Learn",
      "PyGame"
    ],
    "answer": "Scikit-Learn",
    "explanation": "Scikit-Learn is the standard library for machine learning in Python and is designed to work directly with Pandas DataFrames for model training and prediction.",
    "difficulty": "Beginner"
  },
  {
    "id": 20,
    "question": "What does 'handling outliers' mean during data cleaning?",
    "options": [
      "Deleting the dataset",
      "Identifying and managing data points that are significantly different from the rest",
      "Renaming all columns",
      "Converting all text to uppercase"
    ],
    "answer": "Identifying and managing data points that are significantly different from the rest",
    "explanation": "Outliers are extreme values that can skew analysis. Handling them involves identifying these anomalies and deciding whether to remove, cap, or adjust them.",
    "difficulty": "Beginner"
  },
  {
    "id": 21,
    "question": "Which technique is widely used in financial analysis and energy consumption forecasting?",
    "options": [
      "Time-series analytics",
      "Image processing",
      "Sound wave editing",
      "3D modeling"
    ],
    "answer": "Time-series analytics",
    "explanation": "Time-series analytics involves analyzing data points collected over time, which is crucial for tracking stock prices (finance) or usage patterns (energy) to predict future trends.",
    "difficulty": "Beginner"
  },
  {
    "id": 22,
    "question": "What is the benefit of 'grouping' data in Pandas?",
    "options": [
      "It makes the file size larger",
      "It allows you to split data into categories to analyze each group separately",
      "It prevents you from sorting the data",
      "It automatically deletes duplicate rows"
    ],
    "answer": "It allows you to split data into categories to analyze each group separately",
    "explanation": "Grouping aggregates data based on categories (e.g., sales by region), allowing you to compute statistics like sums or averages for each specific group.",
    "difficulty": "Beginner"
  },
  {
    "id": 23,
    "question": "Why is Pandas described as 'fast' for data science performance?",
    "options": [
      "Because it uses human input for calculations",
      "Because it converts raw data into features and statistics in a few lines of code",
      "Because it only works on very small datasets",
      "Because it avoids using the computer's memory"
    ],
    "answer": "Because it converts raw data into features and statistics in a few lines of code",
    "explanation": "Pandas is optimized for performance, allowing complex data manipulations that would take hours in manual processes to be executed almost instantly with concise code.",
    "difficulty": "Beginner"
  },
  {
    "id": 24,
    "question": "In the context of data transformation, what does 'selecting columns' achieve?",
    "options": [
      "It reduces the dataset to only the variables you care about",
      "It creates a backup of the file",
      "It merges the dataset with another",
      "It changes the data type"
    ],
    "answer": "It reduces the dataset to only the variables you care about",
    "explanation": "Selecting columns allows you to focus on specific attributes or features of the data, removing irrelevant information and simplifying the analysis.",
    "difficulty": "Beginner"
  },
  {
    "id": 25,
    "question": "What is the first step typically required when working with time data stored as text strings?",
    "options": [
      "Deleting the text",
      "Date parsing",
      "Merging with another table",
      "Applying a rolling window"
    ],
    "answer": "Date parsing",
    "explanation": "Date parsing converts text strings (like '2023-01-01') into actual datetime objects, enabling Pandas to perform time-specific operations like filtering by month or calculating duration.",
    "difficulty": "Beginner"
  },
  {
    "id": 26,
    "question": "How does Pandas facilitate the path to automation?",
    "options": [
      "By writing code for you automatically",
      "By providing ready datasets that can be processed repeatedly by scripts",
      "By connecting to the internet",
      "By generating random numbers"
    ],
    "answer": "By providing ready datasets that can be processed repeatedly by scripts",
    "explanation": "Once a data cleaning and manipulation script is written using Pandas, it can be run automatically on new data, automating the workflow.",
    "difficulty": "Beginner"
  },
  {
    "id": 27,
    "question": "What is the purpose of 'filling in missing data'?",
    "options": [
      "To add random numbers to the dataset",
      "To ensure the dataset is complete and analysis can proceed without gaps",
      "To increase the complexity of the data",
      "To remove all duplicate rows"
    ],
    "answer": "To ensure the dataset is complete and analysis can proceed without gaps",
    "explanation": "Missing data can cause errors or bias in analysis. Filling it (imputation) ensures the dataset is continuous and ready for processing.",
    "difficulty": "Beginner"
  },
  {
    "id": 28,
    "question": "Which feature is emphasized as a 'new default' for safer data handling in 2026?",
    "options": [
      "Integer-only arrays",
      "Nullable dtypes",
      "Fixed-width strings",
      "Binary objects"
    ],
    "answer": "Nullable dtypes",
    "explanation": "The text highlights nullable dtypes as the new default for clean and safe data handling, addressing previous limitations where missing values were handled inconsistently.",
    "difficulty": "Beginner"
  },
  {
    "id": 29,
    "question": "What does 'replacing values' help achieve in data cleaning?",
    "options": [
      "It changes the data structure",
      "It corrects errors or standardizes categories (e.g., replacing 'N/A' with 'Missing')",
      "It creates a new chart",
      "It increases the number of rows"
    ],
    "answer": "It corrects errors or standardizes categories (e.g., replacing 'N/A' with 'Missing')",
    "explanation": "Replacing values is used to correct typos, standardize formatting, or map values to a consistent set, improving the quality of the dataset.",
    "difficulty": "Beginner"
  },
  {
    "id": 30,
    "question": "Which of the following is a use case for time-series support in Pandas?",
    "options": [
      "Customer behavior analysis",
      "Image recognition",
      "Sound editing",
      "Website layout design"
    ],
    "answer": "Customer behavior analysis",
    "explanation": "The text explicitly mentions customer behavior, finance, forecasting, and energy consumption as fields where time-series support is useful.",
    "difficulty": "Beginner"
  },
  {
    "id": 31,
    "question": "What is the role of Pandas in the Python ecosystem compared to a spreadsheet?",
    "options": [
      "It is less convenient than a spreadsheet",
      "It offers the convenience of a spreadsheet but with programming power",
      "It is only used for storing files",
      "It cannot perform mathematical operations"
    ],
    "answer": "It offers the convenience of a spreadsheet but with programming power",
    "explanation": "Pandas provides a familiar tabular structure like a spreadsheet but adds the ability to automate, scale, and programmatically manipulate data.",
    "difficulty": "Beginner"
  },
  {
    "id": 32,
    "question": "Why is it important to change the format of columns from string to number?",
    "options": [
      "Strings take up too much memory",
      "Mathematical operations cannot be performed on strings",
      "Numbers cannot be read by humans",
      "It makes the data look messy"
    ],
    "answer": "Mathematical operations cannot be performed on strings",
    "explanation": "If numbers are stored as text (strings), you cannot add, average, or perform other calculations on them. Converting them to numeric types is necessary for quantitative analysis.",
    "difficulty": "Beginner"
  },
  {
    "id": 33,
    "question": "What does the term 'Data Transformation' generally refer to?",
    "options": [
      "Deleting the raw data source",
      "Molding the narrative by reshaping data to answer specific questions",
      "Creating a backup on a USB drive",
      "Printing the data on paper"
    ],
    "answer": "Molding the narrative by reshaping data to answer specific questions",
    "explanation": "Data transformation involves changing the structure or format of data—filtering, grouping, pivoting—to make it suitable for answering specific business or research questions.",
    "difficulty": "Beginner"
  },
  {
    "id": 34,
    "question": "How does Pandas improve 'Data Science Performance'?",
    "options": [
      "By requiring a team of 10 people to run it",
      "By converting hours of work into a few lines of efficient code",
      "By slowing down the computer to ensure accuracy",
      "By manually typing all results"
    ],
    "answer": "By converting hours of work into a few lines of efficient code",
    "explanation": "Pandas optimizes operations, allowing complex tasks that would take hours to be performed instantly with concise, efficient code.",
    "difficulty": "Beginner"
  },
  {
    "id": 35,
    "question": "What is the ultimate goal of the data cleaning and manipulation process in Pandas?",
    "options": [
      "To make the data as complicated as possible",
      "To draw insights and answer questions from raw data",
      "To delete all the original data",
      "To create more files than necessary"
    ],
    "answer": "To draw insights and answer questions from raw data",
    "explanation": "While cleaning and transforming are steps, the end goal of using Pandas is to process raw data into a format that reveals patterns, answers questions, and supports decision-making.",
    "difficulty": "Beginner"
  },
  {
    "id": 36,
    "question": "In the context of Pandas 2026 best practices, why might you choose the nullable integer dtype (e.g., 'Int64') over the standard legacy integer dtype ('int64')?",
    "options": [
      "To automatically convert all floating-point numbers to integers",
      "To allow for missing values (NaN) in an integer column without converting the column to float",
      "To increase the arithmetic calculation speed by utilizing GPU acceleration",
      "To enforce strict sorting order on the DataFrame index"
    ],
    "answer": "To allow for missing values (NaN) in an integer column without converting the column to float",
    "explanation": "Standard NumPy-based integer dtypes cannot hold missing values; they force a cast to float64 if NaN is introduced. The nullable 'Int64' dtype uses a underlying bitmask to represent missing data natively, preserving the integer type. This is crucial for data integrity in datasets like IDs or counts where zeros are distinct from missing values. Option A is incorrect as it describes type coercion, not nullability. Option C is incorrect as dtypes do not inherently enable GPU usage. Option D is unrelated to dtype capabilities.",
    "difficulty": "Intermediate"
  },
  {
    "id": 37,
    "question": "When analyzing time-series data for financial forecasting, what is the primary benefit of using the 'shift' method in Pandas?",
    "options": [
      "It changes the frequency of the data index to a lower granularity (downsampling)",
      "It moves data points forward or backward in time to create lagged features for comparison",
      "It fills missing date values by interpolating between existing timestamps",
      "It converts the index to a DatetimeIndex automatically if it is currently a string"
    ],
    "answer": "It moves data points forward or backward in time to create lagged features for comparison",
    "explanation": "The 'shift' method is specifically designed to move the data by a specified number of periods. This is essential for calculating period-over-period growth (e.g., comparing today's stock price to yesterday's) without altering the index or data frequency. Option A describes 'resample' or 'asfreq'. Option C describes 'interpolate'. Option D describes 'to_datetime'.",
    "difficulty": "Intermediate"
  },
  {
    "id": 38,
    "question": "You are working with a large dataset containing customer reviews. You need to clean the data by standardizing the text format. Which approach aligns best with Pandas 2026 recommendations for string handling?",
    "options": [
      "Convert columns to the 'object' dtype and apply standard Python string methods via a lambda loop",
      "Use the modern 'string' dtype which is Arrow-compatible and supports missing data natively",
      "Immediately convert all text columns to categorical dtype to save memory before cleaning",
      "Utilize NumPy vectorized operations on the underlying byte arrays"
    ],
    "answer": "Use the modern 'string' dtype which is Arrow-compatible and supports missing data natively",
    "explanation": "Pandas 2026 emphasizes the 'string' dtype because it offers consistent handling of missing data (using pd.NA instead of np.nan) and better performance through integration with Apache Arrow. It is safer and more efficient than the legacy 'object' dtype for text. Option A is slow and error-prone regarding missing values. Option C is premature before cleaning and only helps for low-cardinality data. Option D is not how Pandas handles string operations.",
    "difficulty": "Intermediate"
  },
  {
    "id": 39,
    "question": "When preparing a DataFrame for a machine learning pipeline using Scikit-Learn, why is it critical to address missing data and categorical strings before feeding the data into the model?",
    "options": [
      "Scikit-Learn estimators generally do not accept missing values (NaN) or non-numeric strings directly",
      "Pandas will automatically drop rows containing missing values during the export process",
      "Machine learning models require categorical strings to calculate standard deviation",
      "The training process is slower if the DataFrame contains any text-based columns"
    ],
    "answer": "Scikit-Learn estimators generally do not accept missing values (NaN) or non-numeric strings directly",
    "explanation": "Most Scikit-Learn algorithms (like LinearRegression or RandomForest) expect a numeric matrix without NaN values. Failure to impute missing values or encode strings (e.g., via One-Hot Encoding) will result in errors during the `.fit()` process. Option B is false; export does not auto-clean. Option C is incorrect; strings must be encoded, not used for raw stat calc. Option D is a secondary concern; the primary blocker is compatibility.",
    "difficulty": "Intermediate"
  },
  {
    "id": 40,
    "question": "You need to analyze energy consumption data recorded hourly but want to visualize daily trends. Which Pandas time-series technique should you apply to change the frequency of your data?",
    "options": [
      "The shift() method",
      "The groupby() method on the hour column",
      "The resample() method",
      "The melt() method"
    ],
    "answer": "The resample() method",
    "explanation": "The `resample()` method is specifically designed for time-series data to convert the frequency of the time index (e.g., from hourly to daily) and aggregate the data (e.g., sum or mean) accordingly. It is the standard way to handle frequency conversion in Pandas. Option A moves data points but doesn't change frequency. Option B is possible but less efficient and requires extracting the hour manually. Option D is for reshaping wide to long formats.",
    "difficulty": "Intermediate"
  },
  {
    "id": 41,
    "question": "What is the primary trade-off to consider when converting a column with low cardinality (few unique values) from the 'object' or 'string' dtype to 'category' dtype?",
    "options": [
      "Category dtype increases memory usage significantly but makes sorting faster",
      "Category dtype uses less memory but makes operations like replacing values computationally more expensive",
      "Category dtype prevents any form of aggregation on that column",
      "Category dtype automatically converts all values to integers for display purposes"
    ],
    "answer": "Category dtype uses less memory but makes operations like replacing values computationally more expensive",
    "explanation": "While 'category' dtype provides substantial memory savings for columns with repetitive strings, it stores data as integers with a lookup table. Operations that involve new values not in the categories (like string replacement or concatenation) can be slower because they require updating the mapping. Option A is incorrect because it reduces memory. Option C is incorrect; you can aggregate categorical data. Option D is incorrect regarding display.",
    "difficulty": "Intermediate"
  },
  {
    "id": 42,
    "question": "When merging two DataFrames on a common key, which join type ensures that you retain all records from the left DataFrame, regardless of whether there is a match in the right DataFrame?",
    "options": [
      "Inner join",
      "Outer join",
      "Left join",
      "Cross join"
    ],
    "answer": "Left join",
    "explanation": "A Left join prioritizes the keys in the left DataFrame. If a key in the left DataFrame does not exist in the right DataFrame, the result will still contain that row, with columns from the right DataFrame filled with NaN. An Inner join drops non-matches. An Outer join keeps all from both. A Cross join creates the Cartesian product.",
    "difficulty": "Intermediate"
  },
  {
    "id": 43,
    "question": "You are tasked with identifying 'outliers' in a numerical column of sales data to clean the dataset. Which of the following approaches uses a standard statistical method for detecting outliers?",
    "options": [
      "Filtering rows where the value is equal to the column mean",
      "Using the Interquartile Range (IQR) to filter values outside 1.5 * IQR",
      "Replacing all negative values with zero",
      "Dropping all duplicate rows in the dataset"
    ],
    "answer": "Using the Interquartile Range (IQR) to filter values outside 1.5 * IQR",
    "explanation": "Using the IQR is a common statistical method for outlier detection. Values falling below Q1 - 1.5*IQR or above Q3 + 1.5*IQR are typically considered outliers. Option A identifies the average, not anomalies. Option C is data sanitization, not outlier detection. Option D handles redundancy, not statistical outliers.",
    "difficulty": "Intermediate"
  },
  {
    "id": 44,
    "question": "In the context of Pandas method chaining, what is the main disadvantage of using 'inplace=True' for operations like `dropna` or `fillna`?",
    "options": [
      "It returns a new DataFrame, consuming double the memory",
      "It prevents you from chaining multiple methods together in a single expression",
      "It automatically converts all integers to floating-point numbers",
      "It cannot be used on MultiIndex DataFrames"
    ],
    "answer": "It prevents you from chaining multiple methods together in a single expression",
    "explanation": "Methods with `inplace=True` usually return `None`. Therefore, you cannot call a subsequent method on the result (e.g., `df.dropna(inplace=True).sort_values()` will fail). Method chaining is often preferred for readability and creating pipelines. Option A is incorrect because inplace modifies the existing object, theoretically saving memory compared to a copy (though the application often holds references). Option C and D are false.",
    "difficulty": "Intermediate"
  },
  {
    "id": 45,
    "question": "Why is it often recommended to define specific dtypes (like 'int32' instead of 'int64' or 'category') when reading a large CSV file using `pd.read_csv()`?",
    "options": [
      "To automatically correct spelling errors in the header row",
      "To parse dates faster than the 'parse_dates' argument",
      "To significantly reduce memory usage by allocating only necessary space for the data",
      "To ensure the DataFrame is sorted by the index immediately upon loading"
    ],
    "answer": "To significantly reduce memory usage by allocating only necessary space for the data",
    "explanation": "Default dtypes are often 'int64' or 'float64', which may be overkill for your data range. Downcasting to smaller types (e.g., 'int32', 'float32') or using 'category' for strings reduces the memory footprint, allowing larger datasets to fit into RAM. Option A is unrelated to dtypes. Option B is handled by specific date arguments. Option D is a sorting operation, not a dtype property.",
    "difficulty": "Intermediate"
  },
  {
    "id": 46,
    "question": "You want to create a new column based on a complex conditional logic applied to existing columns. While `np.where` is an option, what is the more readable, 'Pandas-native' method designed for element-wise conditional logic?",
    "options": [
      "The apply() method with a custom function",
      "The numpy.select() function",
      "The cut() method",
      "The where() method or mask() method"
    ],
    "answer": "The where() method or mask() method",
    "explanation": "Pandas provides `where()` (keep values where condition is True, replace otherwise) and `mask()` (keep values where condition is False) as vectorized, readable methods for conditional logic. While `np.where` works, the Pandas methods align better with the DataFrame/Series API. Option A is slower. Option B is NumPy-based. Option C is for binning numerical data into intervals.",
    "difficulty": "Intermediate"
  },
  {
    "id": 47,
    "question": "When performing a 'groupby' operation, what is the key difference between the `transform` and `apply` functions?",
    "options": [
      "transform is faster but only works on numeric data, while apply works on all data types",
      "transform returns a Series/DataFrame with the same shape as the input, while apply can return arbitrary shapes",
      "apply is used for aggregation, while transform is only used for filtering",
      "There is no difference; they are aliases for the same function"
    ],
    "answer": "transform returns a Series/DataFrame with the same shape as the input, while apply can return arbitrary shapes",
    "explanation": "`transform` is designed to return a result that has the same index as the input, allowing you to easily add grouped statistics (like a group mean) back to the original DataFrame. `apply` is more flexible and can return scalars (for aggregation), Series, or DataFrames of different shapes. Option A is incorrect; both work on various types. Option C is incorrect; `apply` is the standard aggregation tool. Option D is false.",
    "difficulty": "Intermediate"
  },
  {
    "id": 48,
    "question": "Which operation is most efficient for converting a DataFrame from a 'wide' format (many columns representing years) to a 'long' format (rows representing years) suitable for time-series analysis or visualization tools?",
    "options": [
      "The merge() function",
      "The pivot_table() function",
      "The melt() function",
      "The stack() function"
    ],
    "answer": "The melt() function",
    "explanation": "The `melt()` function is specifically designed to unpivot a DataFrame from wide to long format, taking identifier columns and melting the rest into 'variable' and 'value' columns. While `stack()` can also do this, `melt()` is generally more intuitive for general column reshaping. `pivot_table` does the reverse (long to wide). `merge` joins tables.",
    "difficulty": "Intermediate"
  },
  {
    "id": 49,
    "question": "You encounter a 'SettingWithCopyWarning' in Pandas. What does this warning typically indicate about your code structure?",
    "options": [
      "You are trying to modify a view of a DataFrame derived from another DataFrame, potentially affecting the original data unintentionally",
      "You have run out of available RAM memory",
      "You are attempting to assign a string to an integer column",
      "You have used the wrong delimiter when reading a CSV file"
    ],
    "answer": "You are trying to modify a view of a DataFrame derived from another DataFrame, potentially affecting the original data unintentionally",
    "explanation": "The warning usually arises when chaining indexing operations (like `df[df['A'] > 0]['B'] = 10`) where Pandas cannot determine if you are modifying a copy or a view. It suggests that the assignment might fail or not propagate to the original DataFrame as expected. Using `.loc` is the standard fix. Options B, C, and D are unrelated to this specific warning.",
    "difficulty": "Intermediate"
  },
  {
    "id": 50,
    "question": "When working with time-series data, what is the function of `asfreq()` compared to `resample()`?",
    "options": [
      "`asfreq()` converts data to a specified frequency without any aggregation, filling or dropping non-matching timestamps",
      "`resample()` is strictly for upsampling, while `asfreq()` is for downsampling",
      "`asfreq()` automatically calculates the mean of the data points, while `resample()` only returns the count",
      "`resample()` cannot handle missing values, whereas `asfreq()` interpolates them by default"
    ],
    "answer": "`asfreq()` converts data to a specified frequency without any aggregation, filling or dropping non-matching timestamps",
    "explanation": "`asfreq()` is a simpler function that converts the TimeSeries to the specified frequency. It does not aggregate; it simply selects the data point at that frequency or introduces NaNs (unless a fill method is specified). `resample()` is a group-like operation that allows for flexible aggregation (sum, mean, etc.) over the new frequency bins. Option B is incorrect; both handle up/downsampling. Option C is incorrect; `resample` aggregates. Option D is incorrect; `resample` handles missing values via fillna/interpolation logic.",
    "difficulty": "Intermediate"
  },
  {
    "id": 51,
    "question": "In Pandas, what is the primary role of the `index` object beyond just row labels?",
    "options": [
      "It stores the data type information for the columns",
      "It enables fast lookups, data alignment, and relational operations like joining",
      "It is required for the DataFrame to be saved to a CSV file",
      "It limits the DataFrame to a maximum of 1000 rows"
    ],
    "answer": "It enables fast lookups, data alignment, and relational operations like joining",
    "explanation": "The index is critical for Pandas performance. It uses hash-based or sorted data structures to allow for O(1) or O(log n) lookups, ensures automatic alignment of data during arithmetic operations, and serves as the key for merges/joins. It is not merely a label. Options A, C, and D are incorrect functions.",
    "difficulty": "Intermediate"
  },
  {
    "id": 52,
    "question": "You need to calculate a 7-day rolling average for a stock price column to smooth out daily volatility. Which Pandas method should you use?",
    "options": [
      "expanding()",
      "ewm()",
      "rolling()",
      "shift()"
    ],
    "answer": "rolling()",
    "explanation": "The `rolling(window=7)` method creates a rolling window object that allows you to calculate moving statistics (like mean, sum, std) over a fixed-size window of data. `expanding()` grows the window from the start of the data. `ewm()` calculates exponentially weighted moving averages, giving more weight to recent points. `shift()` moves data points.",
    "difficulty": "Intermediate"
  },
  {
    "id": 53,
    "question": "When dealing with a MultiIndex (hierarchical index), what is the purpose of the `stack()` method?",
    "options": [
      "It rotates the lowest level of the column labels to become the innermost level of the row index",
      "It flattens all levels of the index into a single column",
      "It sorts the DataFrame based on the index levels",
      "It drops the outer index level"
    ],
    "answer": "It rotates the lowest level of the column labels to become the innermost level of the row index",
    "explanation": "`stack()` 'compresses' a level in the DataFrame's columns to produce a Series (or a DataFrame with a MultiIndex) with a new innermost row index. It effectively pivots wider data into a longer, indexed format. Option B describes a reset_index or flattening operation. Option C is sort_index. Option D is droplevel.",
    "difficulty": "Intermediate"
  },
  {
    "id": 54,
    "question": "You are performing a `merge` operation and notice duplicate column names (other than the key) in the resulting DataFrame. What does the `suffixes` parameter do?",
    "options": [
      "It drops the duplicate columns automatically",
      "It allows you to specify string suffixes (like '_x', '_y') to append to overlapping column names",
      "It concatenates the duplicate columns into a single column",
      "It renames the merge key columns"
    ],
    "answer": "It allows you to specify string suffixes (like '_x', '_y') to append to overlapping column names",
    "explanation": "When merging DataFrames with columns that have the same name (aside from the join key), Pandas appends default suffixes '_x' and '_y'. The `suffixes` parameter allows you to customize these (e.g., `suffixes=('_left', '_right')`) to make the column names more meaningful. It does not drop or concatenate the data.",
    "difficulty": "Intermediate"
  },
  {
    "id": 55,
    "question": "Which Pandas method is specifically designed to bin a continuous variable into discrete intervals (e.g., converting age to age groups)?",
    "options": [
      "qcut()",
      "cut()",
      "get_dummies()",
      "factorize()"
    ],
    "answer": "cut()",
    "explanation": "`cut()` is used to segment and sort data values into bins. It is best when you need to bin specific ranges (e.g., 0-18, 19-35, 36-60). `qcut()` is similar but bins based on quantiles (equal-sized bins). `get_dummies()` is for one-hot encoding. `factorize()` encodes values as integers.",
    "difficulty": "Intermediate"
  },
  {
    "id": 56,
    "question": "Why might you choose the `eval()` method in Pandas for performing complex arithmetic operations across multiple columns?",
    "options": [
      "It automatically handles missing data by forward-filling",
      "It can be faster and more memory-efficient by avoiding intermediate arrays for large datasets",
      "It converts the DataFrame into a SQL database for processing",
      "It is required for operations involving string concatenation"
    ],
    "answer": "It can be faster and more memory-efficient by avoiding intermediate arrays for large datasets",
    "explanation": "`eval()` uses NumExpr or similar engines to evaluate string expressions efficiently. By compiling the expression into a bytecode or using vectorized operations that minimize temporary storage, it can offer performance and memory benefits for very large DataFrames compared to standard Python/Pandas syntax evaluation. Options A, C, and D are incorrect.",
    "difficulty": "Intermediate"
  },
  {
    "id": 57,
    "question": "What is the implication of setting `errors='ignore'` when using the `to_datetime()` method to convert a column?",
    "options": [
      "It will raise an exception if any conversion fails",
      "It will return the original input (non-converted) for values that cannot be parsed",
      "It will drop all rows containing invalid dates",
      "It will automatically attempt to infer the format string"
    ],
    "answer": "It will return the original input (non-converted) for values that cannot be parsed",
    "explanation": "When `errors='ignore'` is set, invalid parsing (like a string 'N/A') will be returned as-is in the output object, rather than raising an error (default 'raise') or being set to NaT (Not a Time) with `errors='coerce'`. This preserves the original data on failure. Option A describes the default behavior. Option C describes a filtering step. Option D is `infer_datetime_format`.",
    "difficulty": "Intermediate"
  },
  {
    "id": 58,
    "question": "You have a DataFrame with a DatetimeIndex and you want to select all rows from the year 2023. Which of the following is the most idiomatic and efficient way to do this?",
    "options": [
      "df[df.index.year == 2023]",
      "df.loc['2023']",
      "df[df.index.str.startswith('2023')]",
      "df.query('index > 2023-01-01 and index < 2023-12-31')"
    ],
    "answer": "df.loc['2023']",
    "explanation": "Pandas supports partial string indexing for DatetimeIndex objects. `df.loc['2023']` automatically selects all rows falling within that year. This is generally optimized and more readable than boolean indexing with `year == 2023` or string manipulation. Option C fails because the index is datetime, not string. Option D is verbose and requires specific date strings.",
    "difficulty": "Intermediate"
  },
  {
    "id": 59,
    "question": "When using `pd.read_json()`, what is the primary difference between 'orient' settings like 'records' and 'index'?",
    "options": [
      "'records' expects the JSON to be a list of dicts, while 'index' expects a dict of dicts indexed by a key",
      "'records' is for reading files, while 'index' is for writing to files",
      "'records' parses strings faster than 'index'",
      "'index' automatically flattens nested JSON structures, while 'records' does not"
    ],
    "answer": "'records' expects the JSON to be a list of dicts, while 'index' expects a dict of dicts indexed by a key",
    "explanation": "The 'orient' parameter tells Pandas the structure of the incoming JSON. 'records' implies `[{col: val}, {col: val}]`. 'index' implies `{row_id: {col: val}}`. Choosing the wrong orient leads to parsing errors or empty DataFrames. Options B, C, and D are incorrect descriptions of the parameter's function.",
    "difficulty": "Intermediate"
  },
  {
    "id": 60,
    "question": "What is the function of the `pd.get_dummies()` utility in the context of Machine Learning preparation?",
    "options": [
      "It creates dummy variables for missing data points",
      "It converts categorical variables into a series of binary (0/1) columns (One-Hot Encoding)",
      "It generates synthetic data rows to balance the dataset",
      "It detects and removes dummy columns that have zero variance"
    ],
    "answer": "It converts categorical variables into a series of binary (0/1) columns (One-Hot Encoding)",
    "explanation": "One-Hot Encoding is essential for converting categorical text data (like 'Red', 'Blue', 'Green') into a format that machine learning algorithms can process (numerical matrices). `pd.get_dummies()` automates this by creating a new column for each category value with a 1 or 0 indicator.",
    "difficulty": "Intermediate"
  },
  {
    "id": 61,
    "question": "Which method is used to identify duplicate rows in a DataFrame, allowing you to mark them rather than immediately dropping them?",
    "options": [
      "drop_duplicates()",
      "duplicated()",
      "is_unique()",
      "has_duplicates()"
    ],
    "answer": "duplicated()",
    "explanation": "The `duplicated()` method returns a boolean Series indicating whether each row is a duplicate of a previous row. This allows for filtering or analysis before removal. `drop_duplicates()` removes them immediately. `is_unique` and `has_duplicates` are attributes of Index objects, not DataFrame methods.",
    "difficulty": "Intermediate"
  },
  {
    "id": 62,
    "question": "When plotting directly from a Pandas DataFrame using `df.plot()`, which library is actually being used under the hood to render the visualization?",
    "options": [
      "Seaborn",
      "Plotly",
      "Matplotlib",
      "Bokeh"
    ],
    "answer": "Matplotlib",
    "explanation": "Pandas plotting capabilities are a wrapper around Matplotlib. When you call `df.plot()`, Pandas creates a Matplotlib figure and axes object and plots the data on it, providing a convenient interface but relying on Matplotlib for the rendering engine. Options B, C, and D are separate libraries with their own APIs.",
    "difficulty": "Intermediate"
  },
  {
    "id": 63,
    "question": "You want to concatenate two DataFrames vertically (stacking rows). However, they have slightly different column names. Which parameter in `pd.concat` is crucial to ensure you retain all columns from both DataFrames, filling missing data with NaN?",
    "options": [
      "ignore_index",
      "join='inner'",
      "join='outer'",
      "keys"
    ],
    "answer": "join='outer'",
    "explanation": "By default, `pd.concat` uses `join='outer'`, which performs a union of all columns. If one DataFrame lacks a column present in the other, it fills those rows with NaN. Using `join='inner'` would intersect the columns, dropping any not shared by both. `ignore_index` only resets the row index. `keys` creates a hierarchical index.",
    "difficulty": "Intermediate"
  },
  {
    "id": 64,
    "question": "In a time-series DataFrame, you have missing dates (gaps in the DatetimeIndex). What is the correct approach to ensure every date in a range is present, filling missing data with NaN?",
    "options": [
      "Use the fillna(method='ffill') method",
      "Reindex the DataFrame using a complete date_range created with pd.date_range",
      "Sort the DataFrame by the index",
      "Drop the rows with missing dates to ensure consistency"
    ],
    "answer": "Reindex the DataFrame using a complete date_range created with pd.date_range",
    "explanation": "Reindexing allows you to conform a DataFrame to a new index. If the new index contains dates not present in the original data, Pandas introduces NaN (or a fill value) for those rows. `fillna` only fills existing NaNs, it doesn't add missing index entries. Sorting doesn't add missing dates. Dropping rows reduces data density.",
    "difficulty": "Intermediate"
  },
  {
    "id": 65,
    "question": "Why is it considered a best practice to use `.loc` rather than direct assignment (e.g., `df[df['col'] > 0] = 5`) when setting values in a DataFrame?",
    "options": [
      ".loc is the only method that works with string column names",
      "Direct assignment is deprecated in all Python versions after 3.6",
      ".loc ensures you are modifying a copy of the data rather than potentially acting on a view, avoiding the SettingWithCopyWarning",
      ".loc automatically converts integers to floats to prevent precision loss"
    ],
    "answer": ".loc ensures you are modifying a copy of the data rather than potentially acting on a view, avoiding the SettingWithCopyWarning",
    "explanation": "Chained assignment (e.g., `df[mask]['col'] = val`) creates an intermediate object (a view or copy) and assigns to it, which might not update `df`. `.loc` accesses the DataFrame directly in a single step, ensuring the assignment takes effect on the intended object.",
    "difficulty": "Intermediate"
  },
  {
    "id": 66,
    "question": "When reading a CSV file, you encounter a column that should be numeric but contains characters like '$' or ','. Which `read_csv` parameter allows you to convert these values to numbers automatically?",
    "options": [
      "converters",
      "na_values",
      "thousands",
      "dtype"
    ],
    "answer": "converters",
    "explanation": "The `converters` parameter accepts a dictionary mapping column names to functions. This allows you to apply logic (like stripping '$' and ',' and casting to float) during the import process. `thousands` handles thousand separators but not prefixes like '$'. `na_values` defines what constitutes missing data. `dtype` sets the type but doesn't handle cleaning logic.",
    "difficulty": "Intermediate"
  },
  {
    "id": 67,
    "question": "What is the primary function of the `query()` method in Pandas?",
    "options": [
      "To execute SQL-like queries directly on a database connection",
      "To filter rows using a string expression, often providing more readable syntax than boolean indexing",
      "To retrieve metadata about the DataFrame structure",
      "To search for specific strings within object columns"
    ],
    "answer": "To filter rows using a string expression, often providing more readable syntax than boolean indexing",
    "explanation": "`df.query('age > 30 & city == \"New York\"')` allows you to filter data using a concise string expression. It can be faster and more readable, especially when comparing multiple columns, than the standard `df[(df.age > 30) & (df.city == 'New York')]`. It evaluates within the context of the DataFrame, not a SQL database.",
    "difficulty": "Intermediate"
  },
  {
    "id": 68,
    "question": "You are analyzing a dataset of customer transactions and want to rank customers by their total purchase amount within their respective regions. Which method combines `groupby` and ranking functionality effectively?",
    "options": [
      "groupby('region')['amount'].sum().sort_values()",
      "groupby('region')['amount'].rank(method='dense')",
      "groupby('region')['amount'].apply(lambda x: x.rank())",
      "pivot_table(index='region', values='amount', aggfunc='rank')"
    ],
    "answer": "groupby('region')['amount'].rank(method='dense')",
    "explanation": "The `rank()` method calculates the rank of values (1, 2, 3...). When applied after a `groupby`, it calculates the rank within each group (e.g., ranking customers specifically within the 'North' region). Option A sums the amounts but doesn't rank individuals. Option C works but `rank()` directly is optimized for this. Option D is for reshaping.",
    "difficulty": "Intermediate"
  },
  {
    "id": 69,
    "question": "When exporting a DataFrame to a CSV file using `to_csv()`, how can you prevent Pandas from writing the row index (0, 1, 2...) as a column in the file?",
    "options": [
      "Set `index=False`",
      "Set `header=False`",
      "Set `index_col=False`",
      "Use `drop(index=True)` before exporting"
    ],
    "answer": "Set `index=False`",
    "explanation": "The `index=False` parameter in `to_csv()` instructs Pandas not to include the DataFrame index in the output file. `header=False` would omit column names. `index_col` is a parameter for `read_csv`, not `to_csv`. Dropping the index inside the DataFrame changes the data in memory, which might not be desired.",
    "difficulty": "Intermediate"
  },
  {
    "id": 70,
    "question": "What distinguishes the `expanding()` window function from the `rolling()` window function?",
    "options": [
      "`expanding()` uses a fixed window size, while `rolling()` grows the window size",
      "`expanding()` includes all data points from the start of the DataFrame up to the current point, while `rolling()` uses a fixed size window",
      "`expanding()` is only used for time-series data, while `rolling()` is for cross-sectional data",
      "`expanding()` calculates the minimum, while `rolling()` calculates the maximum"
    ],
    "answer": "`expanding()` includes all data points from the start of the DataFrame up to the current point, while `rolling()` uses a fixed size window",
    "explanation": "An expanding window grows with the data. For row 5, it looks at rows 1-5. For row 10, it looks at rows 1-10. This is useful for cumulative statistics (e.g., 'cumulative sum' or 'year-to-date total'). A rolling window (e.g., 7 days) always looks at the last 7 rows (4-10), ignoring older data.",
    "difficulty": "Intermediate"
  },
  {
    "id": 71,
    "question": "In Pandas 2.0+, utilizing Copy-on-Write (CoW) mode changes the behavior of certain operations. Which statement accurately describes the consequence of enabling CoW when modifying a DataFrame slice?",
    "options": [
      "It allows in-place modifications of the slice to automatically propagate back to the parent DataFrame, conserving memory.",
      "It raises a SettingWithCopyWarning exception immediately upon slicing a DataFrame to prevent accidental view manipulation.",
      "It defers the creation of a copy until data is actually modified, ensuring that the original DataFrame remains immutable unless explicitly overwritten.",
      "It forces all indexing operations, such as .loc or .iloc, to return deep copies by default, significantly increasing memory usage."
    ],
    "answer": "It defers the creation of a copy until data is actually modified, ensuring that the original DataFrame remains immutable unless explicitly overwritten.",
    "explanation": "With Copy-on-Write enabled (which will be the default in Pandas 3.0), slicing returns a view referencing the original data. However, any subsequent modification to that view triggers an implicit copy, leaving the original DataFrame unchanged. This prevents the 'chained assignment' issues and confusing side effects common in older Pandas versions, ensuring data integrity by making mutation predictable.",
    "difficulty": "Advanced"
  },
  {
    "id": 72,
    "question": "When analyzing time-series data with irregular frequencies, what is the primary functional distinction between resampling with forward filling (`ffill`) and using `asfreq` with a specific method?",
    "options": [
      "`asfreq` is a lightweight convenience method that strictly changes the index frequency and introduces NaNs, whereas `resample` is a group-by-like aggregation that requires explicit method handling for missing data.",
      "`resample` can only handle numerical data types, while `asfreq` is optimized for datetime objects and categorical time-series data.",
      "`asfreq` automatically interpolates missing values based on linear regression, whereas `resample` with `ffill` only carries the last known value forward.",
      "`resample` returns a DatetimeIndex with business-day frequency by default, while `asfreq` adheres strictly to the specific calendar frequency passed."
    ],
    "answer": "`asfreq` is a lightweight convenience method that strictly changes the index frequency and introduces NaNs, whereas `resample` is a group-by-like aggregation that requires explicit method handling for missing data.",
    "explanation": "While both can fill missing data, `asfreq` is a low-level method primarily used to convert a time series to a fixed frequency (generating NaNs for missing slots unless `method` is specified). `resample` is fundamentally a time-based GroupBy operation; it groups data into time bins and allows for more complex aggregations (sum, mean, ohlc) before handling the missing data logic.",
    "difficulty": "Advanced"
  },
  {
    "id": 73,
    "question": "Consider a DataFrame `df` with a MultiIndex on rows (Level 0: 'Region', Level 1: 'Product') and columns 'Sales' and 'Cost'. What is the behavior of `df.stack()` compared to `df.unstack()` in this context?",
    "options": [
      "`stack()` pivots the innermost column index into the innermost row index, potentially creating a Series, while `unstack()` pivots the innermost row index into the column index.",
      "`stack()` compresses a level in the DataFrame's columns to create a MultiIndex on rows, while `unstack()` expands a level in the rows to create columns.",
      "`stack()` is used to widen the DataFrame by converting row indices to columns, whereas `unstack()` is used to lengthen the DataFrame by converting columns to rows.",
      "`stack()` requires the DataFrame to have a MultiIndex on columns to function, whereas `unstack()` can only be performed on DataFrames with a single-level index."
    ],
    "answer": "`stack()` pivots the innermost column index into the innermost row index, potentially creating a Series, while `unstack()` pivots the innermost row index into the column index.",
    "explanation": "Conceptually, 'stack' rotates the columns (widest part) down into the index (making it 'longer'), effectively converting the DataFrame to a Series (if columns are single-level) or reshaping the MultiIndex. 'Unstack' does the inverse, rotating the innermost row index up into the column index (making it 'wider'). This distinction is crucial for reshaping data between tidy (long) and panel (wide) formats.",
    "difficulty": "Advanced"
  },
  {
    "id": 74,
    "question": "Why does using the Python `and` operator fail when performing boolean indexing on a Pandas Series (e.g., `df[(df['A'] > 5) and (df['B'] < 10)]`), forcing the use of `&` instead?",
    "options": [
      "Python's `and` operator attempts to coerce the Series to a boolean scalar, which is ambiguous for arrays, while `&` performs element-wise bitwise AND.",
      "The `and` operator has higher precedence than comparison operators, causing the logic to evaluate before the comparison completes.",
      "Pandas Series objects override the `__and__` magic method but not the logical `__bool__` method required for `and` evaluation.",
      "Using `and` would trigger an immutable copy warning, whereas `&` is optimized for in-place modification of the underlying NumPy array."
    ],
    "answer": "Python's `and` operator attempts to coerce the Series to a boolean scalar, which is ambiguous for arrays, while `&` performs element-wise bitwise AND.",
    "explanation": "Python's standard logical operators (`and`, `or`, `not`) are designed to work with single boolean values to control program flow (short-circuiting). When used with a Pandas Series or NumPy array, Python tries to evaluate the truthiness of the whole object (`if series:`), which raises a ValueError. The `&` operator performs the intended element-wise logical comparison across the entire array.",
    "difficulty": "Advanced"
  },
  {
    "id": 75,
    "question": "When dealing with highly cardinal categorical data in Pandas, what is the trade-off between using the standard `category` dtype versus the `string` dtype introduced in recent versions?",
    "options": [
      "`category` dtype is memory-efficient but supports fewer string manipulation methods, requiring conversion back to object/string for operations like `.str.upper()`, whereas `string` dtype is fully compatible with string methods but uses more memory.",
      "`category` dtype performs faster in groupby operations but significantly slows down merging, while `string` dtype is optimized specifically for merge keys.",
      "`string` dtype stores data as Python objects in a hash table, making it faster for lookups than `category`, which stores data as integer codes.",
      "There is no functional difference; `string` is simply an alias for `category` to maintain compatibility with PyArrow."
    ],
    "answer": "`category` dtype is memory-efficient but supports fewer string manipulation methods, requiring conversion back to object/string for operations like `.str.upper()`, whereas `string` dtype is fully compatible with string methods but uses more memory.",
    "explanation": "The `category` dtype stores data as integer codes mapping to a small table of unique values, saving massive amounts of memory for repetitive strings. However, you lose direct access to vectorized string methods. The modern `string` dtype (and StringArray) provides a true nullable string type that supports `.str` accessors natively, but it behaves more like a standard object array in terms of memory overhead (unless backed by PyArrow).",
    "difficulty": "Advanced"
  },
  {
    "id": 76,
    "question": "What is the specific risk associated with performing `pd.merge()` on integer columns in two DataFrames where the integers represent categorical data but have different code mappings (e.g., 1=Red in DF1, 1=Blue in DF2)?",
    "options": [
      "Pandas will perform a Cartesian product (cross join) instead of an inner join because the data types do not match the categorical definition.",
      "The merge will succeed numerically, matching rows based on integer value (1 with 1), resulting in semantically incorrect data alignment (Red matched with Blue).",
      "Pandas will automatically detect the categorical mismatch and recode the integers to a new unified schema, altering the original values.",
      "The operation will raise a `MergeError` because categorical keys must be defined as strings or objects to ensure safety."
    ],
    "answer": "The merge will succeed numerically, matching rows based on integer value (1 with 1), resulting in semantically incorrect data alignment (Red matched with Blue).",
    "explanation": "Pandas `merge` does not understand the semantic meaning of your data; it only looks at values. If you use integer codes as keys, it treats them as numbers. If '1' means different things in different contexts, merging on them creates false positives. Best practice for categorical merges is to use the actual labels (strings) or ensure that integer codes are mapped to a consistent global definition before merging.",
    "difficulty": "Advanced"
  },
  {
    "id": 77,
    "question": "How does the `pd.eval()` syntax utilize the `numexpr` library to optimize computation for large DataFrames?",
    "options": [
      "It compiles the string expression into C bytecode, bypassing the Python Global Interpreter Lock (GIL) for entirely arbitrary Python functions.",
      "It parses the string expression into an abstract syntax tree and computes the result using a vectorized C-engine, reducing memory overhead by avoiding intermediate temporary arrays.",
      "It parallelizes the operation across all available CPU cores by converting the DataFrame into a Dask collection automatically.",
      "It re-indexes the DataFrames to ensure alignment happens once at the beginning of the operation, rather than checking alignment on every step."
    ],
    "answer": "It parses the string expression into an abstract syntax tree and computes the result using a vectorized C-engine, reducing memory overhead by avoiding intermediate temporary arrays.",
    "explanation": "Standard Pandas arithmetic (e.g., `df['A'] + df['B'] * df['C']`) creates temporary arrays for each step (temp1 = A+B, result = temp1*C). `pd.eval` uses `numexpr` to evaluate the entire expression at once in a optimized loop, minimizing the creation of intermediate arrays and thus saving memory and often speeding up complex arithmetic on large datasets.",
    "difficulty": "Advanced"
  },
  {
    "id": 78,
    "question": "What is the implication of the `na_sentinel` argument in `pd.factorize()` when used to prepare data for machine learning algorithms?",
    "options": [
      "It determines the value used to represent missing data in the resulting integer codes; setting it to a value outside the valid range (like -1) preserves the information that the data was missing.",
      "It ensures that all `NaN` values are sorted to the end of the array during the factorization process.",
      "It acts as a placeholder to prevent `NaN` values from being counted as a unique category during the encoding process.",
      "It converts `pd.NA` (scalar) to `np.nan` (floating point) to ensure compatibility with scikit-learn's LabelEncoder."
    ],
    "answer": "It determines the value used to represent missing data in the resulting integer codes; setting it to a value outside the valid range (like -1) preserves the information that the data was missing.",
    "explanation": "`pd.factorize()` encodes unique values as integers. By default, `NaN` values are encoded as `-1`. If your data naturally contains negative integers, or if you need the missing value marker to be a specific sentinel (e.g., `999`) to signal to a downstream ML model that this feature was missing, you must adjust `na_sentinel`. It is crucial for distinguishing between a valid category '0' and a missing value.",
    "difficulty": "Advanced"
  },
  {
    "id": 79,
    "question": "When using `df.to_parquet()` with the default `engine='pyarrow'`, how does the behavior of `index_col` differ from simply saving the index as a regular column?",
    "options": [
      "`index_col` writes the index as metadata within the Parquet file structure, allowing it to be restored exactly as the index upon reading, whereas writing it as a column loses the index metadata.",
      "`index_col` compresses the index using a different algorithm (RLE) than the data columns (Dictionary encoding), optimizing storage.",
      "Saving via `index_col` prevents the index from being sorted, maintaining the original insertion order for time-series accuracy.",
      "There is no difference; `to_parquet` automatically detects if the index contains unique values and treats it as a primary key."
    ],
    "answer": "`index_col` writes the index as metadata within the Parquet file structure, allowing it to be restored exactly as the index upon reading, whereas writing it as a column loses the index metadata.",
    "explanation": "If you `reset_index()` before saving to Parquet, your index becomes a regular data column. When you load it back, you have to manually `set_index()` again. Using `index_col` (or `write_index=True` in `to_parquet` context) stores the index metadata specifically in the file footer. `pd.read_parquet` can then recognize this column as the index instantly, preserving the specific index type (e.g., DatetimeIndex, MultiIndex).",
    "difficulty": "Advanced"
  },
  {
    "id": 80,
    "question": "In a MultiIndex DataFrame, why might the `xs` (cross-section) method be preferred over standard `.loc` slicing for retrieving data at a specific level?",
    "options": [
      "`xs` is capable of slicing multiple levels simultaneously, whereas `.loc` can only target one level at a time.",
      "`xs` automatically drops the level being sliced, reducing dimensionality and returning a DataFrame or Series with a simpler index, whereas `.loc` retains the full MultiIndex structure.",
      "`.loc` creates a deep copy of the data, while `xs` always returns a memory-efficient view.",
      "`xs` is the only way to retrieve data based on a sorted MultiIndex without creating a KeyError."
    ],
    "answer": "`xs` automatically drops the level being sliced, reducing dimensionality and returning a DataFrame or Series with a simpler index, whereas `.loc` retains the full MultiIndex structure.",
    "explanation": "While `.loc` can select a specific key in a MultiIndex (e.g., `df.loc[('A', '1'), :]`), the resulting object usually keeps the level in the index structure (even if it has only one unique value left). The `xs` method is semantically designed to take a cross-section of the data, and by default, it removes the selected level from the result, effectively 'drilling down' into the data hierarchy.",
    "difficulty": "Advanced"
  },
  {
    "id": 81,
    "question": "What is the specific danger of using floating-point numbers as Index keys in a Pandas DataFrame?",
    "options": [
      "Pandas relies on integer-based hashing for Index lookups, forcing floats to be cast to integers, causing precision loss.",
      "Floating-point precision errors (IEEE 754 representation) can lead to two numbers that look identical (e.g., 0.1 + 0.2) being treated as distinct keys, breaking alignment.",
      "Float indices prevent the usage of the `loc` accessor, restricting you to only `iloc` for positional access.",
      "GroupBy operations automatically round float indices to 3 decimal places to save memory, potentially merging distinct rows."
    ],
    "answer": "Floating-point precision errors (IEEE 754 representation) can lead to two numbers that look identical (e.g., 0.1 + 0.2) being treated as distinct keys, breaking alignment.",
    "explanation": "Because computers store floats as binary approximations, arithmetic operations often result in tiny rounding errors. When used as an index, `0.30000000000000004` and `0.3` are treated as completely different keys. This makes merging, joining, and looking up values by float index highly risky. It is generally recommended to use integers or decimals for indexing if precise lookups are required.",
    "difficulty": "Advanced"
  },
  {
    "id": 82,
    "question": "When using `pd.cut()` to bin continuous data, what is the semantic difference between `retbins=True` and `labels=False`?",
    "options": [
      "`retbins=True` returns the computed bins edges used for the cutting, while `labels=False` returns integer indicators of the bins rather than the interval labels.",
      "`retbins=True` creates a categorical dtype, while `labels=False` keeps the data as a float64 dtype.",
      "`retbins=True` forces bins to be of equal width, while `labels=False` forces bins to be of equal frequency (quantiles).",
      "`labels=False` is a deprecated parameter that has been replaced by `duplicates='drop'` in newer versions."
    ],
    "answer": "`retbins=True` returns the computed bins edges used for the cutting, while `labels=False` returns integer indicators of the bins rather than the interval labels.",
    "explanation": "`pd.cut` normally returns a Categorical index where values are labeled as intervals like `(10.0, 20.0]`. Setting `labels=False` changes the output to simple integers (0, 1, 2...) representing bin membership. `retbins=True` does not change the content of the binned data but instead returns a tuple of (series, bins_array), which is useful if you want to apply the exact same binning logic to a new dataset (e.g., a test set) based on training data distributions.",
    "difficulty": "Advanced"
  },
  {
    "id": 83,
    "question": "In time-series analysis, how does the `origin` parameter in `resample` (introduced in newer Pandas versions) adjust the grouping of timestamps?",
    "options": [
      "It shifts the resampling window to start from the earliest timestamp found in the data rather than the default Unix epoch (1970-01-01).",
      "It converts the timezone of the index to the specified `origin` timezone before performing the frequency conversion.",
      "It defines a specific anchor day for the resampling bins (e.g., `origin='start_day'`) to align bins with data boundaries rather than calendar midnight.",
      "It fills backward missing values starting from the `origin` timestamp if the `fill_method` is set to 'bfill'."
    ],
    "answer": "It defines a specific anchor day for the resampling bins (e.g., `origin='start_day'`) to align bins with data boundaries rather than calendar midnight.",
    "explanation": "By default, resampling bins are aligned to calendar conventions (e.g., midnight for daily bins). However, if your data starts at 13:00, the first bin is truncated. `origin` allows you to anchor the time bins relative to the start of the data (`origin='start'`) or the end of the data (`origin='end'`), or a specific timestamp. This ensures the resampling windows align logically with the actual observation period rather than arbitrary calendar points.",
    "difficulty": "Advanced"
  },
  {
    "id": 84,
    "question": "When performing a `groupby` operation, what is the computational advantage of using the `observed=True` parameter with categorical data?",
    "options": [
      "It converts categorical columns to strings to speed up the hashing algorithm used during the split phase.",
      "It restricts the grouping to only combinations of categories that are actually present in the data, avoiding the creation of empty groups for unobserved category permutations.",
      "It forces Pandas to use a merge-sort algorithm instead of a hash-table, which is faster for sorted categorical data.",
      "It enables the `numexpr` engine for the aggregation step, allowing for parallel processing."
    ],
    "answer": "It restricts the grouping to only combinations of categories that are actually present in the data, avoiding the creation of empty groups for unobserved category permutations.",
    "explanation": "If you group by a categorical column that has 100 possible categories, but your DataFrame only contains data for 5 of them, Pandas (by default) will generate groups for all 100 categories (filled with NaNs/0). This is extremely memory and CPU intensive. `observed=True` tells Pandas to only form groups for the categories actually seen in the data, significantly speeding up aggregation on high-cardinality sparse categorical data.",
    "difficulty": "Advanced"
  },
  {
    "id": 85,
    "question": "How does the `pd.merge_ordered()` function differ fundamentally from `pd.merge()` when handling time-series or ordered data?",
    "options": [
      "`merge_ordered` performs an inner join by default, whereas `pd.merge` performs an outer join.",
      "`merge_ordered` uses a binary search algorithm to join on sorted keys, which is significantly faster (O(N+M)) than the hash-based join (O(N*M)) used in standard merge.",
      "`merge_ordered` automatically fills in missing values generated by the join using linear interpolation (ffill/bfill), whereas standard merge simply introduces NaNs.",
      "`merge_ordered` requires the merge keys to be unique, whereas standard merge handles duplicate keys via Cartesian products."
    ],
    "answer": "`merge_ordered` automatically fills in missing values generated by the join using linear interpolation (ffill/bfill), whereas standard merge simply introduces NaNs.",
    "explanation": "While both functions join data, `merge_ordered` is designed for sequential data (like timeseries). When there are gaps in the time series between the left and right DataFrames, a standard merge results in disjointed time points with NaNs. `merge_ordered` allows you to specify `fill_method='ffill'` (or similar), which constructs a complete, continuous sequence of the combined time index, carrying values forward to fill the gaps created by the merge.",
    "difficulty": "Advanced"
  },
  {
    "id": 86,
    "question": "When dealing with a MultiIndex, what is the specific constraint regarding the `sortorder` parameter during index creation?",
    "options": [
      "`sortorder` only affects the display order in the console and does not impact the underlying memory layout of the index.",
      "To utilize `sortorder`, the MultiIndex must be lexicographically sorted by the specified level; otherwise, performance optimizations for slicing cannot be applied.",
      "The `sortorder` parameter can only be set to 0 or 1, representing ascending or descending order of the first level.",
      "`sortorder` is deprecated in Pandas 2.0 in favor of `sort_index(axis=1)`."
    ],
    "answer": "To utilize `sortorder`, the MultiIndex must be lexicographically sorted by the specified level; otherwise, performance optimizations for slicing cannot be applied.",
    "explanation": "The `sortorder` parameter in MultiIndex creation indicates which level the index is sorted on. This metadata allows Pandas to use highly optimized search algorithms (like binary search) for `loc` slicing, provided the index is actually sorted. If you specify a `sortorder` but provide unsorted data, operations like `xs` or `loc` will produce incorrect results or errors because they rely on the sort assumption for speed.",
    "difficulty": "Advanced"
  },
  {
    "id": 87,
    "question": "What is the primary advantage of using `df.explode()` on a column containing lists compared to iterating and appending rows?",
    "options": [
      "`explode` preserves the index of the original row, repeating it for each element in the list, whereas iteration typically resets the index.",
      "`explode` automatically converts the list elements into a categorical dtype to save memory.",
      "`explode` allows for simultaneous explosion of multiple columns, whereas iteration can only handle one list column at a time.",
      "`explode` is implemented in C (via Cython), avoiding the slow Python `for` loop overhead associated with appending rows."
    ],
    "answer": "`explode` is implemented in C (via Cython), avoiding the slow Python `for` loop overhead associated with appending rows.",
    "explanation": "Exploding a list by iterating in Python is O(N^2) or worse because DataFrame resizing often requires memory reallocation. `df.explode()` is a vectorized operation implemented at a lower level. It efficiently calculates the new index size and replicates the data, which is orders of magnitude faster for large datasets with nested lists.",
    "difficulty": "Advanced"
  },
  {
    "id": 88,
    "question": "Why might one prefer `df.take()` instead of `df.iloc` for retrieving rows based on a list of integer indices?",
    "options": [
      "`take` supports negative indices without needing to calculate the absolute position, whereas `iloc` raises an IndexError.",
      "`take` is strictly positional and does not attempt to align on the index, offering slightly faster performance for pure positional retrieval on axes.",
      "`iloc` creates a copy of the data, while `take` always returns a view, preserving memory.",
      "`take` allows slicing by string labels of the index position, unlike `iloc` which only accepts integers."
    ],
    "answer": "`take` is strictly positional and does not attempt to align on the index, offering slightly faster performance for pure positional retrieval on axes.",
    "explanation": "While `iloc` is the standard interface for positional indexing, it does some extra work to ensure compatibility with the Index object (like handling slices). `take()` is a lower-level NumPy-like API that strictly grabs data by integer position along an axis. In tight loops or performance-critical code where you already have integer positions and don't need index alignment checks, `take` can be marginally faster.",
    "difficulty": "Advanced"
  },
  {
    "id": 89,
    "question": "In Pandas 2.0+, using `convert_dtypes()` provides functionality similar to `infer_objects()`. What is the specific behavior regarding missing values when using `convert_dtypes`?",
    "options": [
      "It converts all columns to `object` dtype to ensure that missing values can be mixed (None, np.nan, pd.NA).",
      "It uses dtypes that support `pd.NA` (the nullable mask), providing a standard missing value indicator across integers, booleans, and strings.",
      "It automatically replaces all `np.nan` values with 0 to facilitate arithmetic operations.",
      "It converts `float` columns with missing values to `int` columns, rounding the values down."
    ],
    "answer": "It uses dtypes that support `pd.NA` (the nullable mask), providing a standard missing value indicator across integers, booleans, and strings.",
    "explanation": "Legacy Pandas relied on `np.nan` (float) for missing numbers, forcing integer columns with NaNs to become floats. `convert_dtypes()` infers the 'best' type for the data, leveraging the new nullable dtypes (like `Int64`, `string`, `boolean`). This allows an integer column with missing data to remain an integer type (using a null mask) rather than casting to float64, which is critical for data integrity in IDs or counts.",
    "difficulty": "Advanced"
  },
  {
    "id": 90,
    "question": "When performing a `pd.merge`, how does the `validate='one_to_one'` argument assist in debugging data quality issues?",
    "options": [
      "It ensures that the merge keys in both DataFrames are unique, raising a `MergeError` if a one-to-one match is not found (e.g., duplicates exist).",
      "It checks that the data types of the merge keys match exactly between the two DataFrames before attempting the join.",
      "It performs a Venn diagram analysis to ensure that no rows are dropped during an inner join.",
      "It validates that the `on` parameter refers to existing column names in both DataFrames."
    ],
    "answer": "It ensures that the merge keys in both DataFrames are unique, raising a `MergeError` if a one-to-one match is not found (e.g., duplicates exist).",
    "explanation": "Accidentally creating a Cartesian product (many-to-many) or a many-to-one merge when you expect a one-to-one match is a common source of exploding data sizes. `validate='one_to_one'` acts as a safeguard: if the merge operation would result in duplicate keys (indicating the relationship isn't strictly 1:1), Pandas raises an exception immediately, alerting you to duplicates in your keys before you proceed with incorrect analysis.",
    "difficulty": "Advanced"
  },
  {
    "id": 91,
    "question": "What distinguishes `df.compare(other)` from a simple boolean mask like `df != other` when auditing differences between two DataFrames?",
    "options": [
      "`compare` aligns the indices automatically, whereas `!=` requires both DataFrames to have the exact same index length.",
      "`compare` presents the values of both DataFrames side-by-side for the differing cells, whereas `!=` only returns `True`/`False` masks.",
      "`compare` works faster on large DataFrames because it uses a vectorized C-implementation, while `!=` loops in Python.",
      "`compare` ignores NaN values by default, while `!=` treats NaN != NaN as True."
    ],
    "answer": "`compare` presents the values of both DataFrames side-by-side for the differing cells, whereas `!=` only returns `True`/`False` masks.",
    "explanation": "While `df != other` tells you *where* the differences are, it doesn't tell you *what* the values are that changed. `df.compare()` returns a DataFrame (often MultiIndexed) showing the 'self' and 'other' values specifically for the cells that differ. This is critical for auditing and understanding the nature of data changes, not just their location.",
    "difficulty": "Advanced"
  },
  {
    "id": 92,
    "question": "What is the specific effect of using `method='nearest'` with a `DatetimeIndex` in `reindex()` compared to standard forward/backward filling?",
    "options": [
      "It averages the distance between the nearest neighbor points to create an interpolated value.",
      "It finds the closest existing date to the target date regardless of whether it is before or after, enabling 'closest-match' lookups.",
      "It converts the index to a PeriodIndex (Monthly/Daily) to find the nearest equivalent frequency.",
      "It performs a nearest-neighbor search in 2D space if the DataFrame has a MultiIndex."
    ],
    "answer": "It finds the closest existing date to the target date regardless of whether it is before or after, enabling 'closest-match' lookups.",
    "explanation": "Standard `ffill` only looks backward (past) and `bfill` only looks forward (future). `method='nearest'` looks in both directions to find the timestamp with the smallest absolute difference. This is essential for use cases like joining low-frequency sensor data to high-frequency timestamps where you want the nearest reading in time, not just the most recent one.",
    "difficulty": "Advanced"
  },
  {
    "id": 93,
    "question": "When using `df.query()` to filter rows, why is it often necessary to refer to external variables using the `@` prefix?",
    "options": [
      "The `@` symbol signals Pandas to escape the string and treat it as a literal SQL command.",
      "`query` parses a restricted Python expression syntax; without `@`, it attempts to find a column with that name, whereas `@variable` refers to a variable in the local scope.",
      "The `@` symbol enables multi-threading by moving the variable search to the global interpreter lock.",
      "Pandas requires `@` to distinguish between string literals (column names) and integer variables (constants)."
    ],
    "answer": "`query` parses a restricted Python expression syntax; without `@`, it attempts to find a column with that name, whereas `@variable` refers to a variable in the local scope.",
    "explanation": "The `query` method takes a string expression. If you write `df.query('column == val')`, Pandas looks for a column named 'val' in the DataFrame. To reference a Python variable `val` existing in your environment, you must prefix it with `@` (e.g., `df.query('column == @val')`). This namespacing distinguishes the DataFrame's column namespace from the user's variable namespace.",
    "difficulty": "Advanced"
  },
  {
    "id": 94,
    "question": "How does the `rank()` method handle duplicate values when `method='average'` is selected?",
    "options": [
      "It randomly assigns ranks to duplicate values to break the tie.",
      "It assigns the minimum possible rank to all duplicate values.",
      "It assigns the average of the ranks that the duplicates would have occupied if they had distinct values.",
      "It forces all duplicates to share the rank of the first occurrence."
    ],
    "answer": "It assigns the average of the ranks that the duplicates would have occupied if they had distinct values.",
    "explanation": "Consider values [10, 20, 20, 30]. Ranks are 1, 2, 3, 4. For the duplicate 20s, `method='average'` assigns (2+3)/2 = 2.5 to both. `method='min'` would assign 2 to both. `method='dense'` would assign 2 to both, but the next value (30) would be rank 3. `average` is the statistical default used in many ranking systems (like standard competition ranking) to ensure the sum of ranks remains consistent.",
    "difficulty": "Advanced"
  },
  {
    "id": 95,
    "question": "In Pandas, what is the primary limitation of `np.where` applied to a DataFrame compared to `df.where`?",
    "options": [
      "`np.where` cannot handle boolean masks with different shapes, requiring the mask and the arrays to be identical.",
      "`np.where` returns a NumPy array (losing DataFrame index/column metadata), whereas `df.where` preserves the pandas structure and index alignment.",
      "`np.where` is deprecated in Pandas 2.0 and raises a FutureWarning.",
      "`np.where` cannot accept callable functions as its condition argument."
    ],
    "answer": "`np.where` returns a NumPy array (losing DataFrame index/column metadata), whereas `df.where` preserves the pandas structure and index alignment.",
    "explanation": "While `np.where(cond, df1, df2)` works element-wise, the result is a raw NumPy ndarray. `df.where(cond, df2)` performs a similar operation but keeps the result as a Pandas DataFrame, maintaining index labels and column headers. This ensures that the data remains context-aware within the pandas ecosystem.",
    "difficulty": "Advanced"
  },
  {
    "id": 96,
    "question": "What is the specific behavior of `pd.concat` when `keys` are provided along with `axis=0` (vertical stacking)?",
    "options": [
      "It filters the concatenated DataFrames to only include rows where the index values match the provided keys.",
      "It constructs a hierarchical MultiIndex on the rows, using the provided keys to label the source of each original DataFrame.",
      "It renames the columns of the input DataFrames to match the string names provided in the `keys` list.",
      "It attempts an inner merge on the columns specified in the `keys` list before concatenating."
    ],
    "answer": "It constructs a hierarchical MultiIndex on the rows, using the provided keys to label the source of each original DataFrame.",
    "explanation": "When combining multiple DataFrames (e.g., sales from Q1, Q2, Q3), simply stacking them loses the information about which quarter they came from. Passing `keys=['Q1', 'Q2', 'Q3']` to `pd.concat` creates a MultiIndex on the resulting axis. This allows you to easily select, slice, or group by the original datasets (e.g., `result.loc['Q1']`).",
    "difficulty": "Advanced"
  },
  {
    "id": 97,
    "question": "When using `df.applymap` (or `DataFrame.map` in newer versions), what is a key characteristic that distinguishes it from `df.apply`?",
    "options": [
      "`applymap` is optimized for series-specific methods, while `apply` is optimized for DataFrame-wide aggregation.",
      "`applymap` applies a function element-wise to every cell in the DataFrame, whereas `apply` applies a function along an axis (row or column).",
      "`applymap` cannot access index labels, while `apply` can.",
      "`applymap` always returns a DataFrame, but `apply` may return a Series."
    ],
    "answer": "`applymap` applies a function element-wise to every cell in the DataFrame, whereas `apply` applies a function along an axis (row or column).",
    "explanation": "`df.apply(func)` passes a Series (a column or row) to the function. This is useful for column-wise sums or row-wise string concatenations. `df.applymap(func)` (or `.map` in newer pandas) passes a single scalar value to the function for every single cell. It is used for formatting or transforming individual data points regardless of their row or column context.",
    "difficulty": "Advanced"
  },
  {
    "id": 98,
    "question": "What is the function of `pd.api.extensions.register_dataframe_accessor`?",
    "options": [
      "It allows developers to add custom methods to existing Pandas DataFrames without subclassing, enabling 'domain-specific' syntax like `df.rolling.calc_volatility()`.",
      "It creates a bridge between Pandas and NumPy, allowing direct access to the underlying C-arrays.",
      "It registers a new file format type (e.g., `.csv`, `.json`) to be recognized by `pd.read()` automatically.",
      "It caches the result of a calculation in memory to prevent re-computation on subsequent accesses."
    ],
    "answer": "It allows developers to add custom methods to existing Pandas DataFrames without subclassing, enabling 'domain-specific' syntax like `df.rolling.calc_volatility()`.",
    "explanation": "This is a metaprogramming feature used in libraries that extend Pandas (like `pandas-profiling` or `geopandas`). Instead of asking users to call a standalone function `calculate_volatility(df)`, they register an accessor so the user can write `df.volatility.calculate()`. It keeps the core Pandas namespace clean while allowing for intuitive, object-oriented extensions.",
    "difficulty": "Advanced"
  },
  {
    "id": 99,
    "question": "How does the `memory_usage(deep=True)` parameter differ from the default `memory_usage()` calculation regarding object dtypes?",
    "options": [
      "`deep=True` includes the memory consumed by the Index object, while the default only calculates the memory of the data block.",
      "`deep=True` introspects the actual content of strings (objects) to calculate their byte usage, whereas the default only counts the pointer size (8 bytes per object).",
      "`deep=True` forces the system to flush the memory cache before calculating usage.",
      "`deep=True` converts object dtypes to `category` to measure the potential memory savings if converted."
    ],
    "answer": "`deep=True` introspects the actual content of strings (objects) to calculate their byte usage, whereas the default only counts the pointer size (8 bytes per object).",
    "explanation": "The default `memory_usage()` for an `object` column typically reports 8 bytes (or whatever the pointer size is on your OS) per element, ignoring the size of the string data itself (which is stored elsewhere). `deep=True` traverses the Python objects to sum up the actual size of the strings, giving a realistic number for memory consumption, which is crucial when diagnosing memory issues in text-heavy DataFrames.",
    "difficulty": "Advanced"
  }
]