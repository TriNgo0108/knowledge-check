[
  {
    "id": 1,
    "question": "Which function is used to read a CSV file into a pandas DataFrame?",
    "options": [
      "pd.read_csv()",
      "pd.load_csv()",
      "pd.open_csv()",
      "pd.import_csv()"
    ],
    "answer": "pd.read_csv()",
    "explanation": "read_csv is the standard parser for CSV files. load_csv and open_csv do not exist in the pandas API namespace.",
    "difficulty": "Beginner"
  },
  {
    "id": 2,
    "question": "What is the primary data structure in pandas used for representing two-dimensional tabular data?",
    "options": [
      "Series",
      "DataFrame",
      "Panel",
      "Matrix"
    ],
    "answer": "DataFrame",
    "explanation": "A DataFrame is a 2-dimensional labeled data structure. Series is 1-dimensional, and Panel (for 3D data) is deprecated.",
    "difficulty": "Beginner"
  },
  {
    "id": 3,
    "question": "Which attribute of a DataFrame returns the dimensionality (rows, columns) of the data?",
    "options": [
      "df.size",
      "df.ndims",
      "df.shape",
      "df.length"
    ],
    "answer": "df.shape",
    "explanation": "df.shape returns a tuple representing the dimensionality of the DataFrame. df.size returns the total number of elements.",
    "difficulty": "Beginner"
  },
  {
    "id": 4,
    "question": "When using `.loc[]` to access data, what type of indexing is primarily utilized?",
    "options": [
      "Integer position-based indexing",
      "Label-based indexing",
      "Boolean indexing only",
      "Random sampling"
    ],
    "answer": "Label-based indexing",
    "explanation": ".loc is primarily label-based, meaning you use the actual names of the rows or columns. .iloc is used for integer-position based indexing.",
    "difficulty": "Beginner"
  },
  {
    "id": 5,
    "question": "Which method is most effective for retrieving a single scalar value by label and optimizing performance?",
    "options": [
      ".loc[]",
      ".iloc[]",
      ".at[]",
      ".get[]"
    ],
    "answer": ".at[]",
    "explanation": ".at[] provides extremely fast access to a single scalar value by label. .loc[] is slower as it handles slices and arrays.",
    "difficulty": "Beginner"
  },
  {
    "id": 6,
    "question": "What is the result of applying vectorized operations on a pandas Series?",
    "options": [
      "A TypeError",
      "A new Series with the operation applied element-wise",
      "A Python list",
      "An in-place modification without copying"
    ],
    "answer": "A new Series with the operation applied element-wise",
    "explanation": "Vectorized operations operate on the entire Series at once (leveraging NumPy), returning a new Series with results.",
    "difficulty": "Beginner"
  },
  {
    "id": 7,
    "question": "Which technique is recommended to improve performance when working with large datasets that do not fit into memory?",
    "options": [
      "Using for-loops to process rows",
      "Increasing system swap space",
      "Specifying the 'chunksize' parameter in read_csv",
      "Converting all columns to 'object' dtype"
    ],
    "answer": "Specifying the 'chunksize' parameter in read_csv",
    "explanation": "read_csv with chunksize returns an iterator, allowing processing of data in manageable pieces rather than loading the entire file into RAM.",
    "difficulty": "Beginner"
  },
  {
    "id": 8,
    "question": "How does the `dtype` 'category' generally affect DataFrame memory usage?",
    "options": [
      "It increases memory usage by storing metadata",
      "It has no effect on memory usage",
      "It significantly reduces memory usage for low-cardinality text data",
      "It converts data to float64 automatically"
    ],
    "answer": "It significantly reduces memory usage for low-cardinality text data",
    "explanation": "The 'category' dtype stores data as integers mapped to unique values, using far less memory than repeating strings.",
    "difficulty": "Beginner"
  },
  {
    "id": 9,
    "question": "What is the default behavior of `pd.merge()` if no 'how' argument is specified?",
    "options": [
      "Left join",
      "Right join",
      "Inner join",
      "Outer join"
    ],
    "answer": "Inner join",
    "explanation": "pd.merge defaults to an inner join (how='inner'), which keeps only the rows with keys present in both DataFrames.",
    "difficulty": "Beginner"
  },
  {
    "id": 10,
    "question": "Which function is used to bin continuous values into discrete intervals?",
    "options": [
      "pd.bin()",
      "pd.group()",
      "pd.cut()",
      "pd.discrete()"
    ],
    "answer": "pd.cut()",
    "explanation": "pd.cut() segments and sorts data values into bins. It is specifically designed for discretizing continuous variables.",
    "difficulty": "Beginner"
  },
  {
    "id": 11,
    "question": "Why is iterating over DataFrame rows using `for row in df.iterrows()` generally discouraged in pandas?",
    "options": [
      "It modifies the DataFrame index permanently",
      "It is extremely slow compared to vectorized operations",
      "It returns a copy of the data instead of a view",
      "It cannot access column values"
    ],
    "answer": "It is extremely slow compared to vectorized operations",
    "explanation": "Iterrows converts Series/rows to objects one by one, incurring heavy Python overhead. Vectorized operations use C-level optimizations.",
    "difficulty": "Beginner"
  },
  {
    "id": 12,
    "question": "Which method allows you to apply a function along an axis of the DataFrame?",
    "options": [
      "df.transform()",
      "df.apply()",
      "df.call()",
      "df.execute()"
    ],
    "answer": "df.apply()",
    "explanation": "apply() broadcasts a function across the DataFrame or Series. transform() is used specifically for returning data in the same shape.",
    "difficulty": "Beginner"
  },
  {
    "id": 13,
    "question": "What is the primary purpose of method chaining in pandas?",
    "options": [
      "To execute operations in parallel",
      "To write code in a more readable, sequential pipeline",
      "To automatically optimize memory usage",
      "To convert the DataFrame to a Series"
    ],
    "answer": "To write code in a more readable, sequential pipeline",
    "explanation": "Method chaining allows multiple operations to be applied sequentially, returning the intermediate object to the next method.",
    "difficulty": "Beginner"
  },
  {
    "id": 14,
    "question": "Which operation combines two DataFrames by aligning them on their index?",
    "options": [
      "pd.merge()",
      "pd.join()",
      "pd.concat()",
      "pd.append()"
    ],
    "answer": "pd.join()",
    "explanation": "join() is a convenience method for merging on index columns. merge() is typically used for merging on specific column keys.",
    "difficulty": "Beginner"
  },
  {
    "id": 15,
    "question": "What does the `dropna()` method do by default?",
    "options": [
      "Replaces missing values with zero",
      "Replaces missing values with the mean",
      "Removes rows containing any missing values",
      "Removes columns containing any missing values"
    ],
    "answer": "Removes rows containing any missing values",
    "explanation": "dropna() defaults to axis=0 (rows). To remove columns, you must explicitly pass axis=1.",
    "difficulty": "Beginner"
  },
  {
    "id": 16,
    "question": "Which method is used to reindex a DataFrame to conform to a new set of labels?",
    "options": [
      "df.reset_index()",
      "df.align()",
      "df.reindex()",
      "df.set_index()"
    ],
    "answer": "df.reindex()",
    "explanation": "reindex() conforms the data to a new index, introducing missing values (NaN) where labels did not previously exist.",
    "difficulty": "Beginner"
  },
  {
    "id": 17,
    "question": "How can you reduce memory usage of a numerical column in pandas?",
    "options": [
      "Converting it to 'object' type",
      "Downcasting to a smaller dtype (e.g., float64 to float32)",
      "Using .loc for indexing",
      "Setting the index to that column"
    ],
    "answer": "Downcasting to a smaller dtype (e.g., float64 to float32)",
    "explanation": "Lower precision dtypes (like float32 or int8) consume significantly less memory than the default float64/int64.",
    "difficulty": "Beginner"
  },
  {
    "id": 18,
    "question": "What does the `ignore_index=True` parameter do in `pd.concat()`?",
    "options": [
      "Drops the index column from the result",
      "Resets the index to a default range integer index",
      "Ignores duplicate indices without raising an error",
      "Sorts the index alphabetically"
    ],
    "answer": "Resets the index to a default range integer index",
    "explanation": "When concatenating, ignore_index=True creates a new continuous integer index (0, 1, 2...) instead of preserving original indices.",
    "difficulty": "Beginner"
  },
  {
    "id": 19,
    "question": "Which function calculates the frequency of unique values in a Series?",
    "options": [
      "df.count()",
      "df.unique()",
      "df.value_counts()",
      "df.freq()"
    ],
    "answer": "df.value_counts()",
    "explanation": "value_counts() returns a Series containing counts of unique values. unique() only returns the array of unique values.",
    "difficulty": "Beginner"
  },
  {
    "id": 20,
    "question": "What is the difference between `df.isna()` and `df.isnull()`?",
    "options": [
      "isna checks for strings, isnull checks for numbers",
      "There is no difference; they are aliases",
      "isna removes rows, isnull removes columns",
      "isna returns True, isnull returns False"
    ],
    "answer": "There is no difference; they are aliases",
    "explanation": "isna() and isnull() are functionally identical and interchangeable; pandas uses both names for API consistency.",
    "difficulty": "Beginner"
  },
  {
    "id": 21,
    "question": "Which parameter in `read_csv` allows you to specify a list of columns to read from the file?",
    "options": [
      "cols",
      "usecols",
      "columns",
      "names"
    ],
    "answer": "usecols",
    "explanation": "usecols accepts a list of column names or integers to filter the data upon loading, reducing memory usage.",
    "difficulty": "Beginner"
  },
  {
    "id": 22,
    "question": "When using `df.groupby('col')`, which aggregation method returns the first row of each group?",
    "options": [
      ".first()",
      ".head()",
      ".min()",
      ".top()"
    ],
    "answer": ".first()",
    "explanation": ".first() calculates the first valid entry of the group. .head() is a display method and not a specific groupby aggregation.",
    "difficulty": "Beginner"
  },
  {
    "id": 23,
    "question": "What does `df.sort_values(by='col')` do by default regarding missing values (NaN)?",
    "options": [
      "Places them at the beginning",
      "Places them at the end",
      "Removes them automatically",
      "Raises an error"
    ],
    "answer": "Places them at the end",
    "explanation": "By default, NaNs are sorted to the end of the Series or DataFrame, regardless of ascending order.",
    "difficulty": "Beginner"
  },
  {
    "id": 24,
    "question": "Which attribute returns the underlying NumPy array of the DataFrame?",
    "options": [
      "df.values",
      "df.array",
      "df.numpy",
      "df.data"
    ],
    "answer": "df.values",
    "explanation": "The .values property returns the NumPy ndarray representation of the DataFrame.",
    "difficulty": "Beginner"
  },
  {
    "id": 25,
    "question": "What is the result of boolean indexing: `df[df['A'] > 5]`?",
    "options": [
      "A DataFrame containing only column 'A'",
      "A DataFrame containing rows where 'A' is greater than 5",
      "A boolean Series",
      "An integer index of matching rows"
    ],
    "answer": "A DataFrame containing rows where 'A' is greater than 5",
    "explanation": "Passing a boolean Series to `df[]` filters the DataFrame, returning only rows where the condition is True.",
    "difficulty": "Beginner"
  },
  {
    "id": 26,
    "question": "Which method is used to replace specific values in a DataFrame?",
    "options": [
      "df.update()",
      "df.replace()",
      "df.change()",
      "df.map()"
    ],
    "answer": "df.replace()",
    "explanation": "replace() allows replacing values (dicts, lists, or regex) with other values. update() aligns on indices.",
    "difficulty": "Beginner"
  },
  {
    "id": 27,
    "question": "What does `pd.to_datetime(df['col'])` accomplish?",
    "options": [
      "Converts a column to a string format",
      "Converts a column to datetime objects",
      "Extracts the time from a datetime column",
      "Calculates the time difference between rows"
    ],
    "answer": "Converts a column to datetime objects",
    "explanation": "to_datetime converts argument strings or integers to datetime64[ns] dtype, enabling time-series functionality.",
    "difficulty": "Beginner"
  },
  {
    "id": 28,
    "question": "How do you set a specific column as the index of a DataFrame?",
    "options": [
      "df.index = 'col'",
      "df.set_index('col')",
      "df.reindex('col')",
      "df.reset_index('col')"
    ],
    "answer": "df.set_index('col')",
    "explanation": "set_index() sets the DataFrame index (row labels) using one or more existing columns.",
    "difficulty": "Beginner"
  },
  {
    "id": 29,
    "question": "Which pandas data structure is 1-dimensional and homogeneously typed?",
    "options": [
      "DataFrame",
      "Series",
      "Panel",
      "Table"
    ],
    "answer": "Series",
    "explanation": "A Series is a 1-D labeled array capable of holding any data type but effectively homogeneous in optimized memory usage.",
    "difficulty": "Beginner"
  },
  {
    "id": 30,
    "question": "What is the purpose of the `inplace=True` parameter in methods like `df.drop()`?",
    "options": [
      "To return a new DataFrame by default",
      "To modify the existing DataFrame and return None",
      "To perform the operation in parallel",
      "To prevent errors if the item doesn't exist"
    ],
    "answer": "To modify the existing DataFrame and return None",
    "explanation": "inplace=True alters the object directly. The default (False) returns a copy of the object with the operation applied.",
    "difficulty": "Beginner"
  },
  {
    "id": 31,
    "question": "Which method provides a concise summary of a DataFrame's statistics (mean, max, min, etc.)?",
    "options": [
      "df.summary()",
      "df.describe()",
      "df.info()",
      "df.stats()"
    ],
    "answer": "df.describe()",
    "explanation": "describe() generates descriptive statistics for numeric columns. info() provides a concise summary of the dataframe structure (dtypes, non-nulls).",
    "difficulty": "Beginner"
  },
  {
    "id": 32,
    "question": "What is the main difference between `df.iloc` and `df.loc`?",
    "options": [
      "iloc selects by label, loc selects by position",
      "iloc selects by position, loc selects by label",
      "iloc is for rows, loc is for columns",
      "iloc returns a copy, loc returns a view"
    ],
    "answer": "iloc selects by position, loc selects by label",
    "explanation": "iloc is integer-location based indexing (0, 1, 2...), while loc is label based indexing ('a', 'b', 'c'... or date ranges).",
    "difficulty": "Beginner"
  },
  {
    "id": 33,
    "question": "How does Pandas handle operations on DataFrames with different indices during arithmetic?",
    "options": [
      "It throws a dimension mismatch error",
      "It aligns data on indices and introduces NaN where missing",
      "It automatically resets the indices to match",
      "It performs element-wise operations ignoring indices"
    ],
    "answer": "It aligns data on indices and introduces NaN where missing",
    "explanation": "Pandas automatically aligns indices during arithmetic; missing labels in either object result in NaN (missing values) in the result.",
    "difficulty": "Beginner"
  },
  {
    "id": 34,
    "question": "Which method is specifically used to handle duplicate rows in a DataFrame?",
    "options": [
      "df.drop_duplicates()",
      "df.unique()",
      "df.remove_repeats()",
      "df.deduplicate()"
    ],
    "answer": "df.drop_duplicates()",
    "explanation": "drop_duplicates() returns DataFrame with duplicate rows removed. unique() is a Series method.",
    "difficulty": "Beginner"
  },
  {
    "id": 35,
    "question": "When processing large files, why is specifying the `dtype` parameter in `read_csv` beneficial?",
    "options": [
      "It automatically corrects spelling errors",
      "It reduces memory usage by preventing type guessing",
      "It increases the read speed by skipping rows",
      "It converts CSV to JSON format"
    ],
    "answer": "It reduces memory usage by preventing type guessing",
    "explanation": "By explicitly stating dtypes (e.g., int32 instead of float64), you prevent pandas from using generic heavy types, conserving memory.",
    "difficulty": "Beginner"
  },
  {
    "id": 36,
    "question": "Which method is computationally most efficient for accessing a single scalar value by label in a DataFrame?",
    "options": [
      "df.loc[row, col]",
      "df.at[row, col]",
      "df.iloc[row, col]",
      "df.iat[row, col]"
    ],
    "answer": "df.at[row, col]",
    "explanation": ".at[] is optimized for scalar lookups by label, avoiding the overhead of slicing logic present in .loc[] or .iloc[].",
    "difficulty": "Advanced"
  },
  {
    "id": 37,
    "question": "What is the primary memory benefit of converting a column with low cardinality to the 'category' dtype?",
    "options": [
      "It enables parallel processing operations",
      "It stores values as integers instead of strings",
      "It automatically compresses the index to a hash table",
      "It converts the column to a sparse array"
    ],
    "answer": "It stores values as integers instead of strings",
    "explanation": "The category dtype uses a mapping of integer codes to unique values, significantly reducing memory usage when the number of unique values is small relative to the total rows.",
    "difficulty": "Advanced"
  },
  {
    "id": 38,
    "question": "When using pd.eval() for arithmetic operations on large DataFrames, what is the primary performance advantage?",
    "options": [
      "It automatically utilizes all available CPU cores",
      "It avoids intermediate memory allocation for temporary arrays",
      "It compiles the expression to C bytecode",
      "It converts the DataFrame to a NumPy array permanently"
    ],
    "answer": "It avoids intermediate memory allocation for temporary arrays",
    "explanation": "pd.eval() uses string expressions to compute operations in a single pass, preventing the creation of large temporary arrays that standard Python/Pandas arithmetic would generate.",
    "difficulty": "Advanced"
  },
  {
    "id": 39,
    "question": "Why is itertuples() generally faster than iterrows() for iterating over DataFrame rows?",
    "options": [
      "itertuples() does not create a new Series object for each row",
      "itertuples() uses C++ loops internally",
      "itertuples() preserves data types automatically",
      "iterrows() locks the GIL exclusively"
    ],
    "answer": "itertuples() does not create a new Series object for each row",
    "explanation": "iterrows() constructs a Series for every row, which is expensive due to dtype alignment and overhead; itertuples() returns a lightweight namedtupple.",
    "difficulty": "Advanced"
  },
  {
    "id": 40,
    "question": "Which strategy significantly improves performance when merging two DataFrames on their indexes?",
    "options": [
      "Setting the 'sort' parameter to False",
      "Using 'how=inner' instead of 'how=left'",
      "Pre-sorting both DataFrames by the join keys",
      "Converting keys to string type"
    ],
    "answer": "Pre-sorting both DataFrames by the join keys",
    "explanation": "Pandas uses a 'merge join' algorithm which can achieve O(n) linear time complexity if both DataFrames are already sorted by the join keys, avoiding expensive hash-based lookups.",
    "difficulty": "Advanced"
  },
  {
    "id": 41,
    "question": "In the context of Pandas 2.0+ and Copy-on-Write optimization, what behavior changes when chaining assignments?",
    "options": [
      "A 'SettingWithCopyWarning' is always raised",
      "Intermediate copies are eliminated by default",
      "Updates to a view modify the original object immediately",
      "Chained assignments become impossible and throw an error"
    ],
    "answer": "Updates to a view modify the original object immediately",
    "explanation": "Copy-on-Write ensures that any write operation (like chained assignment) triggers a deferred copy, allowing the mutation to occur on a copy that is then propagated back, preventing accidental corruption of the original data.",
    "difficulty": "Advanced"
  },
  {
    "id": 42,
    "question": "Which method is most appropriate for out-of-core processing when a dataset exceeds available RAM?",
    "options": [
      "Using pd.read_csv with the 'chunksize' parameter",
      "Setting 'low_memory=False' in pd.read_csv",
      "Converting all columns to 'object' dtype",
      "Using df.iterrows() to process one row at a time"
    ],
    "answer": "Using pd.read_csv with the 'chunksize' parameter",
    "explanation": "The 'chunksize' parameter returns a TextFileReader iterator, allowing the processing of data in manageable batches that fit into memory.",
    "difficulty": "Advanced"
  },
  {
    "id": 43,
    "question": "What is the function of pd.to_numeric() with the 'errors='coerce' argument?",
    "options": [
      "It automatically detects the optimal numeric dtype (int or float)",
      "It converts non-numeric strings to 0",
      "It converts non-numeric values to NaN (Not a Number)",
      "It raises an exception if any non-numeric value is found"
    ],
    "answer": "It converts non-numeric values to NaN (Not a Number)",
    "explanation": "Setting errors='coerce' forces the conversion of invalid parsing to NaN, allowing the column to be cast to a numeric type (float64) even with dirty data.",
    "difficulty": "Advanced"
  },
  {
    "id": 44,
    "question": "When using df.query(), what allows you to reference variables from the local environment?",
    "options": [
      "Prepending the variable with '@'",
      "Passing variables as a dictionary to the 'locals' argument",
      "Using the 'global' keyword inside the query string",
      "Query automatically accesses all local variables"
    ],
    "answer": "Prepending the variable with '@'",
    "explanation": "The query syntax uses the '@' character to distinguish Python environment variables from DataFrame column names.",
    "difficulty": "Advanced"
  },
  {
    "id": 45,
    "question": "Which dtype extension is required to store missing integers (NaN) in a Pandas Series without upcasting to Float64?",
    "options": [
      "int8",
      "object",
      "Int64",
      "Sparse[int64]"
    ],
    "answer": "Int64",
    "explanation": "Standard numpy integer dtypes do not support NaN. The nullable 'Int64' (capital I) extension array utilizes a bitmask to track missing values, retaining integer precision.",
    "difficulty": "Advanced"
  },
  {
    "id": 46,
    "question": "What is the primary purpose of using the 'engine=python' parameter in pd.read_csv?",
    "options": [
      "To enable automatic parallel processing",
      "To handle C-unsupported delimiters or complex regex separators",
      "To strictly enforce type inference",
      "To bypass the GIL for faster reading"
    ],
    "answer": "To handle C-unsupported delimiters or complex regex separators",
    "explanation": "The C engine is faster but less flexible; the Python engine is necessary for parsing with features like regex separators (sep) that the C parser cannot handle.",
    "difficulty": "Advanced"
  },
  {
    "id": 47,
    "question": "How does the 'memory_usage' method with 'deep=True' differ from the default calculation?",
    "options": [
      "It includes the memory consumed by the object index",
      "It calculates memory usage of object dtypes by actually inspecting their contents",
      "It recursively calculates memory usage of all dependent DataFrames",
      "It forces a garbage collection cycle before measuring"
    ],
    "answer": "It calculates memory usage of object dtypes by actually inspecting their contents",
    "explanation": "By default, object dtype memory usage is a rough estimate; 'deep=True' traverses the actual objects to calculate the true system memory consumption.",
    "difficulty": "Advanced"
  },
  {
    "id": 48,
    "question": "In a MultiIndex DataFrame, what is the effect of the 'dropna' parameter (default True) during a groupby operation?",
    "options": [
      "It removes rows with any NaN values before grouping",
      "It excludes keys with NaN values in the grouping columns from the result",
      "It fills NaN values with 0 for aggregation",
      "It drops the entire column if any NaN is present"
    ],
    "answer": "It excludes keys with NaN values in the grouping columns from the result",
    "explanation": "When dropna=True (default), rows containing NaN in the grouping columns are completely omitted from the aggregation results.",
    "difficulty": "Advanced"
  },
  {
    "id": 49,
    "question": "Which technique avoids the 'SettingWithCopyWarning' when modifying a subset of a DataFrame?",
    "options": [
      "Accessing the subset via .ix[]",
      "Using .copy() to explicitly create an independent DataFrame",
      "Setting pd.options.mode.chained_assignment to None",
      "Using .iloc[] instead of .loc[]"
    ],
    "answer": "Using .copy() to explicitly create an independent DataFrame",
    "explanation": "The warning usually indicates modification on a view; explicitly calling .copy() ensures you are working on a distinct object, clarifying intent and preventing the warning.",
    "difficulty": "Advanced"
  },
  {
    "id": 50,
    "question": "When using pd.merge, how does the 'validate' parameter contribute to data integrity?",
    "options": [
      "It checks if the merge keys are unique in the specified dataset (1:1, 1:m, m:1)",
      "It validates that the data types of the merge keys match",
      "It ensures no NaN values are created during the merge",
      "It checks memory availability before performing the merge"
    ],
    "answer": "It checks if the merge keys are unique in the specified dataset (1:1, 1:m, m:1)",
    "explanation": "The 'validate' argument (e.g., 'one_to_one') raises a MergeError if the cardinality of the keys violates the assumption, preventing accidental many-to-many duplication.",
    "difficulty": "Advanced"
  },
  {
    "id": 51,
    "question": "What is a key performance difference between df['col'].map() and df['col'].replace()?",
    "options": [
      "map() can only accept dictionaries",
      "replace() is optimized for vectorized operations on large datasets",
      "map() is generally faster for element-wise mappings via functions or dicts",
      "replace() modifies the DataFrame in place without creating a copy"
    ],
    "answer": "map() is generally faster for element-wise mappings via functions or dicts",
    "explanation": "While replace() offers more flexibility (lists, regex, dicts), map() is a Series method optimized for value-to-value mapping and often executes faster for simple lookups.",
    "difficulty": "Advanced"
  },
  {
    "id": 52,
    "question": "Which method is preferred to combine multiple DataFrames vertically (concatenation) when the number of DataFrames is large and variable?",
    "options": [
      "Repeatedly calling df.append() in a loop",
      "Collecting DataFrames in a list and calling pd.concat() once",
      "Using df.merge() with a common indicator",
      "Iterating and assigning values to a pre-allocated empty DataFrame"
    ],
    "answer": "Collecting DataFrames in a list and calling pd.concat() once",
    "explanation": "Repeated concatenation creates a new DataFrame every time (quadratic complexity). Collecting objects in a list and concatenating once operates in linear time.",
    "difficulty": "Advanced"
  },
  {
    "id": 53,
    "question": "How does the 'numexpr' library enhance the performance of pd.eval()?",
    "options": [
      "It compiles Pandas operations to machine code",
      "It optimizes the memory bandwidth usage for arithmetic expressions",
      "It provides multi-threading support for numerical expressions",
      "It replaces Pandas string operations with regex"
    ],
    "answer": "It optimizes the memory bandwidth usage for arithmetic expressions",
    "explanation": "numexpr reduces memory overhead by chunking arrays and evaluating expressions in a way that maximizes CPU cache register usage, significantly speeding up large array arithmetic.",
    "difficulty": "Advanced"
  },
  {
    "id": 54,
    "question": "What is the result of passing 'infer_datetime_format=True' to pd.to_datetime()?",
    "options": [
      "It forces parsing to ISO8601 format",
      "It speeds up parsing by detecting a fixed format at the start of the series",
      "It automatically converts all timezones to UTC",
      "It converts the column to a PeriodIndex"
    ],
    "answer": "It speeds up parsing by detecting a fixed format at the start of the series",
    "explanation": "If the format is consistent, inferring it once and applying it to the whole series is significantly faster than parsing every string individually.",
    "difficulty": "Advanced"
  },
  {
    "id": 55,
    "question": "Which operation is best suited to visualize the density of data in a large DataFrame (optional context for data cleaning)?",
    "options": [
      "df.info()",
      "df.duplicated()",
      "df.isnull().sum()",
      "df.plot.density()"
    ],
    "answer": "df.plot.density()",
    "explanation": "Distractor removed. Replacing with: How does 'dtyp_backend='pyarrow'' in Pandas 2.0 help? It enables zero-copy operations.",
    "difficulty": "Advanced"
  },
  {
    "id": 56,
    "question": "What is the primary advantage of using the 'pyarrow' backend for data types in Pandas?",
    "options": [
      "It enables automatic garbage collection",
      "It utilizes zero-copy, memory-mapped arrays for efficiency",
      "It enforces strict typing for all columns",
      "It converts all code to C++"
    ],
    "answer": "It utilizes zero-copy, memory-mapped arrays for efficiency",
    "explanation": "Pyarrow-backed types allow Pandas to leverage Apache Arrow's memory-efficient, immutable structures, reducing overhead and enabling interoperability.",
    "difficulty": "Advanced"
  },
  {
    "id": 57,
    "question": "What does the 'method='ffill'' argument do in df.resample()?",
    "options": [
      "It drops rows with missing forward values",
      "It propagates the last valid observation forward to fill gaps",
      "It applies a linear interpolation",
      "It aggregates data using the 'first' function"
    ],
    "answer": "It propagates the last valid observation forward to fill gaps",
    "explanation": "'ffill' (forward fill) carries the last known value forward to fill new time intervals created by upsampling.",
    "difficulty": "Advanced"
  },
  {
    "id": 58,
    "question": "Why is .values discouraged in favor of .to_numpy() or specific accessors in modern Pandas?",
    "options": [
      ".values creates a copy, increasing memory usage",
      ".values returns a NumPy array regardless of extension types (losing information)",
      ".values cannot handle datetime objects",
      ".values is deprecated in Python 3.10+"
    ],
    "answer": ".values returns a NumPy array regardless of extension types (losing information)",
    "explanation": ".values can convert nullable types (like Int64 or strings) to object dtype or float64 silently. .to_numpy() preserves types or explicitly handles the conversion.",
    "difficulty": "Advanced"
  },
  {
    "id": 59,
    "question": "What is the impact of the 'sparse' parameter in df.pivot()?",
    "options": [
      "It compresses the resulting MultiIndex",
      "It returns a SparseDataFrame if the pivot contains many NaNs",
      "It automatically fills NaN with 0",
      "It reduces the dimensionality of the output"
    ],
    "answer": "It returns a SparseDataFrame if the pivot contains many NaNs",
    "explanation": "If True, the pivot operation creates a DataFrame with sparse values for the structural fill values (NaN), saving memory on wide datasets with lots of missing data.",
    "difficulty": "Advanced"
  },
  {
    "id": 60,
    "question": "How does 'groupby(..., observed=True)' affect categorical data grouping?",
    "options": [
      "It includes all category combinations, even if absent",
      "It only returns categories actually observed in the data",
      "It sorts the groups alphabetically",
      "It converts categorical columns to strings"
    ],
    "answer": "It only returns categories actually observed in the data",
    "explanation": "By default (False), groupby returns all possible category combinations (potentially creating many empty rows). 'observed=True' filters for existing groups.",
    "difficulty": "Advanced"
  },
  {
    "id": 61,
    "question": "Which method is specifically designed to expand a list-like column into individual rows (repeating index values)?",
    "options": [
      "df.stack()",
      "df.explode()",
      "df.melt()",
      "df.unstack()"
    ],
    "answer": "df.explode()",
    "explanation": "explode() transforms each element in a list-like column to a separate row, replicating the index values to maintain alignment with other columns.",
    "difficulty": "Advanced"
  },
  {
    "id": 62,
    "question": "In the context of string operations, what is the main advantage of df['col'].str accessor over standard Python string methods?",
    "options": [
      "It automatically handles NaN values without errors",
      "It runs on the GPU",
      "It modifies the DataFrame in place",
      "It bypasses the Pandas index"
    ],
    "answer": "It automatically handles NaN values without errors",
    "explanation": "The .str accessor vectorizes operations and gracefully handles NaN by propagating them, whereas standard Python methods would raise exceptions or require checks.",
    "difficulty": "Advanced"
  },
  {
    "id": 63,
    "question": "What does the 'as_index=False' parameter in df.groupby() do to the result?",
    "options": [
      "It resets the index of the original DataFrame",
      "It prevents the grouping keys from becoming the index of the result",
      "It creates a MultiIndex on the columns",
      "It disables the use of the index for sorting"
    ],
    "answer": "It prevents the grouping keys from becoming the index of the result",
    "explanation": "By default, groupby uses the unique keys as the index. 'as_index=False' returns a standard RangeIndex (0, 1, 2...) with keys as regular columns.",
    "difficulty": "Advanced"
  },
  {
    "id": 64,
    "question": "Which technique effectively reduces memory usage for a DataFrame containing a column with repetitive strings (e.g., 'Department' names)?",
    "options": [
      "Converting the column to 'object' type",
      "Applying .str.lower() to normalize text",
      "Using pd.factorize() or 'category' dtype",
      "Splitting the column into binary indicators"
    ],
    "answer": "Using pd.factorize() or 'category' dtype",
    "explanation": "Both factorize (integer encoding) and category dtype map repetitive strings to integers, drastically reducing memory overhead compared to storing raw string objects.",
    "difficulty": "Advanced"
  },
  {
    "id": 65,
    "question": "When using df.assign(), what makes it compatible with method chaining?",
    "options": [
      "It modifies the DataFrame in place",
      "It returns a new DataFrame with the new columns added",
      "It accepts lambda functions to refer to previously created columns in the same call",
      "It automatically converts data types to int32"
    ],
    "answer": "It accepts lambda functions to refer to previously created columns in the same call",
    "explanation": "df.assign() allows the use of lambda functions that reference other columns defined in the same assign call (using the previous state), enabling sequential transformations.",
    "difficulty": "Advanced"
  },
  {
    "id": 66,
    "question": "What is the computational complexity trade-off of using a hashed index vs. a sorted index?",
    "options": [
      "Hashed indexes are slower for range queries but faster for equality lookups",
      "Sorted indexes consume 2x the memory of hashed indexes",
      "Hashed indexes cannot handle NaN values",
      "Sorted indexes are not compatible with merge operations"
    ],
    "answer": "Hashed indexes are slower for range queries but faster for equality lookups",
    "explanation": "Sorted indexes (Tree-based) are optimal for slicing and range queries (O(log n) or O(1) for slice), while Hash indexes (if supported/implemented via logic) are generally better for direct point lookups but inefficient for ranges.",
    "difficulty": "Advanced"
  }
]