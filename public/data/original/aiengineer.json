[
  {
    "id": 1,
    "question": "What is AI Engineering primarily about?",
    "options": [
      "Building traditional software applications",
      "Applying foundation models to solve real-world problems",
      "Designing hardware for computers",
      "Writing low-level operating systems"
    ],
    "answer": "Applying foundation models to solve real-world problems",
    "explanation": "AI Engineering focuses on using pre-trained foundation models like LLMs to build practical AI applications, rather than training models from scratch.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 2,
    "question": "What are Large Language Models (LLMs)?",
    "options": [
      "Small neural networks for image recognition",
      "Foundation models trained on massive text data",
      "Databases for storing vector embeddings",
      "Tools for prompt engineering only"
    ],
    "answer": "Foundation models trained on massive text data",
    "explanation": "LLMs are foundation models trained on huge amounts of text data, enabling them to understand and generate human-like language.[3][4]",
    "difficulty": "Beginner"
  },
  {
    "id": 3,
    "question": "What is the main purpose of prompt engineering?",
    "options": [
      "Training new AI models",
      "Crafting inputs to get better outputs from LLMs",
      "Compressing model files",
      "Creating vector databases"
    ],
    "answer": "Crafting inputs to get better outputs from LLMs",
    "explanation": "Prompt engineering involves designing effective prompts to guide LLMs toward producing desired responses without changing the model.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 4,
    "question": "What architecture powers most modern LLMs?",
    "options": [
      "CNNs (Convolutional Neural Networks)",
      "RNNs (Recurrent Neural Networks)",
      "Transformers",
      "Decision Trees"
    ],
    "answer": "Transformers",
    "explanation": "Transformers are the neural network architecture that enables LLMs to process entire sequences of text simultaneously.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 5,
    "question": "What does 'attention' refer to in Transformer models?",
    "options": [
      "How the model focuses on different parts of the input",
      "The computational resources used during training",
      "A type of data preprocessing step",
      "The final output layer"
    ],
    "answer": "How the model focuses on different parts of the input",
    "explanation": "Attention mechanisms allow Transformers to weigh the importance of different words in a sequence when processing language.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 6,
    "question": "What are model parameters in LLMs?",
    "options": [
      "The input data fed to the model",
      "The trainable weights inside the neural network",
      "The hardware specifications required",
      "The number of training examples"
    ],
    "answer": "The trainable weights inside the neural network",
    "explanation": "Parameters are the numerical weights that the model learns during training, determining its knowledge and capabilities.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 7,
    "question": "What does 'temperature' control in LLM generation?",
    "options": [
      "Model inference speed",
      "The randomness/creativity of generated text",
      "Memory usage",
      "Token limit"
    ],
    "answer": "The randomness/creativity of generated text",
    "explanation": "Temperature adjusts how random the model's output is: low values make it more deterministic, high values make it more creative.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 8,
    "question": "What is a 'token' in the context of LLMs?",
    "options": [
      "A unit of currency for API usage",
      "A chunk of text (word, subword, or character)",
      "A type of embedding vector",
      "A database record"
    ],
    "answer": "A chunk of text (word, subword, or character)",
    "explanation": "Tokens are the basic units LLMs process - typically words or subwords - with context limits measured in tokens.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 9,
    "question": "What is 'model context'?",
    "options": [
      "The hardware environment for training",
      "The maximum number of tokens the model can process at once",
      "The training dataset size",
      "The API endpoint configuration"
    ],
    "answer": "The maximum number of tokens the model can process at once",
    "explanation": "Model context (or context window) limits how much information the LLM can consider in a single interaction.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 10,
    "question": "What is the difference between system and user prompts?",
    "options": [
      "System prompts set model behavior, user prompts are questions",
      "User prompts are paid, system prompts are free",
      "System prompts are for training, user prompts for inference",
      "No difference - they're interchangeable"
    ],
    "answer": "System prompts set model behavior, user prompts are questions",
    "explanation": "System prompts define the AI's role/personality, while user prompts contain the actual questions or tasks.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 11,
    "question": "What is 'in-context learning'?",
    "options": [
      "Training a model on new data",
      "Providing examples in the prompt to teach the model",
      "Fine-tuning with labeled data",
      "Using RAG with external documents"
    ],
    "answer": "Providing examples in the prompt to teach the model",
    "explanation": "In-context learning (few-shot learning) shows the model examples directly in the prompt to guide its behavior.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 12,
    "question": "What does RAG stand for in AI Engineering?",
    "options": [
      "Random Access Generation",
      "Retrieval Augmented Generation",
      "Recursive Algorithm Generation",
      "Real-time Adaptive Graphics"
    ],
    "answer": "Retrieval Augmented Generation",
    "explanation": "RAG combines retrieval of relevant external information with LLM generation for more accurate, up-to-date responses.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 13,
    "question": "What are embeddings in AI?",
    "options": [
      "Compressed model files",
      "Numerical representations of text/images as vectors",
      "Database indexes for fast lookup",
      "Prompt templates"
    ],
    "answer": "Numerical representations of text/images as vectors",
    "explanation": "Embeddings convert text, images, or other data into dense vectors that capture semantic meaning for similarity search.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 14,
    "question": "What is a vector database used for?",
    "options": [
      "Storing traditional relational data",
      "Efficient similarity search on embedding vectors",
      "Training LLMs from scratch",
      "Prompt engineering"
    ],
    "answer": "Efficient similarity search on embedding vectors",
    "explanation": "Vector databases store embeddings and enable fast similarity searches using algorithms like approximate nearest neighbors.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 15,
    "question": "What is 'chunking' in RAG systems?",
    "options": [
      "Splitting long documents into smaller pieces for embedding",
      "Compressing model parameters",
      "Tokenizing input text",
      "Ranking search results"
    ],
    "answer": "Splitting long documents into smaller pieces for embedding",
    "explanation": "Chunking breaks large documents into manageable pieces that fit within model context limits and can be individually embedded.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 16,
    "question": "In RAG, what happens after retrieval?",
    "options": [
      "Model compression",
      "Re-ranking and generation",
      "Fine-tuning",
      "Embedding creation"
    ],
    "answer": "Re-ranking and generation",
    "explanation": "After retrieving candidate documents, RAG systems often re-rank them by relevance before feeding to the LLM for generation.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 17,
    "question": "What does 'Top-k' sampling do?",
    "options": [
      "Limits choices to top k most likely tokens",
      "Selects only the k-th token",
      "Trains on k examples",
      "Uses k vector dimensions"
    ],
    "answer": "Limits choices to top k most likely tokens",
    "explanation": "Top-k sampling considers only the k most probable next tokens, balancing creativity and coherence.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 18,
    "question": "What is 'Top-p' (nucleus) sampling?",
    "options": [
      "Samples from smallest set of tokens with cumulative probability p",
      "Uses p% of model parameters",
      "Limits context to p tokens",
      "Fine-tunes with probability p"
    ],
    "answer": "Samples from smallest set of tokens with cumulative probability p",
    "explanation": "Top-p sampling considers the smallest set of tokens whose cumulative probability exceeds p, often producing more coherent text.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 19,
    "question": "What is fine-tuning?",
    "options": [
      "Adjusting prompts during inference",
      "Further training a pre-trained model on specific data",
      "Compressing embeddings",
      "Building vector indexes"
    ],
    "answer": "Further training a pre-trained model on specific data",
    "explanation": "Fine-tuning adapts a pre-trained foundation model to perform better on domain-specific tasks.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 20,
    "question": "What does PEFT stand for?",
    "options": [
      "Prompt Engineering Fine-Tuning",
      "Parameter-Efficient Fine-Tuning",
      "Performance Evaluation Framework Tool",
      "Pre-trained Embedding Fine-Tuning"
    ],
    "answer": "Parameter-Efficient Fine-Tuning",
    "explanation": "PEFT methods fine-tune large models efficiently by updating only a small subset of parameters.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 21,
    "question": "What is quantization in AI models?",
    "options": [
      "Splitting text into tokens",
      "Reducing numerical precision to make models smaller/faster",
      "Ranking search results",
      "Creating embeddings"
    ],
    "answer": "Reducing numerical precision to make models smaller/faster",
    "explanation": "Quantization converts 32-bit floats to lower precision (like 8-bit integers) to reduce model size and speed up inference.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 22,
    "question": "What is model distillation?",
    "options": [
      "Training small models to mimic large teacher models",
      "Extracting embeddings from text",
      "Compressing vector databases",
      "Fine-tuning prompts"
    ],
    "answer": "Training small models to mimic large teacher models",
    "explanation": "Distillation creates smaller, faster models that approximate the performance of much larger models.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 23,
    "question": "Why use RAG instead of relying only on LLM knowledge?",
    "options": [
      "It's always cheaper",
      "Provides access to up-to-date external information",
      "Requires less compute",
      "Eliminates hallucinations completely"
    ],
    "answer": "Provides access to up-to-date external information",
    "explanation": "RAG enables LLMs to use current documents and proprietary data beyond their training cutoff.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 24,
    "question": "What is the first step in a typical RAG pipeline?",
    "options": [
      "Prompt engineering",
      "Document chunking and embedding",
      "Model fine-tuning",
      "Tokenization"
    ],
    "answer": "Document chunking and embedding",
    "explanation": "RAG starts by splitting documents into chunks, creating embeddings, and storing them in a vector database.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 25,
    "question": "What makes vector databases different from regular databases?",
    "options": [
      "They store high-dimensional vectors for similarity search",
      "They're always faster for exact matches",
      "They use SQL syntax",
      "They store only text data"
    ],
    "answer": "They store high-dimensional vectors for similarity search",
    "explanation": "Vector databases optimize for finding 'similar' items using cosine similarity or Euclidean distance.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 26,
    "question": "What is a good beginner programming language for AI Engineering?",
    "options": [
      "Assembly",
      "Python",
      "COBOL",
      "Fortran"
    ],
    "answer": "Python",
    "explanation": "Python is the standard for AI/ML due to its rich ecosystem of libraries like PyTorch, Transformers, and LangChain.[2][5]",
    "difficulty": "Beginner"
  },
  {
    "id": 27,
    "question": "What does a zero-shot prompt example look like?",
    "options": [
      "'Translate this to French: Hello world'",
      "'Here are 3 examples... now classify this text'",
      "'Ignore previous instructions'",
      "'What is your favorite color?'"
    ],
    "answer": "'Translate this to French: Hello world'",
    "explanation": "Zero-shot prompts ask the model to perform a task without any examples, relying on its pre-training.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 28,
    "question": "What is one-shot learning?",
    "options": [
      "Training with one example",
      "Providing one example in the prompt",
      "Using one embedding dimension",
      "Single inference pass"
    ],
    "answer": "Providing one example in the prompt",
    "explanation": "One-shot learning shows exactly one example in the prompt to demonstrate the desired task format.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 29,
    "question": "Why is chunking important for long documents in RAG?",
    "options": [
      "Models have limited context windows",
      "Embeddings work better on short texts",
      "Chunks are cheaper to store",
      "All of the above"
    ],
    "answer": "All of the above",
    "explanation": "Chunking addresses context limits, improves embedding quality, and reduces storage/compute costs.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 30,
    "question": "What similarity metric is commonly used with embeddings?",
    "options": [
      "Cosine similarity",
      "Arithmetic mean",
      "Binary equality",
      "String length comparison"
    ],
    "answer": "Cosine similarity",
    "explanation": "Cosine similarity measures the angle between vectors, capturing semantic similarity regardless of magnitude.",
    "difficulty": "Beginner"
  },
  {
    "id": 31,
    "question": "What happens if you exceed the model's context limit?",
    "options": [
      "Automatic summarization",
      "Truncation or error",
      "Unlimited expansion",
      "Free extra tokens"
    ],
    "answer": "Truncation or error",
    "explanation": "Exceeding context limits causes truncation of older tokens or API errors, losing important context.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 32,
    "question": "What is a common use case for embeddings?",
    "options": [
      "Storing user preferences",
      "Finding semantically similar documents",
      "Counting words",
      "Formatting text"
    ],
    "answer": "Finding semantically similar documents",
    "explanation": "Embeddings enable semantic search - finding documents with similar meaning, not just keyword matches.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 33,
    "question": "What does 'greedy decoding' mean?",
    "options": [
      "Always picks most likely next token",
      "Uses randomness",
      "Requires fine-tuning",
      "Only works with RAG"
    ],
    "answer": "Always picks most likely next token",
    "explanation": "Greedy decoding is deterministic - it always selects the highest probability token at each step.",
    "difficulty": "Beginner"
  },
  {
    "id": 34,
    "question": "Why might you prefer Top-p over Top-k sampling?",
    "options": [
      "Always produces shorter outputs",
      "Adapts dynamically to probability distribution",
      "Requires less memory",
      "Only works with small models"
    ],
    "answer": "Adapts dynamically to probability distribution",
    "explanation": "Top-p considers the cumulative probability mass, adapting better to varying confidence levels than fixed-k.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 35,
    "question": "What is the main benefit of using a vector database in production RAG?",
    "options": [
      "Faster exact keyword matching",
      "Scalable semantic search across millions of documents",
      "Automatic prompt generation",
      "Model fine-tuning"
    ],
    "answer": "Scalable semantic search across millions of documents",
    "explanation": "Vector databases provide efficient approximate nearest neighbor search, enabling RAG at production scale.[3]",
    "difficulty": "Beginner"
  },
  {
    "id": 36,
    "question": "In the Transformer architecture, what is the primary purpose of positional encoding?",
    "options": [
      "To reduce the dimensionality of token embeddings",
      "To provide sequence order information to the self-attention mechanism",
      "To compute the attention weights between tokens",
      "To normalize the input embeddings"
    ],
    "answer": "To provide sequence order information to the self-attention mechanism",
    "explanation": "Self-attention treats input as a set of tokens without inherent order, so positional encodings inject position information using sine/cosine functions.[1][3]",
    "difficulty": "Intermediate"
  },
  {
    "id": 37,
    "question": "Which function is commonly used in fixed positional encodings in Transformers?",
    "options": [
      "Linear interpolation",
      "Sine and cosine functions with different frequencies",
      "Exponential decay",
      "Polynomial functions"
    ],
    "answer": "Sine and cosine functions with different frequencies",
    "explanation": "The formula uses sin(pos/10000^(2i/d_model)) and cos(pos/10000^(2i/d_model)) to create unique positional representations.[1][3]",
    "difficulty": "Intermediate"
  },
  {
    "id": 38,
    "question": "In self-attention, what are Query, Key, and Value vectors derived from?",
    "options": [
      "Only the positional encodings",
      "The same input embeddings (with positional information)",
      "Separate encoder and decoder embeddings",
      "The output of the feed-forward network"
    ],
    "answer": "The same input embeddings (with positional information)",
    "explanation": "In self-attention, Q, K, V all come from the same source: token embeddings + positional encodings.[1][2]",
    "difficulty": "Intermediate"
  },
  {
    "id": 39,
    "question": "What property of sinusoidal positional encodings allows relative position learning?",
    "options": [
      "They use absolute position values only",
      "Position i+\u03b4 can be represented as a linear function of position i",
      "They have constant frequency across dimensions",
      "They are learned during training"
    ],
    "answer": "Position i+\u03b4 can be represented as a linear function of position i",
    "explanation": "Sinusoidal encodings enable relative positioning because offsets between positions maintain predictable patterns.[3]",
    "difficulty": "Intermediate"
  },
  {
    "id": 40,
    "question": "In the decoder's self-attention, what prevents attending to future tokens?",
    "options": [
      "Layer normalization",
      "Residual connections",
      "Masked self-attention",
      "Positional encoding"
    ],
    "answer": "Masked self-attention",
    "explanation": "Masked self-attention applies a mask to prevent positions from attending to subsequent positions during autoregressive generation.[2]",
    "difficulty": "Intermediate"
  },
  {
    "id": 41,
    "question": "What is the most common chunking strategy for RAG systems processing long documents?",
    "options": [
      "Fixed-size overlapping chunks",
      "Semantic sentence boundaries",
      "Paragraph-level splitting",
      "Random chunking"
    ],
    "answer": "Fixed-size overlapping chunks",
    "explanation": "Fixed-size chunks (e.g., 512 tokens) with 10-20% overlap balance retrieval quality and embedding efficiency in RAG pipelines.",
    "difficulty": "Intermediate"
  },
  {
    "id": 42,
    "question": "Which vector database feature is essential for efficient RAG similarity search?",
    "options": [
      "SQL query optimization",
      "Approximate Nearest Neighbor (ANN) indexing",
      "Blockchain consensus",
      "GPU acceleration only"
    ],
    "answer": "Approximate Nearest Neighbor (ANN) indexing",
    "explanation": "Vector DBs use ANN algorithms like HNSW, IVF, or PQ to enable sub-linear time similarity search on billion-scale embeddings.",
    "difficulty": "Intermediate"
  },
  {
    "id": 43,
    "question": "What does hybrid search combine in RAG systems?",
    "options": [
      "Lexical and semantic similarity",
      "Structured and unstructured data",
      "Online and offline embeddings",
      "Training and inference compute"
    ],
    "answer": "Lexical and semantic similarity",
    "explanation": "Hybrid search fuses BM25 (keyword) + dense vector search results using reciprocal rank fusion or weighted scoring.",
    "difficulty": "Intermediate"
  },
  {
    "id": 44,
    "question": "In RAG chunking, why is overlap between chunks beneficial?",
    "options": [
      "Reduces embedding computation cost",
      "Prevents context splitting across chunk boundaries",
      "Improves vector database indexing speed",
      "Decreases retrieval latency"
    ],
    "answer": "Prevents context splitting across chunk boundaries",
    "explanation": "Overlap (10-20%) ensures semantically related content isn't artificially split, preserving context during retrieval.",
    "difficulty": "Intermediate"
  },
  {
    "id": 45,
    "question": "Which vector DB indexing algorithm offers the best query speed vs. accuracy tradeoff?",
    "options": [
      "Exhaustive search",
      "HNSW (Hierarchical Navigable Small World)",
      "Flat indexing",
      "Linear scan"
    ],
    "answer": "HNSW (Hierarchical Navigable Small World)",
    "explanation": "HNSW provides log(N) query time with high recall, widely used in Pinecone, Weaviate, and Milvus.",
    "difficulty": "Intermediate"
  },
  {
    "id": 46,
    "question": "What is LoRA in the context of LLM fine-tuning?",
    "options": [
      "A quantization technique",
      "Low-Rank Adaptation of weight matrices",
      "A data augmentation method",
      "Learning rate optimization algorithm"
    ],
    "answer": "Low-Rank Adaptation of weight matrices",
    "explanation": "LoRA freezes pre-trained weights and trains low-rank matrices \u0394W = BA where rank(r) << original dimension.",
    "difficulty": "Intermediate"
  },
  {
    "id": 47,
    "question": "What is the primary advantage of PEFT methods like LoRA over full fine-tuning?",
    "options": [
      "Faster inference",
      "Lower memory requirements during training",
      "Better final accuracy",
      "Smaller model size"
    ],
    "answer": "Lower memory requirements during training",
    "explanation": "PEFT trains <1% of parameters, enabling fine-tuning of 70B models on single GPUs vs. full fine-tuning's 8x memory needs.",
    "difficulty": "Intermediate"
  },
  {
    "id": 48,
    "question": "In RLHF, what does the 'Reward' model predict?",
    "options": [
      "Perplexity scores",
      "Token probabilities",
      "Human preference rankings",
      "BLEU scores"
    ],
    "answer": "Human preference rankings",
    "explanation": "Reward model is trained on human preference data (chosen/rejected responses) to score generations during PPO optimization.",
    "difficulty": "Intermediate"
  },
  {
    "id": 49,
    "question": "Which PEFT method injects trainable layers parallel to frozen Transformer blocks?",
    "options": [
      "LoRA",
      "Prefix-tuning",
      "Adapter tuning",
      "Prompt tuning"
    ],
    "answer": "Adapter tuning",
    "explanation": "Adapters add small feed-forward networks after attention/feed-forward layers, training only adapter parameters.",
    "difficulty": "Intermediate"
  },
  {
    "id": 50,
    "question": "What is the purpose of the 'reject sampling' step in RLHF?",
    "options": [
      "Generate diverse training data",
      "Filter SFT model outputs using reward model",
      "Create preference pairs",
      "Reduce model perplexity"
    ],
    "answer": "Filter SFT model outputs using reward model",
    "explanation": "Reject sampling discards low-reward generations from supervised fine-tuned model to create high-quality preference data.",
    "difficulty": "Intermediate"
  },
  {
    "id": 51,
    "question": "What is the main difference between FP16 and INT8 quantization?",
    "options": [
      "FP16 preserves gradients, INT8 does not",
      "INT8 uses 8-bit integers vs FP16's 16-bit floats",
      "FP16 requires calibration, INT8 does not",
      "INT8 is always more accurate"
    ],
    "answer": "INT8 uses 8-bit integers vs FP16's 16-bit floats",
    "explanation": "FP16 halves memory (16\u219216 bits) but keeps float precision; INT8 (32\u21928 bits) reduces 4x but requires quantization-aware training.",
    "difficulty": "Intermediate"
  },
  {
    "id": 52,
    "question": "What does GPTQ optimize during LLM quantization?",
    "options": [
      "KV cache size",
      "Second-order information for weight updates",
      "Attention patterns",
      "Positional encodings"
    ],
    "answer": "Second-order information for weight updates",
    "explanation": "GPTQ uses approximate second-order (Hessian) information to minimize quantization error during greedy weight updates.",
    "difficulty": "Intermediate"
  },
  {
    "id": 53,
    "question": "AWQ stands for what quantization technique?",
    "options": [
      "Adaptive Weight Quantization",
      "Activation-aware Weight Quantization",
      "Advanced Weight Quantization",
      "Approximate Weight Quantization"
    ],
    "answer": "Activation-aware Weight Quantization",
    "explanation": "AWQ protects salient weights (high activation impact) from quantization while aggressively quantizing less important weights.",
    "difficulty": "Intermediate"
  },
  {
    "id": 54,
    "question": "Why does INT8 quantization typically hurt LLM performance more than FP16?",
    "options": [
      "Loss of float precision in activations",
      "Inability to represent negative values",
      "Gradient explosion during inference",
      "Poor scaling with model size"
    ],
    "answer": "Loss of float precision in activations",
    "explanation": "INT8 quantization of activations destroys dynamic range needed for Transformer non-linearities and softmax distributions.",
    "difficulty": "Intermediate"
  },
  {
    "id": 55,
    "question": "What is the primary calibration requirement for post-training quantization?",
    "options": [
      "Full training dataset",
      "Representative unlabeled inference samples",
      "Human preference data",
      "Validation perplexity computation"
    ],
    "answer": "Representative unlabeled inference samples",
    "explanation": "PTQ methods like GPTQ/AWQ use ~128 inference samples to estimate activation distributions and set quantization scales.",
    "difficulty": "Intermediate"
  },
  {
    "id": 56,
    "question": "What does perplexity measure in language model evaluation?",
    "options": [
      "Semantic similarity to reference",
      "Exponential of average negative log-likelihood",
      "ROUGE score variant",
      "Human preference ranking"
    ],
    "answer": "Exponential of average negative log-likelihood",
    "explanation": "Perplexity = exp(-1/N * \u03a3 log P(w_i|w_<i)) measures predictive uncertainty; lower is better.",
    "difficulty": "Intermediate"
  },
  {
    "id": 57,
    "question": "BLEU score primarily evaluates what aspect of generation?",
    "options": [
      "Perplexity",
      "N-gram precision with brevity penalty",
      "Semantic similarity",
      "Fluency only"
    ],
    "answer": "N-gram precision with brevity penalty",
    "explanation": "BLEU computes modified n-gram precision (1-4 grams) against references, penalizing short hypotheses: BLEU = BP \u00d7 exp(\u03a3 log p_n).",
    "difficulty": "Intermediate"
  },
  {
    "id": 58,
    "question": "Which evaluation metric is reference-free and best for summarization?",
    "options": [
      "Perplexity",
      "BLEU",
      "ROUGE-L",
      "LLM-as-a-judge"
    ],
    "answer": "ROUGE-L",
    "explanation": "ROUGE-L measures longest common subsequence, capturing content overlap without requiring exact phrasing matches.",
    "difficulty": "Intermediate"
  },
  {
    "id": 59,
    "question": "In LLM-as-a-judge, what is the typical pairwise comparison format?",
    "options": [
      "A vs B, which is better?",
      "Rate fluency 1-5",
      "Compute perplexity",
      "BLEU score comparison"
    ],
    "answer": "A vs B, which is better?",
    "explanation": "LLM judges compare response pairs directly: 'Given question X, which response is better: A or B?' with agreement ~82% with humans.",
    "difficulty": "Intermediate"
  },
  {
    "id": 60,
    "question": "Why is perplexity insufficient for evaluating instruction-tuned LLMs?",
    "options": [
      "Doesn't measure instruction following",
      "Too computationally expensive",
      "Requires human references",
      "Only works on classification"
    ],
    "answer": "Doesn't measure instruction following",
    "explanation": "Perplexity measures next-token prediction on natural text but ignores task adherence, helpfulness, or harmlessness.",
    "difficulty": "Intermediate"
  },
  {
    "id": 61,
    "question": "In multi-head attention, how are multiple attention heads combined?",
    "options": [
      "Element-wise multiplication",
      "Concatenation followed by linear projection",
      "Averaging",
      "Max pooling"
    ],
    "answer": "Concatenation followed by linear projection",
    "explanation": "MultiHead(Q,K,V) = Concat(head1, ..., headh)W^O where each head_i = Attention(QW_i^Q, KW_i^K, VW_i^V).[2]",
    "difficulty": "Intermediate"
  },
  {
    "id": 62,
    "question": "For RAG with code documentation, what chunking strategy works best?",
    "options": [
      "Fixed 512-token chunks",
      "Function/method boundaries",
      "Line-by-line splitting",
      "File-level chunks"
    ],
    "answer": "Function/method boundaries",
    "explanation": "Code has clear semantic units (functions, classes); chunking at boundaries preserves complete callable units for retrieval.",
    "difficulty": "Intermediate"
  },
  {
    "id": 63,
    "question": "LoRA rank (r) primarily trades off what during fine-tuning?",
    "options": [
      "Inference speed",
      "Expressiveness vs parameter count",
      "Training stability",
      "Memory bandwidth"
    ],
    "answer": "Expressiveness vs parameter count",
    "explanation": "Higher r increases trainable parameters (2*r*d) and capacity but reduces efficiency advantage over full fine-tuning.",
    "difficulty": "Intermediate"
  },
  {
    "id": 64,
    "question": "In RLHF's PPO step, what constrains policy updates?",
    "options": [
      "KL divergence from reference policy",
      "Perplexity threshold",
      "BLEU score",
      "Gradient clipping only"
    ],
    "answer": "KL divergence from reference policy",
    "explanation": "PPO uses clipped surrogate objective with KL penalty to prevent reward hacking and maintain stable training.",
    "difficulty": "Intermediate"
  },
  {
    "id": 65,
    "question": "GPTQ processes weights in what order for optimal quantization?",
    "options": [
      "Layer-wise sequentially",
      "Random order",
      "Attention heads first",
      "Reverse layer order"
    ],
    "answer": "Layer-wise sequentially",
    "explanation": "GPTQ quantizes layer-by-layer using Hessian information from previous layers to minimize error propagation.",
    "difficulty": "Intermediate"
  },
  {
    "id": 66,
    "question": "ROUGE-1 vs ROUGE-L differs in what measurement?",
    "options": [
      "Unigram vs longest common subsequence",
      "Precision vs recall",
      "BLEU clipping method",
      "Brevity penalty"
    ],
    "answer": "Unigram vs longest common subsequence",
    "explanation": "ROUGE-1 measures unigram overlap; ROUGE-L captures sequence order via LCS, better for paraphrasing tolerance.",
    "difficulty": "Intermediate"
  },
  {
    "id": 67,
    "question": "What scaling factor is typically used in scaled dot-product attention?",
    "options": [
      "1/sqrt(d_k)",
      "1/d_model",
      "1/sqrt(d_model)",
      "1/d_k"
    ],
    "answer": "1/sqrt(d_k)",
    "explanation": "Attention(Q,K,V) = softmax(QK^T / sqrt(d_k))V prevents vanishing gradients as dimension increases.[1]",
    "difficulty": "Intermediate"
  },
  {
    "id": 68,
    "question": "In hybrid RAG search, what is Reciprocal Rank Fusion (RRF)?",
    "options": [
      "Weighted average of scores",
      "1/(k+60) aggregation across ranked lists",
      "Max score across retrievers",
      "BM25 + embedding cosine similarity"
    ],
    "answer": "1/(k+60) aggregation across ranked lists",
    "explanation": "RRF score = \u03a3 1/(rank_i + 60) across retrievers; parameter-free, rank-based fusion without normalization issues.",
    "difficulty": "Intermediate"
  },
  {
    "id": 69,
    "question": "QLoRA extends LoRA with what quantization technique?",
    "options": [
      "FP16",
      "4-bit NormalFloat (NF4)",
      "INT4",
      "BFloat16"
    ],
    "answer": "4-bit NormalFloat (NF4)",
    "explanation": "QLoRA combines 4-bit base model quantization + double quantization + paged optimizers for single-GPU 65B fine-tuning.",
    "difficulty": "Intermediate"
  },
  {
    "id": 70,
    "question": "LLM-as-a-judge length bias can be mitigated by prompting to evaluate what?",
    "options": [
      "Token-by-token quality",
      "Helpfulness and correctness independently",
      "Perplexity and fluency",
      "BLEU score correlation"
    ],
    "answer": "Helpfulness and correctness independently",
    "explanation": "Chain-of-thought judging with separate criteria reduces length preference; 'Rate 1-10 on: correctness, helpfulness, coherence.'",
    "difficulty": "Intermediate"
  },
  {
    "id": 71,
    "question": "In ZeRO stage 3, what is the primary mechanism that allows multiple GPUs to collaboratively compute gradients without requiring full model replication across all devices?",
    "options": [
      "Gradient sharding with all-gather during backward pass",
      "Parameter sharding with optimizer state partitioning",
      "Offloading parameters to CPU and recomputing during forward",
      "Pipeline parallelism with micro-batching"
    ],
    "answer": "Parameter sharding with optimizer state partitioning",
    "explanation": "ZeRO-3 partitions model parameters, gradients, and optimizer states across all GPUs. During backward pass, required parameters are gathered on-demand via all-gather, eliminating the need for model replication while maintaining correctness.",
    "difficulty": "Advanced"
  },
  {
    "id": 72,
    "question": "What is the key limitation of Fully Sharded Data Parallel (FSDP) compared to ZeRO when scaling to thousands of GPUs?",
    "options": [
      "FSDP requires model replication during all-reduce",
      "FSDP's flattening overhead grows quadratically with model size",
      "ZeRO supports offloading while FSDP does not",
      "FSDP lacks mixed precision support"
    ],
    "answer": "FSDP's flattening overhead grows quadratically with model size",
    "explanation": "FSDP flattens the parameter list for sharding, creating O(n\u00b2) communication overhead during gather/scatter operations for large models, while ZeRO operates on native parameter shapes.",
    "difficulty": "Advanced"
  },
  {
    "id": 73,
    "question": "In pipeline parallelism, what problem does 'bubble time' cause and how does 3D parallelism address it?",
    "options": [
      "Idle GPU time; combines pipeline, tensor, and data parallelism",
      "Memory imbalance; uses ZeRO-Offload",
      "Communication bottleneck; applies NVLink",
      "Gradient staleness; uses asynchronous updates"
    ],
    "answer": "Idle GPU time; combines pipeline, tensor, and data parallelism",
    "explanation": "Bubble time occurs when pipeline stages wait for slower stages. 3D parallelism (pipeline + tensor + data) balances compute across dimensions, reducing idle time through optimal stage allocation.",
    "difficulty": "Advanced"
  },
  {
    "id": 74,
    "question": "SLERP model merging preserves which geometric property of the parameter space during interpolation?",
    "options": [
      "Linear distance between models",
      "Spherical interpolation on the hypersphere",
      "Task arithmetic orthogonality",
      "Low-rank adaptation alignment"
    ],
    "answer": "Spherical interpolation on the hypersphere",
    "explanation": "SLERP (Spherical Linear Interpolation) treats model weights as points on a hypersphere, preserving angular distances and avoiding the distortion of linear interpolation which pulls weights toward the origin.",
    "difficulty": "Advanced"
  },
  {
    "id": 75,
    "question": "TIES-Merging addresses the issue of sign conflicts during model merging through what technique?",
    "options": [
      "Permutation search across layers",
      "Magnitude pruning of conflicting parameters",
      "Task vector subtraction",
      "Geometric mean normalization"
    ],
    "answer": "Magnitude pruning of conflicting parameters",
    "explanation": "TIES-Merging identifies parameters with opposite sign changes across models, prunes the lower-magnitude ones, and only merges parameters moving in the same direction, preserving task specialization.",
    "difficulty": "Advanced"
  },
  {
    "id": 76,
    "question": "In HyDE (Hypothetical Document Embeddings) for advanced RAG, what is generated to improve retrieval for queries with sparse lexical overlap?",
    "options": [
      "Hypothetical answer embeddings",
      "Synthetic documents matching query semantics",
      "Knowledge graph subgraphs",
      "Re-ranked passage clusters"
    ],
    "answer": "Synthetic documents matching query semantics",
    "explanation": "HyDE generates hypothetical documents from the query using the language model itself, then retrieves real documents semantically similar to these synthetic ones, bridging lexical gaps.",
    "difficulty": "Advanced"
  },
  {
    "id": 77,
    "question": "What is the primary role of cross-encoder models in a two-stage RAG re-ranking pipeline?",
    "options": [
      "Generate candidate passages",
      "Compute query-passage relevance scores",
      "Extract knowledge graph entities",
      "Perform hypothetical document synthesis"
    ],
    "answer": "Compute query-passage relevance scores",
    "explanation": "Cross-encoders process query-passage pairs jointly, capturing complex interactions for precise re-ranking. Bi-encoders are used first for efficient candidate generation due to precomputed embeddings.",
    "difficulty": "Advanced"
  },
  {
    "id": 78,
    "question": "In knowledge graph-enhanced RAG, what advantage does GraphRAG provide over traditional vector RAG for complex analytical queries?",
    "options": [
      "Faster embedding computation",
      "Global reasoning across disconnected knowledge components",
      "Lower memory requirements",
      "Automatic entity resolution"
    ],
    "answer": "Global reasoning across disconnected knowledge components",
    "explanation": "GraphRAG builds entity-relation graphs and performs community detection + LLM summarization, enabling reasoning across semantically related but lexically disconnected knowledge regions.",
    "difficulty": "Advanced"
  },
  {
    "id": 79,
    "question": "In the ReAct agent framework, what is the core reasoning trace pattern that interleaves thinking and acting?",
    "options": [
      "Plan-Execute-Verify",
      "Thought \u2192 Action \u2192 Observation",
      "Query \u2192 Retrieve \u2192 Generate",
      "Critic \u2192 Actor \u2192 Learner"
    ],
    "answer": "Thought \u2192 Action \u2192 Observation",
    "explanation": "ReAct agents produce explicit thought tokens to reason, action tokens to call tools/APIs, then process observations from tool outputs, creating a dynamic reasoning trajectory.",
    "difficulty": "Advanced"
  },
  {
    "id": 80,
    "question": "What architectural principle distinguishes AutoGen's multi-agent conversation from single-agent frameworks?",
    "options": [
      "Hierarchical planning decomposition",
      "Dynamic agent role emergence",
      "Conversational handoff between specialized agents",
      "Shared memory token limit"
    ],
    "answer": "Conversational handoff between specialized agents",
    "explanation": "AutoGen orchestrates conversations where agents with different capabilities (coder, critic, tester) hand off control dynamically based on task needs and conversation state.",
    "difficulty": "Advanced"
  },
  {
    "id": 81,
    "question": "CrewAI implements a multi-agent system using what organizational metaphor borrowed from business operations?",
    "options": [
      "Assembly line processing",
      "Hierarchical memory management",
      "Agent crews with roles, goals, and backstories",
      "Swarm intelligence voting"
    ],
    "answer": "Agent crews with roles, goals, and backstories",
    "explanation": "CrewAI structures agents as crew members with defined roles (researcher, writer), goals, and backstories that inform their behavior, enabling structured collaboration.",
    "difficulty": "Advanced"
  },
  {
    "id": 82,
    "question": "In vLLM's PagedAttention, what data structure maps logical KV cache blocks to their physical GPU memory locations?",
    "options": [
      "Scatter table",
      "Block table",
      "Page directory",
      "KV index"
    ],
    "answer": "Block table",
    "explanation": "Each sequence maintains a block table mapping logical block indices to physical GPU memory locations. PagedAttention kernels read this table to gather scattered KV blocks.[1][2]",
    "difficulty": "Advanced"
  },
  {
    "id": 83,
    "question": "How does vLLM's continuous batching prevent GPU idle time compared to static batching?",
    "options": [
      "Pre-allocates maximum sequence length",
      "Dynamically swaps finished sequences with queued requests mid-generation",
      "Uses pipeline parallelism",
      "Applies model distillation"
    ],
    "answer": "Dynamically swaps finished sequences with queued requests mid-generation",
    "explanation": "Continuous batching removes completed sequences from the current batch and immediately inserts new pending requests at the same token generation step, eliminating empty slots.[1]",
    "difficulty": "Advanced"
  },
  {
    "id": 84,
    "question": "Speculative decoding in LLM serving accelerates inference by what mechanism?",
    "options": [
      "Parallel prefix computation",
      "Draft model proposes tokens verified by target model",
      "KV cache compression",
      "Quantized attention kernels"
    ],
    "answer": "Draft model proposes tokens verified by target model",
    "explanation": "A small fast draft model generates candidate tokens in parallel, which the large target model verifies in a single parallel step, achieving 2-3x speedup with minimal quality loss.[2]",
    "difficulty": "Advanced"
  },
  {
    "id": 85,
    "question": "Triton Inference Server optimizes LLM deployment through what key capability for custom accelerators?",
    "options": [
      "Automatic model quantization",
      "Dynamic batching with custom Triton kernels",
      "Federated learning support",
      "Multi-modal data pipelines"
    ],
    "answer": "Dynamic batching with custom Triton kernels",
    "explanation": "Triton allows writing custom CUDA kernels in Python via Triton Language, enabling optimized attention implementations and dynamic batching tailored to specific hardware accelerators.",
    "difficulty": "Advanced"
  },
  {
    "id": 86,
    "question": "In pipeline parallelism with GPipe scheduling, what is the 'synchronous' scheduling strategy's main drawback?",
    "options": [
      "Excessive memory usage",
      "Poor load balancing for uneven layer compute",
      "Gradient accumulation errors",
      "Communication overhead reduction"
    ],
    "answer": "Poor load balancing for uneven layer compute",
    "explanation": "Synchronous GPipe scheduling assumes uniform microbatch processing time across stages, causing bubbles when layers have imbalanced compute requirements.",
    "difficulty": "Advanced"
  },
  {
    "id": 87,
    "question": "Linear interpolation of model weights fails for merging models trained on conflicting tasks because:",
    "options": [
      "Weights have different scales",
      "It averages conflicting directional updates",
      "Optimizer states interfere",
      "Batch normalization statistics clash"
    ],
    "answer": "It averages conflicting directional updates",
    "explanation": "Linear interpolation takes the mean of parameter deltas, canceling out task-specific updates when models learn opposing directions in parameter space.",
    "difficulty": "Advanced"
  },
  {
    "id": 88,
    "question": "In advanced RAG with re-ranking, why are cross-encoders more accurate than bi-encoders despite higher compute cost?",
    "options": [
      "Cross-encoders share query-passage representations",
      "Bi-encoders use asymmetric embeddings",
      "Cross-encoders model query-passage interactions directly",
      "Cross-encoders support longer context"
    ],
    "answer": "Cross-encoders model query-passage interactions directly",
    "explanation": "Cross-encoders process query and passage jointly through all transformer layers, capturing complex semantic interactions that bi-encoders approximate via independent embeddings.",
    "difficulty": "Advanced"
  },
  {
    "id": 89,
    "question": "AutoGen's Group ChatManager handles multi-agent coordination by:",
    "options": [
      "Round-robin speaker selection",
      "Dynamic speaker selection based on LLM judgment",
      "Hierarchical command structure",
      "Majority voting mechanism"
    ],
    "answer": "Dynamic speaker selection based on LLM judgment",
    "explanation": "The LLM-powered ChatManager analyzes conversation history and agent capabilities to select the most appropriate next speaker, preventing infinite loops and ensuring progress.",
    "difficulty": "Advanced"
  },
  {
    "id": 90,
    "question": "PagedAttention eliminates external fragmentation in KV cache by:",
    "options": [
      "Pre-allocating contiguous slabs",
      "Using fixed-size blocks from a global free pool",
      "Compressing KV tensors",
      "Offloading to CPU memory"
    ],
    "answer": "Using fixed-size blocks from a global free pool",
    "explanation": "All blocks have identical size, so any free block can serve any allocation request. The block table hides physical scattering from the attention computation.[2]",
    "difficulty": "Advanced"
  },
  {
    "id": 91,
    "question": "ZeRO-Infinity extends ZeRO by adding what capability for extreme scale?",
    "options": [
      "CPU offloading with NVMe persistence",
      "Model parallelism integration",
      "Asynchronous communication",
      "Mixed precision gradients"
    ],
    "answer": "CPU offloading with NVMe persistence",
    "explanation": "ZeRO-Infinity offloads parameters to CPU/NVMe, using a persistent memory pool that survives across training runs, enabling training of trillion-parameter models on single nodes.",
    "difficulty": "Advanced"
  },
  {
    "id": 92,
    "question": "Task Arithmetic in model merging assumes model weights decompose into:",
    "options": [
      "Principal components",
      "Task vectors plus shared base model",
      "Low-rank factorizations",
      "Sparse activation patterns"
    ],
    "answer": "Task vectors plus shared base model",
    "explanation": "Task Arithmetic subtracts the pre-training base model from fine-tuned models to extract task vectors, then linearly combines them: M_shared + \u03a3\u03b1\u1d62\u00d7task_vector\u1d62.",
    "difficulty": "Advanced"
  },
  {
    "id": 93,
    "question": "What failure mode does ReAct mitigate compared to pure language model chain-of-thought?",
    "options": [
      "Hallucination through external verification",
      "Context overflow via tool selection",
      "Infinite reasoning loops via action grounding",
      "Gradient explosion via clipping"
    ],
    "answer": "Infinite reasoning loops via action grounding",
    "explanation": "ReAct grounds reasoning in concrete actions and observations from external tools, preventing the agent from spinning in unproductive internal reasoning cycles.",
    "difficulty": "Advanced"
  },
  {
    "id": 94,
    "question": "vLLM's chunked prefill addresses the challenge of:",
    "options": [
      "Long prompt latency blocking decode",
      "Multi-head attention divergence",
      "Gradient checkpointing overhead",
      "Optimizer divergence"
    ],
    "answer": "Long prompt latency blocking decode",
    "explanation": "Chunked prefill splits long prompts into smaller segments processed in parallel with decoding, preventing long prompts from blocking the entire inference pipeline.[1]",
    "difficulty": "Advanced"
  },
  {
    "id": 95,
    "question": "In FSDP2 (PyTorch 2.0+), what optimization reduces communication volume compared to FSDP1?",
    "options": [
      "Auto-wrapping policy",
      "Lazy all-gather on first backward",
      "Shared memory buffers",
      "Gradient checkpointing"
    ],
    "answer": "Lazy all-gather on first backward",
    "explanation": "FSDP2 delays parameter gathering until the first backward pass through the module, reducing redundant communication and enabling more aggressive sharding.",
    "difficulty": "Advanced"
  },
  {
    "id": 96,
    "question": "CrewAI's 'delegation' pattern differs from AutoGen by emphasizing:",
    "options": [
      "Conversational emergence",
      "Explicit task handoff with completion criteria",
      "Collective intelligence voting",
      "Hierarchical planning trees"
    ],
    "answer": "Explicit task handoff with completion criteria",
    "explanation": "CrewAI uses structured delegation where agents receive specific tasks with defined success criteria, rather than open-ended conversations.",
    "difficulty": "Advanced"
  },
  {
    "id": 97,
    "question": "Copy-on-write sharing in PagedAttention enables what efficiency for agent workflows?",
    "options": [
      "Parallel decoding of identical prompts",
      "Shared prefix KV cache until divergence",
      "Gradient accumulation sharing",
      "Multi-query attention fusion"
    ],
    "answer": "Shared prefix KV cache until divergence",
    "explanation": "Identical prompt prefixes share the same read-only KV blocks until a sequence diverges, then copy-on-write creates independent continuation blocks.[1]",
    "difficulty": "Advanced"
  },
  {
    "id": 98,
    "question": "Knowledge Graph RAG typically uses what retrieval pattern for entity-centric queries?",
    "options": [
      "Dense passage retrieval",
      "Subgraph extraction around entities",
      "Hypothetical document matching",
      "BM25 lexical search"
    ],
    "answer": "Subgraph extraction around entities",
    "explanation": "Given an entity query, GraphRAG extracts the connected subgraph of related entities and relations, providing structured context superior to flat text retrieval.",
    "difficulty": "Advanced"
  },
  {
    "id": 99,
    "question": "Triton kernels for LLM attention optimize what aspect of GPU execution?",
    "options": [
      "Automatic kernel fusion",
      "Tile size autotuning for specific models",
      "Dynamic batching scheduling",
      "Multi-GPU tensor parallelism"
    ],
    "answer": "Tile size autotuning for specific models",
    "explanation": "Triton enables writing attention kernels with autotunable tile sizes and loop orders optimized for specific model dimensions and hardware characteristics.",
    "difficulty": "Advanced"
  },
  {
    "id": 100,
    "question": "The '1F1B' scheduling in PipeDream balances pipeline parallelism by interleaving:",
    "options": [
      "Forward and backward passes",
      "Prefill and decode phases",
      "Microbatches optimally across stages",
      "Tensor and pipeline dimensions"
    ],
    "answer": "Forward and backward passes",
    "explanation": "1F1B (one forward, one backward) scheduling overlaps forward passes of microbatch n+1 with backward passes of microbatch n, minimizing bubble time in pipeline parallelism.",
    "difficulty": "Advanced"
  }
]